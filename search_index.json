[["index.html", "Spring 2021 EDAV Community Contributions Chapter 1 Community contribution assignment 1.1 Logistics", " Spring 2021 EDAV Community Contributions 2021-03-29 Chapter 1 Community contribution assignment This fairly open-ended assignment provides an opportunity to receive credit for contributing to the collective learning of the class, and perhaps beyond. It should reflect a minimum of 2-3 hours of work. To complete the assignment you must submit a short description of your contribution. If appropriate, attach relevant files. There are many ways in which you can contribute: lead a Zoom conversation on anything: hobby, happy hour, etc. Think about how you will organize it… do students need to sign up? Will you have breakout rooms? help students find final project partners give a well-rehearsed 5 minute lightning talk in class on a datavis topic (theory or tool) (email me to set the date – it may be after the assignment due date but you need to schedule it before) create a video tutorial (any length) create a cheatsheet or other resource be a Piazza super user (that is, answer a lot of questions) write a tutorial for a tool that’s not well documented translate a useful resource into another language build a viz product (ex. htmlwidget) for class use create a web site for sharing class resources publicly provide significant subject matter help to a classmate organize and lead a help session on a particular topic (the date may be after the assignment due date but you need to schedule it before) [your own idea] You may draw on and expand existing resources. When doing so, it is critical that you cite your sources. Examples from last year: https://jtr13.github.io/cc20/ 1.1 Logistics 1.1.1 Groups You may work on your own or with a partner of your choosing. If you work alone, you do not need to join a group of 1, you simply submit your work on CourseWorks as with any other solo assignment. If you work with a partner, add yourselves to a group on the CC page on the People tab. Piazza can be used to find partners with similar interests. 1.1.2 Submitting your assignment You must submit your assignment twice: once on CourseWorks (so it can be graded) and once to the class, details to follow. CourseWorks submission: submit your work as an .Rmd and rendered .pdf or .html file, just as with problem sets. If your work does not lend itself to that format, then write in the assignment text box what you did. Class (GitHub) submission: see the next chapter. "],["github-submission-instructions.html", "Chapter 2 GitHub submission instructions 2.1 Background 2.2 Preparing your .Rmd file 2.3 Submission steps 2.4 Optional tweaks 2.5 FAQ", " Chapter 2 GitHub submission instructions This chapter gives you all the information you need to upload your community contribution. Please read this entire document carefully before making your submission. Of particular note is the fact that bookdown requires a different .Rmd format than you’re used to, so you must make changes to the beginning of the file as described below before submitting. 2.1 Background This web site makes use of the bookdown package to render a collection of .Rmd files into a nicely formatted online book with chapters and subchapters. Your job will be to submit a slightly modified version of your community contribution .Rmd file to the GitHub repository where the source files for this web site are stored. On the backend, the admins will divide the chapters into book sections and order them. If your community contribution is in a different format, then create a short .Rmd file that explains what you did, and includes links to any relevant files, such as slides, etc. which you can post on your GitHub repo (or another online site.) 2.2 Preparing your .Rmd file You should only submit ONE Rmd file. After completing these modifications, your .Rmd should look like this sample .Rmd. Create a concise, descriptive name for your project. For instance, name it base_r_ggplot_graph or something similar if your work is about contrasting/working with base R graphics and ggplot2 graphics. Check the .Rmd filenames in the project repo to make sure your name isn’t already taken. Your project name should be words only and joined with underscores, no white space. Use .Rmd not .rmd. In addition, all letters must be lowercase. Create a copy of your .Rmd file with the new name. Completely delete the YAML header (the section at the top of the .Rmd that includes name, title, date, output, etc.) including the --- line. Choose a short, descriptive, human readable title for your project as your title will show up in the table of contents – look at examples in the 2020 EDAV rendered book of community contributions https://jtr13.github.io/cc20. Capitalize the first letter only (“sentence case”). On the first line of your document, enter a single hashtag, followed by a single whitespace, and then your title. It is important to follow this format so that bookdown renders your title as a header. Do not use single # headers anywhere else in the document. Note: if your chapter is the first in a section, a second # header and section description will be requested during the pull-request and merge phase. The second line should be blank, followed by your name(s): # Base R vs. ggplot2 Aaron Burr and Alexander Hamilton Your content starts here. If your project requires data, please use a built-in dataset or read directly from a URL, such as: df &lt;- readr::read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv\") If you absolutely must include a data file, please use a small one, as for many reasons it is desirable to keep the repository size as small as possible. If you have included a setup chunk in your Rmd file, please remember to remove the label setup in the chunk, i.e., use: {r, include=FALSE} instead of: {r setup, include=FALSE} If your project requires libraries to be installed and included in the document, please adhere to the following conventions. Do not evaluate any install.packages() statements in your document. Consumers of an .Rmd file won’t want packages to get installed when they knit your document. Include any library() statements at the top of your Rmd file, below the title, name, and setup, but above any content. If your chapter requires the installation of a package from source (is a GitHub installation), please add a comment identifying it as such. Please mention this as well in your PR. Here is an example library() section with install statements that won’t be evaluated: install.packages(&quot;devtools&quot;) # used for installation from source install.packages(&quot;dplyr&quot;) install.packages(&quot;ggplot2&quot;) library(&quot;devtools&quot;) devtools::install_github(&quot;twitter/AnomalyDetection&quot;) library(&quot;AnomalyDetection&quot;) # must be installed from source library(&quot;dplyr&quot;) library(&quot;ggplot2&quot;) You could include both sections, or you could just include the second library() section and trust R users to install any packages themselves. If you developed your Rmd file before moving your library() statements above the rest of the file content, it is highly recommended to knit and review your document again. This may change the namespace that was available in each section of your code during development, causing a function not to work or to exhibit unexpected behavior. This common issue is mentioned in the EDAV R Basics “Functions stop working” section. Your file should not contain getwd() / setwd() calls (you should never use these in scripts anyway!) nor write statements. Want to get fancy? See the optional tweaks section below. 2.3 Submission steps To submit your work, we will be following “Workflow #4” – that is submitting a pull request to someone else’s repository to which you do not have write access. Instructions are available in this tutorial. They are repeated below in abbreviated form, with specific instructions on naming conventions, content information, and other important details. Fork cc21 repo (this repo) to your GitHub account. Clone/download the forked repo to your local computer. Create a new branch and name it with your project name, in our case sample_project. Do not skip this step. We will not merge your PR if it doesn’t come from a branch. If you already forgot to do this, check this tutorial for how to fix it. Copy your modified .Rmd file with the same name into the root directory on the branch. In our example, it is sample_project.Rmd. Do not include an .html file. (In order for the bookdown package to work, all .Rmd files will be rendered behind the scenes.) [OPTIONAL] If you have other resources (such as images) included in your project, create a folder under resources/. In our example, it is resources/sample_project/. Put the resources files there. Be sure to change all the links in your .Rmd to include resources/.../, for example: ![Test Photo](resources/sample_project/election.jpg) When you are ready to submit your project, push your branch to your remote repo. Follow this tutorial to create a pull request. At this point a back and forth will begin with the team managing the pull requests. If you are asked to make changes, simply make the changes on your local branch, save, commit, and push to GitHub. The new commits will be added to your pull request; you do not need to, and should not, create a new pull request. (If, based on the circumstances, it does make sense to close the pull request and start a new one, we will tell you.) Once your pull request is merged, it’s fine to delete your local clone (folder) as well as the forked repository in your GitHub account. 2.4 Optional tweaks If you prefer for links from your chapter to open in new tabs, add {target=\"_blank\"} after the link, such as: [edav.info](edav.info){target=\"_blank\"} Note that your headers (##, ###, etc.) will be converted to numbered headings as such: ## –&gt; 3.1 ### –&gt; 3.1.1 These headings will appear as chapter subheadings and sub-subheadings in the navigation panel on the left. Think about a logical structure for users to navigate your chapter. We recommend using only ## and ### headings since “sub-sub-subheadings” such as 4.1.3.4 are generally unnecessary and look messy. Unfortunately, there’s no simple way to preview your chapter before it’s actually merged into the project. (bookdown has preview_chapter() option but it only works after the entire book has been rendered at least once and that will become more and more complex and require more and more packages as the project grows.) If you really want to preview it, fork and clone this minimal bookdown repo, add your .Rmd file, click the “Build book” button on the Build tab (next to Git), and then open any of the .html files in the _book folder in a web browser to see the rendered book. If you’re interested in more bookdown options, see the official reference book. Have more useful tweaks to share? Submit an issue or PR. 2.5 FAQ 2.5.1 What should I expect after creating a pull request? Within a week after you create a pull request, we will apply a label to it and assign an administrator who will review all the files you submit to see if they meet the requirements. It will take some time before we can process all the pull requests, so as long as you see your pull request has been labeled and assigned to an admin, don’t worry. However, if the admin contacts you regarding the pull request, that usually means your files fail to meet some requirements. The admin will clearly state what is wrong, so please fix them as soon as possible. 2.5.2 What if I catch mistakes before my pull request is merged? Just make the changes on your branch, commit and push them to GitHub. They will automatically be added to the pull request. 2.5.3 What if I catch mistakes after my pull request is merged? You may submit additional pull requests to fix material on the site. If the edits are small, such as fixing typos, it is easiest to make the edits directly on GitHub, following these instructions. We will merge first pull requests before edits, so please be patient. 2.5.4 Other questions If you encounter other problems, please submit an issue and we will look into it. Thank you for your contributions! "],["sample-project.html", "Chapter 3 Sample project", " Chapter 3 Sample project Joe Biden and Donald Trump This chapter gives a sample layout of your Rmd file. Test Photo "],["date-time-handling-in-r-cheatsheet.html", "Chapter 4 Date-Time Handling in R Cheatsheet", " Chapter 4 Date-Time Handling in R Cheatsheet Saloni Gupta Ajay Kumar This project includes a pdf version cheatsheet for basic date-time handling in R. This includes conversion to Date and Datetime objects, extraction of date elements, date arithmetic and time zone conversions. Click the following link to check out the cheatsheet: https://github.com/C130187/EDAVCC/blob/main/DateTime-cheatsheet-in-R.pdf "],["tidy-data-cheatsheet.html", "Chapter 5 Tidy data Cheatsheet", " Chapter 5 Tidy data Cheatsheet Yifan Jing This project includes a pdf version cheatsheet for tidy data. It includes functions in tidyr and dplyr. The link is below: https://github.com/JackJing001/STAT5702/blob/main/Tidy%20data%20cheat%20sheet.pdf "],["ggvis-cheatsheet.html", "Chapter 6 GGvis cheatsheet 6.1 ggvis 6.2 aesthetics 6.3 Interaction 6.4 Layers", " Chapter 6 GGvis cheatsheet Chenhui Mao Link to the cheatsheet 6.1 ggvis Official website: http://ggvis.rstudio.com/ ggvis is a visualization package that can: - plot the graph in the same way as ggplot2 - create interaction strategies where you can interact with the graph locally or in the website 6.1.1 Getting start First thing first, we shall import some necessary packages #install.packages(&quot;dplyr&quot;) #install.packages(&quot;ggvis&quot;) library(ggvis) library(dplyr) This is a simple demonstration on how to use ggvis. As can be seen, ggvis can create graph in the same way ggplot does! p &lt;- ggvis(mtcars, x = ~wt, y = ~ mpg) layer_points(p) Renderer: SVG | Canvas Download Similar way to write basic ggvis layer_points(ggvis(mtcars, x = ~wt, y = ~ mpg)) Renderer: SVG | Canvas Download Apply pipeline to write ggvis mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points Renderer: SVG | Canvas Download We can add a theme on it mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points() %&gt;% add_axis(&quot;x&quot;, title = &quot;Weight&quot;, ticks = 40, properties = axis_props( ticks = list(stroke = &quot;red&quot;), majorTicks = list(strokeWidth = 2), grid = list(stroke = &quot;red&quot;), labels = list( fill = &quot;steelblue&quot;, angle = 50, fontSize = 14, align = &quot;left&quot;, baseline = &quot;middle&quot;, dx = 3 ), title = list(fontSize = 16), axis = list(stroke = &quot;#333&quot;, strokeWidth = 1.5) ) ) Renderer: SVG | Canvas Download 6.2 aesthetics Try to adjust the parameters mtcars %&gt;% ggvis(~mpg, ~disp, stroke = ~vs) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~mpg, ~disp, fill = ~vs) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~mpg, ~disp, size = ~vs) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~mpg, ~disp, shape = ~factor(cyl)) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~wt, ~mpg, fill := &quot;red&quot;, stroke := &quot;black&quot;) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~wt, ~mpg, size := 300, opacity := 0.4) %&gt;% layer_points() Renderer: SVG | Canvas Download 6.3 Interaction 6.3.1 basic interaction mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points() %&gt;% layer_smooths( span = input_slider(0.2, 1, value = 0.5, step = 0.05, label = &quot;span&quot;) ) Renderer: SVG | Canvas Download Next, we add some interaction strategies into it. We can add the following interaction into the graph: - Change the size of the dot - Change the opacity of the dot mtcars %&gt;% ggvis(~wt, ~mpg, size := input_slider(10, 100), opacity := input_slider(0, 1) ) %&gt;% layer_points() Renderer: SVG | Canvas Download Add interaction strategies to a histograms - Change the width of the histograms - Change the center of the histograms mtcars %&gt;% ggvis(~wt) %&gt;% layer_histograms( width = input_slider(0, 2, step = 0.10, label = &quot;width&quot;), center = input_slider(0, 2, step = 0.05, label = &quot;center&quot;)) Renderer: SVG | Canvas Download Add tooltip mtcars %&gt;% ggvis(~wt, ~mpg) %&gt;% layer_points()%&gt;% add_tooltip(function(df) df$wt) Renderer: SVG | Canvas Download 6.3.2 Input option 6.3.2.1 input_select() mtcars %&gt;% ggvis(x = ~wt) %&gt;% layer_densities( adjust = input_slider(.1, 2, value = 1, step = .1, label = &quot;Bandwidth adjustment&quot;), kernel = input_select( c(&quot;Gaussian&quot; = &quot;gaussian&quot;, &quot;Epanechnikov&quot; = &quot;epanechnikov&quot;, &quot;Rectangular&quot; = &quot;rectangular&quot;, &quot;Triangular&quot; = &quot;triangular&quot;, &quot;Biweight&quot; = &quot;biweight&quot;, &quot;Cosine&quot; = &quot;cosine&quot;, &quot;Optcosine&quot; = &quot;optcosine&quot;), label = &quot;Kernel&quot;) ) Renderer: SVG | Canvas Download 6.3.2.2 Checkbox input mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points( opacity := input_checkbox(label = &quot;Semi-transparent&quot;, map = function(val) ifelse(val, .3, 1))) %&gt;% layer_model_predictions( model = input_checkbox(label = &quot;LOESS (curve) model fit&quot;, map = function(val) ifelse(val, &quot;loess&quot;, &quot;lm&quot;))) Renderer: SVG | Canvas Download 6.3.2.3 Text input mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points(fill := input_text(label = &quot;Point color&quot;, value = &quot;red&quot;)) %&gt;% layer_model_predictions(model = input_text(label = &quot;Model type&quot;, value = &quot;loess&quot;)) Renderer: SVG | Canvas Download 6.3.2.4 Numeric input mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points(size := input_numeric(value = 25, label = &quot;Point size&quot;)) %&gt;% layer_smooths(span = input_numeric(value = 0.5, label = &quot;Interpolation points&quot;)) Renderer: SVG | Canvas Download 6.3.2.5 Radio buttons mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points() %&gt;% layer_model_predictions( model = input_radiobuttons(c(&quot;LOESS&quot; = &quot;loess&quot;, &quot;Linear&quot; = &quot;lm&quot;), label = &quot;Model type&quot;), stroke := input_radiobuttons(c(&quot;Red&quot; = &quot;red&quot;, &quot;Black&quot; = &quot;black&quot;), label = &quot;Line color&quot;) ) Renderer: SVG | Canvas Download 6.3.2.6 Checkbox group mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points( fill := input_checkboxgroup( choices = c(&quot;Red&quot; = &quot;r&quot;, &quot;Green&quot; = &quot;g&quot;, &quot;Blue&quot; = &quot;b&quot;), label = &quot;Point color components&quot;, map = function(val) { rgb(0.8 * &quot;r&quot; %in% val, 0.8 * &quot;g&quot; %in% val, 0.8 * &quot;b&quot; %in% val) } ) ) Renderer: SVG | Canvas Download 6.3.3 Map 6.3.3.1 Example with value map function mtcars %&gt;% ggvis(x = ~wt, y = ~mpg) %&gt;% layer_points() %&gt;% layer_model_predictions(model = &quot;loess&quot;, model_args = list(n = input_select( choices = c(&quot;Two&quot;, &quot;Six&quot;, &quot;Eighty&quot;), map = function(value) switch(value, Two = 2, Six = 6, Eighty = 80), label = &quot;Number of points&quot; )) ) Renderer: SVG | Canvas Download new_vals &lt;- input_select(c(&quot;Set A&quot; = &quot;A&quot;, &quot;Set B&quot; = &quot;B&quot;), label = &quot;Dynamically-generated column&quot;, map = function(value) { vals &lt;- switch(value, &quot;A&quot; = rep(c(&quot;One&quot;, &quot;Two&quot;)), &quot;B&quot; = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) rep(vals, length = nrow(mtcars)) }) mtcars %&gt;% ggvis(x = ~wt, y = ~mpg, fill = new_vals) %&gt;% layer_points() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis( x = ~wt, y = ~mpg, fill = input_select(c(&quot;mpg&quot;, &quot;wt&quot;), map = as.name) ) %&gt;% layer_points() Renderer: SVG | Canvas Download 6.4 Layers 6.4.1 Simple layers include primitives like points, lines and rectangles. mtcars %&gt;% ggvis(~wt, ~mpg) %&gt;% layer_points() Renderer: SVG | Canvas Download Paths and polygons df &lt;- data.frame(x = 1:10, y = runif(10)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_paths() Renderer: SVG | Canvas Download If you add fill, you will get a polygon t &lt;- seq(0, 2 * pi, length = 100) df &lt;- data.frame(x = sin(t), y = cos(t)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_paths(fill := &quot;red&quot;) Renderer: SVG | Canvas Download df &lt;- data.frame(x = 1:10, y = runif(10)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_ribbons() Renderer: SVG | Canvas Download Rectangle set.seed(1014) df &lt;- data.frame(x1 = runif(5), x2 = runif(5), y1 = runif(5), y2 = runif(5)) df %&gt;% ggvis(~x1, ~y1, x2 = ~x2, y2 = ~y2, fillOpacity := 0.1) %&gt;% layer_rects() Renderer: SVG | Canvas Download Text df &lt;- data.frame(x = 3:1, y = c(1, 3, 2), label = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) df %&gt;% ggvis(~x, ~y, text := ~label) %&gt;% layer_text(fontSize := 50) Renderer: SVG | Canvas Download 6.4.2 Compound layers df &lt;- data.frame(x = sin(t), y = cos(t)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_paths() Renderer: SVG | Canvas Download df %&gt;% ggvis(~x, ~y) %&gt;% layer_lines() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~mpg) %&gt;% layer_histograms() Renderer: SVG | Canvas Download mtcars %&gt;% ggvis(~wt, ~mpg) %&gt;% layer_smooths() Renderer: SVG | Canvas Download "],["statistical-hypothesis-testing-cheatsheet.html", "Chapter 7 Statistical_hypothesis_testing_cheatsheet", " Chapter 7 Statistical_hypothesis_testing_cheatsheet Mingyan Zou This contribution includes a pdf version cheatsheet for an introduction to statistical hypothesis testing, comparison between different testing methods, and the test statistics formulas. Please check out the below link: https://github.com/Mingyan-zou/EDAVCC/blob/main/statistical_hypothesis_cheatsheet.pdf "],["basics-r-cheatsheet.html", "Chapter 8 Basics R Cheatsheet", " Chapter 8 Basics R Cheatsheet Wanxiao Hong This is an PDF version cheat sheet for basic R functions, such as data read/write, libraries, math functions. I personally find that for R beginners who are familiar with other programming languages, the most basic functions and attributes are the most un-intuitive part. And these basic information are often scattered in different webs. Searching every single time is inconvenient. So I make this cheat sheet for quicker look up. Check the cheat sheet by clicking the following Github link: https://github.com/jueerEcho/Columbia/blob/main/STATGR5702_EDAV/R_basics_cheatsheet_wh2493.pdf "],["alluvial-diagram-cheatsheet.html", "Chapter 9 Alluvial Diagram Cheatsheet", " Chapter 9 Alluvial Diagram Cheatsheet Yeqi Zhang This project includes a pdf version cheatsheet for Alluvial Diagram Cheatsheet. It includes instructions for plotting, analyzing, faceting and etc. Click the following link to check out the cheatsheet: https://github.com/zhangyeqi98/Community-Contribution "],["ggplot2-cheatsheet.html", "Chapter 10 ggplot2_cheatsheet 10.1 Some Advanced Techs from ggplot2 10.2 Dataset 10.3 Facet 10.4 Facet layer basics 10.5 Many variables 10.6 Formula notation 10.7 Labeling facets 10.8 Coordinates 10.9 Expand and clip 10.10 Flipping axes 10.11 Pie charts 10.12 Conclusion", " Chapter 10 ggplot2_cheatsheet Xiaorui Zhang 10.1 Some Advanced Techs from ggplot2 10.2 Dataset library(ggplot2) library(dplyr) ## [, 1] mpg Miles/(US) gallon ## [, 2] cyl Number of cylinders ## [, 3] disp Displacement (cu.in.) ## [, 4] hp Gross horsepower ## [, 5] drat Rear axle ratio ## [, 6] wt Weight (1000 lbs) ## [, 7] qsec 1/4 mile time ## [, 8] vs Engine (0 = V-shaped, 1 = straight) ## [, 9] am Transmission (0 = automatic, 1 = manual) ## [,10] gear Number of forward gears ## [,11] carb Number of carburetors head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 10.3 Facet 10.4 Facet layer basics ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Facet rows by am and columns by cyl facet_grid(rows = vars(am),cols = vars(cyl)) 10.5 Many variables mtcars$fcyl_fam &lt;- interaction(mtcars$cyl,mtcars$am,sep = &quot;:&quot;) ## Update the plot ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam, size = disp)) + geom_point() + scale_color_brewer(palette = &quot;Paired&quot;) + ## Grid facet on gear and vs facet_grid(rows = vars(gear), cols = vars(vs)) 10.6 Formula notation ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Facet rows by am using formula notation facet_grid(am ~ .) ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Facet columns by cyl using formula notation facet_grid(. ~ cyl) ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Facet rows by am and columns by cyl using formula notation facet_grid(am ~ cyl) 10.7 Labeling facets ## Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Displaying both the values and the variables facet_grid(cols = vars(cyl), labeller = label_both) ## Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + ## Label context facet_grid(cols = vars(cyl), labeller = label_context) 10.8 Coordinates 10.9 Expand and clip ## Expand sets a buffer margin around the plot, so data and axes don&#39;t overlap. ## Setting expand to 0 draws the axes to the limits of the data. ## Clip decides whether plot elements that would lie outside the plot panel ## are displayed or ignored (&quot;clipped&quot;). ggplot(mtcars, aes(wt, mpg)) + geom_point(size = 2) + ## Add Cartesian coordinates with zero expansion coord_cartesian(expand = 0) + theme_classic() ggplot(mtcars, aes(wt, mpg)) + geom_point(size = 2) + ## Turn clipping off coord_cartesian(expand = 0, clip = &quot;off&quot;) + theme_classic() + ## Remove axis lines theme(axis.line = element_blank()) 10.10 Flipping axes ## Plot fcyl bars, filled by fam ggplot(mtcars, aes(as.factor(cyl), fill = as.factor(am))) + ## Place bars side by side geom_bar(position = &quot;dodge&quot;) ggplot(mtcars, aes(as.factor(cyl), fill = as.factor(am))) + geom_bar(position = &quot;dodge&quot;) + ## Flip the x and y coordinates coord_flip() ggplot(mtcars, aes(as.factor(cyl), fill = as.factor(am))) + ## Set a dodge width of 0.5 for partially overlapping bars geom_bar(position = position_dodge(width = 0.5)) + coord_flip() 10.11 Pie charts ## Run the code, view the plot, then update it ggplot(mtcars, aes(x = 1, fill = as.factor(cyl))) + geom_bar()+ ## Add a polar coordinate system coord_polar(theta = &quot;y&quot;) ggplot(mtcars, aes(x = 1, fill = as.factor(cyl))) + ## Reduce the bar width to 0.1 geom_bar(width = 0.1) + coord_polar(theta = &quot;y&quot;) + ## Add a continuous x scale from 0.5 to 1.5 scale_x_continuous(limits = c(0.5,1.5)) 10.12 Conclusion This is only a little part of ggplot2, and then in my additional resources, compared with ggplot2, everyone can learn the visualization tools from Python if needed. "],["colors-in-r-chinese-translation.html", "Chapter 11 Colors_in_R_Chinese_translation 11.1 在R中使用颜色", " Chapter 11 Colors_in_R_Chinese_translation Xiaoyan Li Source: http://www.sthda.com/english/wiki/colors-in-r 11.1 在R中使用颜色 在 R 中，颜色可以使用指定名称（例如 col = “red”）或使用十六进制编码（如 col =“#FFCC00”）。您还可以使用其他颜色系统，例如从 R 包 RColorBrewer 中获取的彩色系统。 11.1.1 R自带的指定颜色名称 我们将使用以下自定义 R 函数生成一张包含 R 中可用的颜色名称的图： # 生成一张包含R自带颜色名称的图片 #++++++++++++++++++++++++++++++++++++++++++++ # cl : 要绘制的颜色矢量 # bg: 图片背景 # rot: 字体旋转角度 # usage = showCols(bg=&quot;gray33&quot;) showCols &lt;- function(cl=colors(), bg = &quot;grey&quot;, cex = 0.75, rot = 30) { m &lt;- ceiling(sqrt(n &lt;-length(cl))) length(cl) &lt;- m*m; cm &lt;- matrix(cl, m) require(&quot;grid&quot;) grid.newpage(); vp &lt;- viewport(w = .92, h = .92) grid.rect(gp=gpar(fill=bg)) grid.text(cm, x = col(cm)/m, y = rev(row(cm))/m, rot = rot, vp=vp, gp=gpar(cex = cex, col = cm)) } 下图包含前60种颜色的名称： # 前60种颜色的名称 showCols(bg=&quot;gray20&quot;,cl=colors()[1:60], rot=30, cex=0.9) # 使用颜色名称绘制柱状图 barplot(c(2,5), col=c(&quot;chartreuse&quot;, &quot;blue4&quot;)) 要查看 R 的所有内置颜色名称（n = 657），请使用以下 R 代码： showCols(cl= colors(), bg=&quot;gray33&quot;, rot=30, cex=0.75) 11.1.2 使用十六进制编码指定颜色 颜色可以通过十六进制编码指定，例如“#FFC00” （来源：http://www.visibone.com） # 用十六进制编码绘制柱状图 barplot(c(2,5), col=c(&quot;#009999&quot;, &quot;#0000FF&quot;)) 11.1.3 使用 RColorBrewer 调色盘 你需要安装 RColorBrewer 包： # install.packages(&quot;RColorBrewer&quot;) 与 RColorBrewer 包关联的调色板可以使用 display.brewer.all（）绘制，如下： library(&quot;RColorBrewer&quot;) display.brewer.all() 调色板有3种类型：循序、发散和定性。 循序调色板适合用于从低到高（梯度）的有序数据。调色板名称分别是是： Blues, BuGn, BuPu, GnBu, Greens, Greys, Oranges, OrRd, PuBu, PuBuGn, PuRd, Purples, RdPu, Reds, YlGn, YlGnBu YlOrBr, YlOrRd。 发散调色板重点强调了数据范围的两极和中间值。调色板分别有：BrBG, PiYG, PRGn, PuOr, RdBu, RdGy, RdYlBu, RdYlGn, Spectral。 定性调色板最适合表示名义数据或分类数据，并不表明群体之间的幅度差异。调色板名称是： Accent, Dark2, Paired, Pastel1, Pastel2, Set1, Set2, Set3。 你也可以指定查看单个的RColorBrewer调色盘： # 查看单个调色板 display.brewer.pal(n = 8, name = &#39;RdBu&#39;) # 查看单个调色板中颜色的十六进制编码 brewer.pal(n = 8, name = &quot;RdBu&quot;) ## [1] &quot;#B2182B&quot; &quot;#D6604D&quot; &quot;#F4A582&quot; &quot;#FDDBC7&quot; &quot;#D1E5F0&quot; &quot;#92C5DE&quot; &quot;#4393C3&quot; ## [8] &quot;#2166AC&quot; # 用 RColorBrewer 绘制柱状图 barplot(c(2,5,7), col=brewer.pal(n = 3, name = &quot;RdBu&quot;)) 11.1.4 使用 Wes Anderson 调色盘 此调色板可以安装和加载如下： # 安装 # install.packages(&quot;wesanderson&quot;) # 加载 library(wesanderson) 提供调色盘如下： 使用调色板： # 一个简单的柱状图 barplot(c(2,5,7), col=wes_palette(n=3, name=&quot;GrandBudapest1&quot;)) library(ggplot2) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point(size = 2) + scale_color_manual(values = wes_palette(n=3, name=&quot;GrandBudapest1&quot;)) 11.1.5 创建一个n个连续颜色的矢量 你也可以使用函数rainbow(n), heat.colors(n), terrain.colors(n), topo.colors(n), and cm.colors(n)创建一个包含n个连续的颜色的矢量。 # 使用彩虹的颜色 barplot(1:5, col=rainbow(5)) # 使用热量图的颜色 barplot(1:5, col=heat.colors(5)) # 使用 terrain.colors barplot(1:5, col=terrain.colors(5)) # 使用 topo.colors barplot(1:5, col=topo.colors(5)) # 使用 cm.colors barplot(1:5, col=cm.colors(5)) "],["managing-data-frames-with-dplyr-package-in-chinese.html", "Chapter 12 Managing Data Frames with dplyr Package in Chinese 12.1 Short Description of My Contribution 12.2 数据表 12.3 dplyr包 12.4 dplyr语法 12.5 安装dplyr包 12.6 select() 12.7 filter() 12.8 arrange() 12.9 rename() 12.10 mutate() 12.11 group_by() 12.12 %&gt;% 12.13 总结 12.14 Cited Resourses", " Chapter 12 Managing Data Frames with dplyr Package in Chinese Haoyue Qi library(dplyr) 12.1 Short Description of My Contribution The dplyr package is a very useful package in R for manipulating data frame, and there are many students in China interested in data science and R. However, it is difficult to find a good tutorial for dplyr package in Chinese online. Therefore, my community contribution is that I translate book “R Programming for Data Science”’s 13th chapter “Managing Data Frames with the dplyr package” into Chinese. 12.2 数据表 数据表在统计学和R中是一个核心的数据结构。一个数据表的基础结构是：每一行代表一个观测数据，每一列代表一个变量或观测数据的一个特征。R对数据表有一个内部实现，并且这会是你最常用的。然而，CRAN上会有一些其他实现数据表的包，这些包通过使用关系型数据库的概念可以允许你操作非常非常大的数据表(但是我们在这里不会讨论)。 因为管理数据表的重要性，有好的工具去管理数据表非常重要。在之前的章节中我们已经讨论过了一些工具如subset()函数，还有使用操作符[和$去提取部分数据表。然而，还有一些其他的操作，如过滤，排序，折叠，这些操作在R中非常繁琐，并且语法不是很直观。dplyr包被设计的目的就是去减轻这些问题，并提供一组高度优化过的函数，专门用于操作数据表。 12.3 dplyr包 dplyr包由Rstudio的Hadley Wickham开发，并且dplyr包是他开发的另一个包plyr包的优化精简版本。dplyr包并没有为R提供新的功能。换句话来说就是，dplyr包能做的所有事情base R都可以做，但dplyr包大幅度简化了base R的繁琐操作。 dplyr包的一个重要贡献是它为数据操作和操作数据表提供了一种“语法”(特别是动词)。通过这种语法，你可以与其他懂这种语法的人轻松地交流你对数据表做了什么。这很有用因为它为数据操作提供了一种抽象，这种抽象是之前不存在的。另一个有用的贡献是dolyr函数非常的快，因为其中很多核心操作使用C++编码的。 12.4 dplyr语法 dplyr包提供的一些核心“动词”有： select: 返回数据表的一部分列。 filter: 基于逻辑条件提取数据表的一部分行。 arrange: 重新排列数据表的行。 rename: 重新命名数据表的变量。 mutate: 添加一个新变量/列。或改变一个现有的变量。 summarise/summarize: 生成数据表中不同变量的统计数据。 %&gt;%: 管道操作符，用于连接多个动词操作形成一个管道。 dplyr包有一些它可以利用的自己的数据类型。例如，有一个方便的print函数可以防止你打印很多的数据在控制台里。大多数时间里，这些额外的数据类型对用户是透明的，所以用户不需要去担心。 12.4.1 通用dplyr函数属性 所有我们这章讨论的函数都有一些共同的特点。例如: 第一个参数是数据表。 后续的参数描述了对数据表做什么，并且你可以直接指定数据表中的列，不需要使用$操作符(直接用列名即可)。 函数的返回结果是一个新的数据表。 数据表必须有正确的格式和注释。特别是，数据必须是整洁的(http://www.jstatsoft.org/v59/i10/paper)。简短来说，整洁的意思是必须每行一个观测数据，并且每列代表一个特征。 12.5 安装dplyr包 dplyr包可以通过CRAN安装，也可以使用devtools包和install_github()函数通过GitHub安装。GitHub仓库通常有包的最新版本。 通过CRAN安装，可运行如下代码 install.packages(“dplyr”) 通过GitHub安装，可运行如下代码 install_github(“hadley/dplyr”) 安装完包之后，你需要使用library()函数把它载入到R会话中。 library(dplyr) 你有可能会看到一些警告当包被载入的时候因为dplyr包中有一些函数的名字跟其他包中的一些函数的名字一样。目前为止你可以忽略这些警告。 12.6 select() 这一章节的例子我们用的是一组数据包含芝加哥的空气污染和温度数据。数据网址是http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip。 解压文件后，你可以把数据加载到R中通过readRDS()函数。 rds &lt;- tempfile() download.file(&quot;http://www.biostat.jhsph.edu/~rpeng/leanpub/rprog/chicago_data.zip&quot;,rds) chicago &lt;- readRDS(gzcon(unz(rds, &quot;chicago.rds&quot;))) 你可以看到数据集的一些基础特点通过dim()和str()函数。 dim(chicago) ## [1] 6940 8 str(chicago) ## &#39;data.frame&#39;: 6940 obs. of 8 variables: ## $ city : chr &quot;chic&quot; &quot;chic&quot; &quot;chic&quot; &quot;chic&quot; ... ## $ tmpd : num 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ... ## $ dptp : num 31.5 29.9 27.4 28.6 28.9 ... ## $ date : Date, format: &quot;1987-01-01&quot; &quot;1987-01-02&quot; ... ## $ pm25tmean2: num NA NA NA NA NA NA NA NA NA NA ... ## $ pm10tmean2: num 34 NA 34.2 47 NA ... ## $ o3tmean2 : num 4.25 3.3 3.33 4.38 4.75 ... ## $ no2tmean2 : num 20 23.2 23.8 30.4 30.3 ... select()函数可以被用作去选择你想关注的数据表的列。通常来说，你会有一个很大的数据表包含所有的数据，但是分析只需要一部分变量或观测数据。select()函数允许你获得一些你需要的列。 假设我们只想要前3列。有几个方法可以做到。例如我们可以用数值索引。但是我们也可以直接用列名。 names(chicago)[1:3] ## [1] &quot;city&quot; &quot;tmpd&quot; &quot;dptp&quot; subset &lt;- select(chicago, city:dptp) head(subset) ## city tmpd dptp ## 1 chic 31.5 31.500 ## 2 chic 33.0 29.875 ## 3 chic 33.0 27.375 ## 4 chic 29.0 28.625 ## 5 chic 32.0 28.875 ## 6 chic 40.0 35.125 注意，一般来说:不可以用于名字或字符串，但在select()函数里面，你可以把它用于指定一个范围的变量名。 你也可以不要一些变量通过使用负号在select()函数中。通过select()你可以做 select(chicago, -(city:dptp)) 这句话的意思是包含所有变量除了变量city到dptp。同样意思的代码用base R编写为 i &lt;- match(&quot;city&quot;, names(chicago)) j &lt;- match(&quot;dptp&quot;, names(chicago)) head(chicago[, -(i:j)]) ## date pm25tmean2 pm10tmean2 o3tmean2 no2tmean2 ## 1 1987-01-01 NA 34.00000 4.250000 19.98810 ## 2 1987-01-02 NA NA 3.304348 23.19099 ## 3 1987-01-03 NA 34.16667 3.333333 23.81548 ## 4 1987-01-04 NA 47.00000 4.375000 30.43452 ## 5 1987-01-05 NA NA 4.750000 30.33333 ## 6 1987-01-06 NA 48.00000 5.833333 25.77233 这种写法并不直观。 select()函数也允许一个特殊的语法，这种语法可以允许你指定列名通过模式。例如，你想要所有的变量结尾是“2”，我们可以做 subset &lt;- select(chicago, ends_with(&quot;2&quot;)) str(subset) ## &#39;data.frame&#39;: 6940 obs. of 4 variables: ## $ pm25tmean2: num NA NA NA NA NA NA NA NA NA NA ... ## $ pm10tmean2: num 34 NA 34.2 47 NA ... ## $ o3tmean2 : num 4.25 3.3 3.33 4.38 4.75 ... ## $ no2tmean2 : num 20 23.2 23.8 30.4 30.3 ... 或者我们想要所有的变量开头是“d”，我们可以做 subset &lt;- select(chicago, starts_with(&quot;d&quot;)) str(subset) ## &#39;data.frame&#39;: 6940 obs. of 2 variables: ## $ dptp: num 31.5 29.9 27.4 28.6 28.9 ... ## $ date: Date, format: &quot;1987-01-01&quot; &quot;1987-01-02&quot; ... 你也可以用正则表达式。请看帮助文档(?select)获取更多细节。 12.7 filter() filter()函数可以被用作获取数据表的一部分行。这个函数跟base R中的subset()函数一样，但比subset()函数快。 假设我们想要去获取chicago数据表的一些行，这些行的PM2.5大于30，我们可以做 chic.f &lt;- filter(chicago, pm25tmean2 &gt; 30) str(chic.f) ## &#39;data.frame&#39;: 194 obs. of 8 variables: ## $ city : chr &quot;chic&quot; &quot;chic&quot; &quot;chic&quot; &quot;chic&quot; ... ## $ tmpd : num 23 28 55 59 57 57 75 61 73 78 ... ## $ dptp : num 21.9 25.8 51.3 53.7 52 56 65.8 59 60.3 67.1 ... ## $ date : Date, format: &quot;1998-01-17&quot; &quot;1998-01-23&quot; ... ## $ pm25tmean2: num 38.1 34 39.4 35.4 33.3 ... ## $ pm10tmean2: num 32.5 38.7 34 28.5 35 ... ## $ o3tmean2 : num 3.18 1.75 10.79 14.3 20.66 ... ## $ no2tmean2 : num 25.3 29.4 25.3 31.4 26.8 ... 你可以看到返回的数据表只有194行。pm25tmean2变量的分布是 summary(chic.f$pm25tmean2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 30.05 32.12 35.04 36.63 39.53 61.50 我们可以放很复杂的逻辑链在filter()函数里。这样我们就可以得到PM2.5大于30并且温度大于80华氏度的行。 chic.f &lt;- filter(chicago, pm25tmean2 &gt; 30 &amp; tmpd &gt; 80) select(chic.f, date, tmpd, pm25tmean2) ## date tmpd pm25tmean2 ## 1 1998-08-23 81 39.60000 ## 2 1998-09-06 81 31.50000 ## 3 2001-07-20 82 32.30000 ## 4 2001-08-01 84 43.70000 ## 5 2001-08-08 85 38.83750 ## 6 2001-08-09 84 38.20000 ## 7 2002-06-20 82 33.00000 ## 8 2002-06-23 82 42.50000 ## 9 2002-07-08 81 33.10000 ## 10 2002-07-18 82 38.85000 ## 11 2003-06-25 82 33.90000 ## 12 2003-07-04 84 32.90000 ## 13 2005-06-24 86 31.85714 ## 14 2005-06-27 82 51.53750 ## 15 2005-06-28 85 31.20000 ## 16 2005-07-17 84 32.70000 ## 17 2005-08-03 84 37.90000 只有17个观测数据两个条件都满足。 12.8 arrange() arrange()函数被用于根据一个变量重新排序数据表的行。重新排列数据表的行的同时保持其他列的顺序对于R来说是很难的。arrange()函数极大简化了这一过程。 这里我们可以通过date变量排序数据表，所以第一行是最早的观测数据，最后一行是最迟的观测数据。 chicago &lt;- arrange(chicago, date) 现在我们可以检查前几行 head(select(chicago, date, pm25tmean2), 3) ## date pm25tmean2 ## 1 1987-01-01 NA ## 2 1987-01-02 NA ## 3 1987-01-03 NA 和最后几行 tail(select(chicago, date, pm25tmean2), 3) ## date pm25tmean2 ## 6938 2005-12-29 7.45000 ## 6939 2005-12-30 15.05714 ## 6940 2005-12-31 15.00000 列也可以以降序重新排序通过用desc()操作符。 chicago &lt;- arrange(chicago, desc(date)) 看看降序排序的前三行和后三行。 head(select(chicago, date, pm25tmean2), 3) ## date pm25tmean2 ## 1 2005-12-31 15.00000 ## 2 2005-12-30 15.05714 ## 3 2005-12-29 7.45000 tail(select(chicago, date, pm25tmean2), 3) ## date pm25tmean2 ## 6938 1987-01-03 NA ## 6939 1987-01-02 NA ## 6940 1987-01-01 NA 12.9 rename() 重新命名数据表的一个变量是非常难得在R中。rename()函数被设计让这个过程变得简单。 这里你可以看到chicago数据表的前5个变量名。 head(chicago[, 1:5], 3) ## city tmpd dptp date pm25tmean2 ## 1 chic 35 30.1 2005-12-31 15.00000 ## 2 chic 36 31.0 2005-12-30 15.05714 ## 3 chic 35 29.4 2005-12-29 7.45000 dptp列应该代表露点温度，pm25tmean2列提供了PM2.5数据。然而，这些名字非常模糊，所以它们应该被重新命名让它们更容易被理解。 chicago &lt;- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2) head(chicago[, 1:5], 3) ## city tmpd dewpoint date pm25 ## 1 chic 35 30.1 2005-12-31 15.00000 ## 2 chic 36 31.0 2005-12-30 15.05714 ## 3 chic 35 29.4 2005-12-29 7.45000 rename()函数里的语法是把新名字放在等号的左边，把旧名字放在等号的右边。 你可以尝试如何用base R完成重命名任务。 12.10 mutate() mutate()函数存在的目的是计算数据表中变量的转换。通常来说，你想要根据一个存在的变量创造一个新变量。mutate()可以简洁的帮你完成这个任务。 例如，对于空气污染数据，我们经常想减去平均数从而减小数据。这样我们就可以看出某天的空气污染是高于还是低于平均值。 这里我们创造一个pm25detrend变量通过从pm25变量里减去平均数。 chicago &lt;- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE)) head(chicago) ## city tmpd dewpoint date pm25 pm10tmean2 o3tmean2 no2tmean2 ## 1 chic 35 30.1 2005-12-31 15.00000 23.5 2.531250 13.25000 ## 2 chic 36 31.0 2005-12-30 15.05714 19.2 3.034420 22.80556 ## 3 chic 35 29.4 2005-12-29 7.45000 23.5 6.794837 19.97222 ## 4 chic 37 34.5 2005-12-28 17.75000 27.5 3.260417 19.28563 ## 5 chic 40 33.6 2005-12-27 23.56000 27.0 4.468750 23.50000 ## 6 chic 35 29.6 2005-12-26 8.40000 8.5 14.041667 16.81944 ## pm25detrend ## 1 -1.230958 ## 2 -1.173815 ## 3 -8.780958 ## 4 1.519042 ## 5 7.329042 ## 6 -7.830958 还有一个transmute()函数，所做的事情跟mutate()函数一样，但是会去掉所有没有转换的变量。 这里我们减小了PM10和ozone变量。 head(transmute(chicago, pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE), o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE))) ## pm10detrend o3detrend ## 1 -10.395206 -16.904263 ## 2 -14.695206 -16.401093 ## 3 -10.395206 -12.640676 ## 4 -6.395206 -16.175096 ## 5 -6.895206 -14.966763 ## 6 -25.395206 -5.393846 注意返回表只有2列。 12.11 group_by() group_by()函数用于从数据表中根据一个变量生成统计数据。例如，在这个空气污染数据集中，你想要知道每年平均PM2.5值是多少。所以层是year，year可以从date变量里获得。在与group_by()函数结合使用时，我们经常使用summarize()函数。 这里基本的操作是通过group_by()根据一个变量分割数据表，然后再应用一个summary函数在这些子集上。 首先，你可以创造一个year变量通过as.POSIXlt()。 chicago &lt;- mutate(chicago, year = as.POSIXlt(date)$year + 1900) 现在你可以创建一个新表，新表根据year分割了原数据表。 years &lt;- group_by(chicago, year) 最终，你可以用summarize()函数计算数据表中每一年的统计数据。 summarize(years, pm25 = mean(pm25, na.rm = TRUE), o3 = max(o3tmean2, na.rm = TRUE), no2 = median(no2tmean2, na.rm = TRUE), .groups = &quot;drop&quot;) ## # A tibble: 19 x 4 ## year pm25 o3 no2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1987 NaN 63.0 23.5 ## 2 1988 NaN 61.7 24.5 ## 3 1989 NaN 59.7 26.1 ## 4 1990 NaN 52.2 22.6 ## 5 1991 NaN 63.1 21.4 ## 6 1992 NaN 50.8 24.8 ## 7 1993 NaN 44.3 25.8 ## 8 1994 NaN 52.2 28.5 ## 9 1995 NaN 66.6 27.3 ## 10 1996 NaN 58.4 26.4 ## 11 1997 NaN 56.5 25.5 ## 12 1998 18.3 50.7 24.6 ## 13 1999 18.5 57.5 24.7 ## 14 2000 16.9 55.8 23.5 ## 15 2001 16.9 51.8 25.1 ## 16 2002 15.3 54.9 22.7 ## 17 2003 15.2 56.2 24.6 ## 18 2004 14.6 44.5 23.4 ## 19 2005 16.2 58.8 22.6 summarize()返回了一个数据表，year是第一列，然后是pm25, o3, 和no2的年平均数。 在一个更复杂的例子里，我们想要知道在各个pm25分位数的区间里，o3和no2的平均是是多少。一个更简便的方法是通过回归模型，但是我们也可以用group_by()和summarize()很快的实现。 首先，我们可以创造一个pm25的类别变量，把PM25分成5等份。 qq &lt;- quantile(chicago$pm25, seq(0, 1, 0.2), na.rm = TRUE) chicago &lt;- mutate(chicago, pm25.quint = cut(pm25, qq)) 现在我们可以通过pm25.quint变量聚合数据表。 quint &lt;- group_by(chicago, pm25.quint) 最终，我们可以计算o3和no2的平均值。 summarize(quint, o3 = mean(o3tmean2, na.rm = TRUE), no2 = mean(no2tmean2, na.rm = TRUE), .groups = &quot;drop&quot;) ## # A tibble: 6 x 3 ## pm25.quint o3 no2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (1.7,8.7] 21.7 18.0 ## 2 (8.7,12.4] 20.4 22.1 ## 3 (12.4,16.7] 20.7 24.4 ## 4 (16.7,22.6] 19.9 27.3 ## 5 (22.6,61.5] 20.3 29.6 ## 6 &lt;NA&gt; 18.8 25.8 根据表，看上去pm25和o3没有什么关系，但pm25和no2正相关。更复杂的统计模型可以帮助你给出更准确的答案，但用dplyr函数也可以给出一个还不错的答案。 12.12 %&gt;% 管道操作符%&gt;%可以非常方便的连接多个dplyr函数。注意上文中每次我们想要应用多于一个函数的时候，代码会有一系列嵌套函数，非常难以阅读，例如: third(second(first(x))) 嵌套不是一个自然的方法去思考一系列操作。%&gt;%操作符允许你去从左到右连接操作。 例如: first(x) %&gt;% second %&gt;% third 拿上一章节我们计算o3和no2平均数的例子为例。我们需要做: 1. 创造一个新变量pm25.quint 2. 根据新变量分割数据表 3. 在每个分割组中计算o3和no2的平均数 这可以用如下的单个R表达式做到 mutate(chicago, pm25.quint = cut(pm25, qq)) %&gt;% group_by(pm25.quint) %&gt;% summarize(o3 = mean(o3tmean2, na.rm = TRUE), no2 = mean(no2tmean2, na.rm = TRUE), .groups = &quot;drop&quot;) ## # A tibble: 6 x 3 ## pm25.quint o3 no2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (1.7,8.7] 21.7 18.0 ## 2 (8.7,12.4] 20.4 22.1 ## 3 (12.4,16.7] 20.7 24.4 ## 4 (16.7,22.6] 19.9 27.3 ## 5 (22.6,61.5] 20.3 29.6 ## 6 &lt;NA&gt; 18.8 25.8 用这种方式我们不需要创造一堆暂时变量，也不需要用一堆嵌套函数。 注意到在上面代码中，我们把chicago数据表放在了mutate()的第一个参数上，之后我们不再需要把数据表放在group_by()和summarize()的第一个参数上。当我们用%&gt;%的时候，管道中之前元素的输出值会成为第一个参数。 另一个例子是计算每月的平均污染水平。这对查看数据是否有季节性趋势很有用。 mutate(chicago, month = as.POSIXlt(date)$mon + 1) %&gt;% group_by(month) %&gt;% summarize(pm25 = mean(pm25, na.rm = TRUE), o3 = max(o3tmean2, na.rm = TRUE), no2 = median(no2tmean2, na.rm = TRUE), .groups = &quot;drop&quot;) ## # A tibble: 12 x 4 ## month pm25 o3 no2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 17.8 28.2 25.4 ## 2 2 20.4 37.4 26.8 ## 3 3 17.4 39.0 26.8 ## 4 4 13.9 47.9 25.0 ## 5 5 14.1 52.8 24.2 ## 6 6 15.9 66.6 25.0 ## 7 7 16.6 59.5 22.4 ## 8 8 16.9 54.0 23.0 ## 9 9 15.9 57.5 24.5 ## 10 10 14.2 47.1 24.2 ## 11 11 15.2 29.5 23.6 ## 12 12 17.5 27.7 24.5 这里我们可以看到o3在冬天会少，在夏天会多。然而no2在冬天会多，在夏天会少。 12.13 总结 dplyr包提供了一些简洁的操作用于管理数据表。用这些函数我们可以做很多复杂的操作用一点点代码。特别是，我们经常可以用group_by()和summarize()函数开始我们的探索性数据分析。 当你学会了dplyr的语法后，还有一些其他好处。 dplyr可以和其他数据表后端如SQL数据库一起工作。通过DBI包有一个用于关系型数据库的SQL接口。 dplyr可以和data.table包集成用于快速的操作很大的表。 dplyr包可以简单快速的管理数据表。你很少会同时获得这样的组合。 12.14 Cited Resourses Peng, Roger D.. “13. Managing Data Frames with the dplyr package.” “R Programming for Data Science”, 2020, pp.52-64. "],["introduction-to-rstudio-in-portuguese.html", "Chapter 13 Introduction to Rstudio in Portuguese", " Chapter 13 Introduction to Rstudio in Portuguese Barbara Bettencourt For my community contribution I translated a document on an introduction to Rstudio from Princeton University to Portuguese. You can find the pdf here: https://github.com/barbarabettencourt/community_contribution/blob/main/RStudio101Translation.pdf "],["chinese-translation-of-using-arima-models-in-r.html", "Chapter 14 Chinese translation of using ARIMA models in R 14.1 ARIMA模型", " Chapter 14 Chinese translation of using ARIMA models in R Kehao Guo Source: link 14.1 ARIMA模型 指数平滑方法对于预测很有用，并且不对时间序列中的相邻数据作假设。但是，如果你想用指数平滑方法作预测区间，预测区间会需要不相关的、正态分布的、零均值化的且水平方差的预测误差。 虽然指数平滑方法不对时间序列中的相邻数据作假设，在某些情况下你可以通过考虑数据间的相关性来做出更好的预测模型。差分整合移动平均自回归模型（ARIMA）包含了一个允许异常成分中非零自相关性的统计模型。 14.1.1 差异化一组时间序列 ARIMA模型是为平稳时间序列定义的。所以如果需要处理一组非平稳时间序列，你首先需要‘差异化’时间序列直至获得一组平稳时间序列。如果你需要差异化d次来获得平稳时间序列，那你就会用ARIMA(p,d,q)模型，d代表差异次数。 你可以在R中使用‘diff()’函数来差异化时间序列。比如，1866年至1911年女士裙子的直径均值上下浮动且不平稳： 我们可以输入以下指令来差异化一次时间序列并且对差异化后的系列作图： skirtsseriesdiff1 &lt;- diff(skirtsseries, differences=1) plot.ts(skirtsseriesdiff1) 以上的一次差异化时间序列不像是有稳定均值的。所以，我们可以作两次差异化以检测结果是否是稳定时间序列： skirtsseriesdiff2 &lt;- diff(skirtsseries, differences=2) plot.ts(skirtsseriesdiff2) 以上的二次差异化时间序列看上去是有稳定均值和方差的，因为序列的水平大致保持一致。所以，我们需要作二次差异化以得到裙子直径的稳定平稳时间序列。 如果你需要对原数据作d次差异化来得到平稳时间序列，那你可以使用ARIMA(p,d,q)模型，d代表差异化次数。在女裙直径的时间序列的例子中，我们可以使用ARIMA(p,2,q)模型，因为差异化次数应为2。下一步我们需要求得p与q的值。 另一例子是英格兰国王死亡岁数的时间序列： 从以上图表可以看出，该时间序列没有平稳均值。我们输入以下代码以计算一次差异化的时间序列并作图： kingtimeseriesdiff1 &lt;- diff(kingstimeseries, differences=1) plot.ts(kingtimeseriesdiff1) 一次差异化的该时间序列像是有平稳均值和方差的，所以ARIMA(p,1,q)应是一个合适的模型。一次差异化祛除了英格兰国王死亡岁数的时间序列的趋势成分，只留下了不规则成分。我们现在可以检查相邻的不规则成分间是否存在相关性；如果有，这会帮助我们找出一个预测国王死亡岁数的模型。 14.1.2 选择ARIMA候选模型 如果时间序列是平稳的，或通过差异化d次得到一个平稳的时间序列，那下一步则为选择一个合适的ARIMA模型，也就是找出合适的p和q的值。为此还需要检验平稳时间序列的相关图和部分相关图。 我们需要R中的“acf()”和“pacf()”函数来分别作出相关图和部分相关图。为了得到自相关和偏自相关的实际值，我们设“acf()”和“pacf()”函数中“plot=FALSE”。 14.1.2.1 例：英格兰国王死亡岁数 比如，要对一次差异化的英格兰国王死亡岁数的时间序列的1-20滞后作相关图，并得到自相关的值，我们输入： acf(kingtimeseriesdiff1, lag.max=20) # plot a correlogram acf(kingtimeseriesdiff1, lag.max=20, plot=FALSE) # get the autocorrelation values Autocorrelations of series &#39;kingtimeseriesdiff1&#39;, by lag 0 1 2 3 4 5 6 7 8 9 10 1.000 -0.360 -0.162 -0.050 0.227 -0.042 -0.181 0.095 0.064 -0.116 -0.071 11 12 13 14 15 16 17 18 19 20 0.206 -0.017 -0.212 0.130 0.114 -0.009 -0.192 0.072 0.113 -0.093 我们从相关图中看到滞后1的自相关（-0.360）超过了显著性界限，但其他滞后的自相关都没有超过显著性界限。 要对一次差异化的英格兰国王死亡岁数的时间序列的1-20滞后作部分相关图，并得到偏自相关的值，我们使用“pacf()”函数并输入： pacf(kingtimeseriesdiff1, lag.max=20) # plot a partial correlogram pacf(kingtimeseriesdiff1, lag.max=20, plot=FALSE) # get the partial autocorrelation values Partial autocorrelations of series &#39;kingtimeseriesdiff1&#39;, by lag 1 2 3 4 5 6 7 8 9 10 11 -0.360 -0.335 -0.321 0.005 0.025 -0.144 -0.022 -0.007 -0.143 -0.167 0.065 12 13 14 15 16 17 18 19 20 0.034 -0.161 0.036 0.066 0.081 -0.005 -0.027 -0.006 -0.037 该部分相关图显示偏自相关值在1、2和3滞后时超过了显著性界限，是负的，并且随着滞后增加而大小缓慢减少。在滞后3后偏自相关值减少至0。 由此，以下为对一次差异化时间序列而言可能的ARMA(自回归滑动平均)模型： ARMA(3,0)模型：一个p=3的自回归模型，因为部分相关途中pacf在滞后3后为零，且自相关图中减少至0。 ARMA(0,1)模型：一个q=1的滑动平均模型，因为自回归图在滞后1后为零，且偏自回归图减少至0。 ARMA(p,q)模型：一个p和q的值大于0的混合模型，应为自回归图和偏自回归图减少至0。 我们用简约原则来决定哪一个是最好的模型：我们假设参数最少的模型是最好的。以上模型分别有3个，1个和至少2个参数。所以，ARMA(0,1)被认定为最佳模型。 ARMA(0,1)模型，即MA(1)模型，是一阶的滑动平均模型。该模型可被写作X_t - mu = Z_t - (theta * Z_t-1),X_t为平稳时间序列（英格兰国王死亡岁数的一次差异化序列），mu为时间序列的均值，Z_t是零均值和方差不变的白噪音，theta是可被预测的参数。 滑动平均模型通常被用来对相邻观测值间有短期依赖的时间序列建模。直观上，MA模型可以被用作对英格兰国王死亡岁数的时间序列中的不规则成分建模，因为我们可以说一个国王死亡岁数可能和他后面一两个继任者的死亡岁数有关联，但和他更后面的继任者的死亡岁数可能就没什么关系了。 因为ARMA(0,1)模型（p=0, q=1）被认定为该一次差异化时间序列的最佳模型候选，所以原时间序列可以适用于ARIMA(0,1,1)模型（p=0, d=1, q=1）。 14.1.3 使用ARIMA模型进行预测 一旦你选择了最佳ARIMA(p,d,q)模型候选，你就可以预测该模型的参数，并且为时间序列的未来值进行预测。 在R中你可以用“arima()”函数来估计一个ARIMA(p,d,q)模型的参数。 14.1.3.1 例：英格兰国王死亡岁数 如上，ARIMA(0,1,1)模型对该时间序列来说似乎是一个可行的模型。你可以用“arima()”函数的“order”选项明确ARIMA模型的p,d和q的值。要对一组时间序列拟合一个ARIMA(p,d,q)模型，我们输入： kingstimeseriesarima &lt;- arima(kingstimeseries, order=c(0,1,1)) # fit an ARIMA(0,1,1) model kingstimeseriesarima ARIMA(0,1,1) Coefficients: ma1 -0.7218 s.e. 0.1208 sigma^2 estimated as 230.4: log likelihood = -170.06 AIC = 344.13 AICc = 344.44 BIC = 347.56 如上所说，如果我们对时间序列拟合一个ARIMA(0,1,1)模型，我们其实也是对一次差异化的时间序列拟合一个ARMA(0,1)模型。ARMA(0,1)模型可被写作X_t - mu = Z_t - (theta * Z_t-1)，theta是要被估计的参数。从以上的R的“arima()”函数的输出来看，theta的估值（R输出中的‘ma1’）为-0.7218。 然后我们可以用R的“forecast”包中的“forcast.Arima()”函数对时间序列的未来值进行预测。比如，要对接下来5个英格兰国王的死亡岁数进行预测，我们输入： library(&quot;forecast&quot;) # load the &quot;forecast&quot; R library kingstimeseriesforecasts &lt;- forecast.Arima(kingstimeseriesarima, h=5) kingstimeseriesforecasts Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 43 67.75063 48.29647 87.20479 37.99806 97.50319 44 67.75063 47.55748 87.94377 36.86788 98.63338 45 67.75063 46.84460 88.65665 35.77762 99.72363 46 67.75063 46.15524 89.34601 34.72333 100.77792 47 67.75063 45.48722 90.01404 33.70168 101.79958 原时间序列包括了42位英格兰国王的死亡岁数。forcast.Arima()给出了接下来5位(第43到第48位国王)的死亡岁数，以及相应的80%和95%预测区间。第42位国王的死亡岁数为56岁，而ARIMA模型预测下五位国王的死亡岁数为67.8岁。 我们可以将前42位国王的实际死亡岁数和用ARIMA(0,1,1)模型预测的后5位国王的死亡岁数作图，输入： plot.forecast(kingstimeseriesforecasts) 如果是指数平滑模型，那最好能调查ARIMA模型的预测误差是否是正态分布的、零均值化的且水平方差的，且是否存在相邻预测误差间的相关。 比如，我们可以对ARIMA(0,1,1)模型对英格兰国王死亡岁数的预测误差作相关图，并且对1-20滞后作LB检验，输入： acf(kingstimeseriesforecasts$residuals, lag.max=20) Box.test(kingstimeseriesforecasts$residuals, lag=20, type=&quot;Ljung-Box&quot;) Box-Ljung test data: kingstimeseriesforecasts$residuals X-squared = 13.5844, df = 20, p-value = 0.851 由于相关图中没有1-20滞后的样本自相关值超过显著性界限，且LB测试的p值为0.9，我们可以得出有极少证据支持1-20滞后的非零自相关的结论。 要调查预测误差是否是正态分布的，零均值的，且水平方差的，我们可以作一张预测误差的时间图和(覆盖正态曲线的)柱状图： plot.ts(kingstimeseriesforecasts$residuals) # make time plot of forecast errors plotForecastErrors(kingstimeseriesforecasts$residuals) # make a histogram 样本预测误差的时间图显示了预测误差的方差大约保持一致。柱状图显示了预测误差大约是正态分布的且均值约等于0。所以，可以说预测误差是正态分布的，零均值的，且水平方差的。 因为相邻的预测误差不像是相关的，且预测误差是正态分布的，零均值的，且水平方差的，ARIMA(0,1,1)模型的确提供了一个合适的预测英格兰国王死亡岁数的模型。 "],["chinese-translation-of-broom-package.html", "Chapter 15 Chinese translation of broom package 15.1 序言 15.2 整理函数 15.3 其他案例 15.4 规则", " Chapter 15 Chinese translation of broom package Wenjie Wei and Yinqiu Feng library(broom) 15.1 序言 broom包可以将R语言中内置函数（例如lm，nls或t.test）的混乱输出转换为整齐的tibbel(一种R特有的数据框）。  broom 是一种可以将不整齐的估测结果调整为我们想要的整齐数据的工具。它以3种S3类函数为中心，每种方法都采用R统计函数（lm,t.test,nls等等）产生的对象并转换结果至tibble。设计broom是为了和Hadley的dplyr包一起使用。  broom应与reshape2和tidyr之类的包区分开来,这些包会将数据帧重新排列和重塑为不同的形式。这些软件包在整理好的的数据分析中执行关键任务，但专注于将一种特定格式的数据帧转换为另一种格式。相反，broom被设计为采用非表格数据格式的格式，并将其转换为整洁的数据框。  整理模型输出不是一门精准的学科，它基于数据科学家对于从整洁的数据分析中得出的各种值（例如，估计值，测试统计数据和p值）进行判断的期望。您可能会丢失所需的原始对象中的某些信息，或者保留了比所需更多的信息。如果您认为应该更改模型的整理输出，或者缺少所需的S3类的整理功能，欢迎提出问题或需求。  15.2 整理函数 这个软件包提供了三种S3类函数，可以进行三种不同的整理：  tidy：绘制tibble，总结该模型的统计结果。这包括回归中每个项的系数和p值，聚类应用中的每个集群信息或多测试功能的每个测试信息。  augment：在建模的原始数据中增加列数。这包括预测，残差和聚类分配。 glance：建立一行关于模型的精简总结。它通常包含诸如R方，调整R方和残余标准误差之类的值，这些值会针对整个模型计算一次。  请注意，某些类可能只定义了其中一种或两种方法。  作为说明性示例，我们来对R内置的mtcars数据集进行线性拟合。  lmfit &lt;- lm(mpg ~ wt, mtcars) lmfit ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Coefficients: ## (Intercept) wt ## 37.285 -5.344 summary(lmfit) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 如果您只想看下总结数据，到这就已足够。但是，将其转换为包含所有相同信息的表格数据，以便您可以将其与其他模型结合或进行进一步的分析并非易事。您必须执行coef（summary（lmfit））以获得系数矩阵，术语仍存储在行名称中，并且列名称与其他包不一致（例如与p.value相比，Pr（&gt; | t |）不一致）。 现在，您可以在fit上用broom包里的tidy函数： tidy(lmfit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 37.3 1.88 19.9 8.24e-19 ## 2 wt -5.34 0.559 -9.56 1.29e-10 这为您提供了表格数据表示形式。请注意，行名称已移至称为term的列中，并且这些列名称简单且一致（可以使用$进行访问）。  您可能对回归中每个原始点的拟合值和残差感兴趣，而不是查看系数。为此，请使用augment函数，它会使用来自模型的信息来扩充原始数据： augment(lmfit) ## # A tibble: 32 x 9 ## .rownames mpg wt .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 2.62 23.3 -2.28 0.0433 3.07 1.33e-2 -0.766 ## 2 Mazda RX4 Wag 21 2.88 21.9 -0.920 0.0352 3.09 1.72e-3 -0.307 ## 3 Datsun 710 22.8 2.32 24.9 -2.09 0.0584 3.07 1.54e-2 -0.706 ## 4 Hornet 4 Drive 21.4 3.22 20.1 1.30 0.0313 3.09 3.02e-3 0.433 ## 5 Hornet Sportabo… 18.7 3.44 18.9 -0.200 0.0329 3.10 7.60e-5 -0.0668 ## 6 Valiant 18.1 3.46 18.8 -0.693 0.0332 3.10 9.21e-4 -0.231 ## 7 Duster 360 14.3 3.57 18.2 -3.91 0.0354 3.01 3.13e-2 -1.31 ## 8 Merc 240D 24.4 3.19 20.2 4.16 0.0313 3.00 3.11e-2 1.39 ## 9 Merc 230 22.8 3.15 20.5 2.35 0.0314 3.07 9.96e-3 0.784 ## 10 Merc 280 19.2 3.44 18.9 0.300 0.0329 3.10 1.71e-4 0.100 ## # … with 22 more rows 注意，每个新列都以a开头。 （以避免覆盖任何原始列）  最后，我们为整个回归模型计算了一些汇总统计量，例如R方和F统计量。这些可以通过glance函数访问： glance(lmfit) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.753 0.745 3.05 91.4 1.29e-10 1 -80.0 166. 170. ## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; 关于tidy,augment和glance函数的区别，可以指路k-means vignette https://www.tidymodels.org/learn/statistics/k-means/ 15.3 其他案例 15.3.1 广义线性模型和非线性模型 这些功能同样适用于glm的输出结果： glmfit &lt;- glm(am ~ wt, mtcars, family = &quot;binomial&quot;) tidy(glmfit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.0 4.51 2.67 0.00759 ## 2 wt -4.02 1.44 -2.80 0.00509 augment(glmfit) ## # A tibble: 32 x 9 ## .rownames am wt .fitted .resid .std.resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 1 2.62 1.50 0.635 0.680 0.126 0.803 0.0184 ## 2 Mazda RX4 Wag 1 2.88 0.471 0.985 1.04 0.108 0.790 0.0424 ## 3 Datsun 710 1 2.32 2.70 0.360 0.379 0.0963 0.810 0.00394 ## 4 Hornet 4 Drive 0 3.22 -0.897 -0.827 -0.860 0.0744 0.797 0.0177 ## 5 Hornet Sportabout 0 3.44 -1.80 -0.553 -0.572 0.0681 0.806 0.00647 ## 6 Valiant 0 3.46 -1.88 -0.532 -0.551 0.0674 0.807 0.00590 ## 7 Duster 360 0 3.57 -2.33 -0.432 -0.446 0.0625 0.809 0.00348 ## 8 Merc 240D 0 3.19 -0.796 -0.863 -0.897 0.0755 0.796 0.0199 ## 9 Merc 230 0 3.15 -0.635 -0.922 -0.960 0.0776 0.793 0.0242 ## 10 Merc 280 0 3.44 -1.80 -0.553 -0.572 0.0681 0.806 0.00647 ## # … with 22 more rows glance(glmfit) ## # A tibble: 1 x 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 43.2 31 -9.59 23.2 26.1 19.2 30 32 注意，通过glance计算的统计信息对于glm模型与lm模型的不同（例如，偏差而不是R方）  这些函数还可以用于其他拟合，例如非线性模型（nls）： nlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0)) tidy(nlsfit) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 k 45.8 4.25 10.8 7.64e-12 ## 2 b 4.39 1.54 2.85 7.74e- 3 augment(nlsfit, mtcars) ## # A tibble: 32 x 14 ## .rownames mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 … 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 D… 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Spo… 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 Duster 360 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 Merc 240D 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 Merc 230 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 Merc 280 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows, and 2 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt; glance(nlsfit) ## # A tibble: 1 x 9 ## sigma isConv finTol logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2.77 TRUE 0.0000000288 -77.0 160. 164. 231. 30 32 15.3.2 假设检验 tidy函数也可以应用于htest对象，例如最近很火的内置函数（如t.test，cor.test和wilcox.test）输出的对象。 tt &lt;- t.test(wt ~ am, mtcars) tidy(tt) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.36 3.77 2.41 5.49 0.00000627 29.2 0.853 1.86 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 在一些情况下，列数可能较少（例如，没有置信区间）： wt &lt;- wilcox.test(wt ~ am, mtcars) tidy(wt) ## # A tibble: 1 x 4 ## statistic p.value method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 230. 0.0000435 Wilcoxon rank sum test with continuity correc… two.sided 由于tidy输出结果已经只有一行，因此glance返回相同的输出： glance(tt) ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.36 3.77 2.41 5.49 0.00000627 29.2 0.853 1.86 ## # … with 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; glance(wt) ## # A tibble: 1 x 4 ## statistic p.value method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 230. 0.0000435 Wilcoxon rank sum test with continuity correc… two.sided 因为没有必要，我们仅针对卡方检验定义了扩充方法，对于其他检验，假设检验产生关于每个初始数据点的输出，则没有其他意义。 chit &lt;- chisq.test(xtabs(Freq ~ Sex + Class, data = as.data.frame(Titanic))) tidy(chit) ## # A tibble: 1 x 4 ## statistic p.value parameter method ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 350. 1.56e-75 3 Pearson&#39;s Chi-squared test augment(chit) ## # A tibble: 8 x 9 ## Sex Class .observed .prop .row.prop .col.prop .expected .resid .std.resid ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Male 1st 180 0.0818 0.104 0.554 256. -4.73 -11.1 ## 2 Female 1st 145 0.0659 0.309 0.446 69.4 9.07 11.1 ## 3 Male 2nd 179 0.0813 0.103 0.628 224. -3.02 -6.99 ## 4 Female 2nd 106 0.0482 0.226 0.372 60.9 5.79 6.99 ## 5 Male 3rd 510 0.232 0.295 0.722 555. -1.92 -5.04 ## 6 Female 3rd 196 0.0891 0.417 0.278 151. 3.68 5.04 ## 7 Male Crew 862 0.392 0.498 0.974 696. 6.29 17.6 ## 8 Female Crew 23 0.0104 0.0489 0.0260 189. -12.1 -17.6 15.4 规则 为了保持一致性，我们制定了一些有关返回数据结构的规则。 15.4.1 全部函数 tidy, augment 和 glance的输出一定是tibble。 所有的输出都没有行名。这样就可以确保用户将输出的tibble与整理好的数据组合在一起，同时不用担心丢失信息（因为R语言里行名不能有重复）。 有一些列的名称保持一致，以便可以在不同模型之间进行组合，从而输出我们所期望的内容（避免R每次都会问“是pval还是PValue？”）。下面这些示例没有包括所有可能的列名，也不是所有的输出都包含所有这些列，有的输出甚至不包含任何这些列。仅供参考。 15.4.2 tidy函数 Tidy的每一行输出都指代一些定义明确的概念，例如一个线性回归公式，一项测试或一个类别。虽然在不同模型里的含义不同，但可以比较明显地看出。每行输出并不能指代初始数据点（这个可以用augment函数）。 常见的列名称包括： • Term：指正在估计的回归或模型中的术语。 • p.value：用来匹配R的内置统计信息包中的功能（代替pvalue，PValue或pval） • statistic：一种测试统计信息，通常是用于计算p值的统计信息。将这些内容组合到许多子组中来执行（例如自举假设测试法） • estimate 估计值 • conf.low ：estimate的置信区间的下限 • conf.high ：estimate的置信区间的上限 • df：自由度 15.4.3 augment函数 augment(model, data)将列添加到原始数据。 • 如果缺少data参数，可以用augment函数从模型中重建数据（有的时候实现不了，而且通常不会包含模型中未使用的列）。 augment输出中的每一行要与原始数据中的对应行匹配。如果原始数据有行名，augment需要将其转换成名为.rownames的列。 新添加的列名以 . 开头。以避免覆盖原始数据中的列。 常见的列名称包括： • .fitted 拟合值：与数据相同规模的预测值。 • .resid 残差：实际y值减去拟合值 • .cluster 集群：群集分配 15.4.4 glance函数 glance只能返回一个单行tibble。 • 唯一的例外是glance（NULL）返回一个空的tibble。 我们不会在输出里添加建模函数的参数。例如，glm的输出不需要包含family这个词，因为用户调用glm而不是建模函数本身。 常见的列名称包括： • r.squared ：由模型解释的方差 • adj.r.squared：根据自由度调整方差 • sigma：残差的估计方差的平方根 "],["translation-of-visualizations-that-really-work.html", "Chapter 16 Translation of Visualizations That Really Work", " Chapter 16 Translation of Visualizations That Really Work Jing Lu I wished to contribute to the community by translating the following paper into Chinese.this paper was post in Harvard Business Review the titil is “Visualizations That Really Work”. I hope its translation would be helpful for our classmates as well. The original paper could be found on https://hbr.org/2016/06/visualizations-that-really-work The translation github repo counld be found on https://github.com/JFLR12/STA5702_cc "],["chinese-translation-of-bar-chart-and-cleveland-dot-plot.html", "Chapter 17 Chinese Translation of Bar Chart and Cleveland Dot Plot 17.1 图表：条形图 17.2 图表：克利夫兰点图", " Chapter 17 Chinese Translation of Bar Chart and Cleveland Dot Plot Changhao He Source: https://edav.info/bar.html https://edav.info/cleveland.html 17.1 图表：条形图 17.1.1 综述 这一章节包含了如何制作条形图。 17.1.2 太长不看 我想要一个漂亮的例子。不是明天，不是在早饭之后，而是现在！ 这里就是一个显示了皇家游轮泰坦尼克号乘客生还率的条形图： 接下来是代码： library(datasets) # 数据 library(ggplot2) # 绘图工具 library(dplyr) # 数据处理 # 结合儿童和成年人的数据 ship_grouped &lt;- as.data.frame(Titanic) %&gt;% group_by(Class, Sex, Survived) %&gt;% summarise(Total = sum(Freq)) ggplot(ship_grouped, aes(x = Survived, y = Total, fill = Sex)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + geom_text(aes(label = Total), position = position_dodge(width = 0.9), vjust = -0.4, color = &quot;grey68&quot;) + facet_wrap(~Class) + # 排版 ylim(0, 750) + ggtitle(&quot;Don&#39;t Be A Crew Member On The Titanic&quot;, subtitle = &quot;Survival Rates of Titanic Passengers by Class and Gender&quot;) + scale_fill_manual(values = c(&quot;#b2df8a&quot;, &quot;#a6cee3&quot;)) + labs(y = &quot;Passenger Count&quot;, caption = &quot;Source: titanic::titanic_train&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) 想要知道关于这个数据的更多信息，在控制台输入?datasets::Titanic。 17.1.3 简单的例子 我的眼睛比我的胃还大，请更加简单一点！ 让我们来用HairEyeColor这个数据。刚开始，让我们来看女性发色的不同分类。 colors &lt;- as.data.frame(HairEyeColor) # 通过dplyr只获取女性发色 colors_female_hair &lt;- colors %&gt;% filter(Sex == &quot;Female&quot;) %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) # 浏览数据 head(colors_female_hair) ## # A tibble: 4 x 2 ## Hair Total ## &lt;fct&gt; &lt;dbl&gt; ## 1 Black 52 ## 2 Brown 143 ## 3 Red 37 ## 4 Blond 81 让我们根据这个数据来做一些图形。 17.1.3.1 用base R做条形图 barplot(colors_female_hair[[&quot;Total&quot;]], names.arg = colors_female_hair[[&quot;Hair&quot;]], main = &quot;Bar Graph Using Base R&quot;) 我们建议用Base R来做那些只是为了你自己的简单条形图。和其他所有的Base R一样，它设置起来很简单。注意：Base R需要一个矢量或者矩阵,因此在条形图调用时需要用双中括号（获取每一列作为列表）。 17.1.3.2 用ggplot2做条形图 library(ggplot2) # 绘图工具 ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) ggplot2中的条形图非常简单。你输出一个数据框，让它知道你想要哪一部分映射到不同的轴上。注意：在这个例子里，我们有一个含有数值的表，并想要通过独立的柱的高度来画出这些数值。因此，我们规定y轴为Total这一列，但我们也需要规定stat = \"identity在geom_bar()里，从而让它知道怎么正确的绘图。通常你会有每一行都是一个观测值的数据集，同时你想要把这些观测值通过柱来分组。在这种情况下，y轴和stat = \"identity不需要特别规定。 17.1.4 理论 想要知道绘制分类数据的更多信息，阅读教科书第四章。 17.1.5 何时使用 条形图最好用在分类数据。通常你会有一个你想要把它分成不同组的因子数据的集合。 17.1.6 注意事项 17.1.6.1 不要用于连续数据 如果你发现你的条形图看起来不太对，请确保你的数据是分类的而不是连续的。如果你想通过柱来绘制连续数据，那是直方图的功能。 17.1.7 修改 这些修改假设你在使用ggplot2。 17.1.7.1 旋转条形图 为了旋转方向，添加coord_flip()就可以了： ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) + coord_flip() 17.1.7.2 对柱子重新排序 在base R和ggplot2里对于字符型数据柱是通过首字母顺序排列的，对于因子型数据柱是通过层次来排列的。但是，因为因子型数据的层次的默认顺序是首字母顺序，所以在两种情况下柱都会通过首字母顺序排列。请看这个教程来获得关于条形图里的柱应该怎么排序细致的解释，和如何通过forcats程序包来帮助你完成重新排序。 17.1.7.3 封装型分面 你可以通过facet_wrap()把图表分成几个小图。（别忘记了腭化符号，~）： ggplot(colors, aes(x = Sex, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Hair) 17.1.8 外部资源 Cookbook for R: 讨论了关于因子层次的重新排序。 DataCamp Exercise:一些用ggplot2做条形图的一些简单练习。 ggplot2 cheatsheet:在你身边总是好的。 17.2 图表：克利夫兰点图 这个页面是一项正在进行的工作，我们感谢任何你可能有的贡献。如果你想帮助改进这个页面，请考虑对我们的版本库做出贡献。 17.2.1 综述 这一章节包含了如何做克利夫兰点图。克利夫兰点图是简单条形图的一个很好的替代品，特别是如果你有不止一些的项的时候。不需要太多的项就会让条形图看起来很凌乱。在相同的空间里，更多的数值可以被包含在一个点图里面，而且点图也更容易阅读。R 有一个内置的基本函数，dotchart()，但是因为它是一个画起来很简单的图形，在ggplot2或者base里面“从零开始”让我们有更多的定制选项。 代码： # 创造一个能被再使用的点图主题。 theme_dotplot &lt;- theme_bw(14) + theme(axis.text.y = element_text(size = rel(.75)), axis.ticks.y = element_blank(), axis.title.x = element_text(size = rel(.75)), panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5), panel.grid.minor.x = element_blank()) # 把行的名字变成数据框的一列。 df &lt;- swiss %&gt;% tibble::rownames_to_column(&quot;Province&quot;) # 创造一个图表。 ggplot(df, aes(x = Fertility, y = reorder(Province, Fertility))) + geom_point(color = &quot;blue&quot;) + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, 10)) + theme_dotplot + xlab(&quot;\\nannual live births per 1,000 women aged 15-44&quot;) + ylab(&quot;French-speaking provinces\\n&quot;) + ggtitle(&quot;Standardized Fertility Measure\\nSwitzerland, 1888&quot;) 17.2.2 多点图 在这个例子里面我们将用2010年纽约城公立学校的一个样本的SAT平均分数的数据： df &lt;- read_csv(&quot;data/SAT2010.csv&quot;, na = &quot;s&quot;) set.seed(5293) tidydf &lt;- df %&gt;% filter(!is.na(`Critical Reading Mean`)) %&gt;% sample_n(20) %&gt;% rename(Reading = &quot;Critical Reading Mean&quot;, Math = &quot;Mathematics Mean&quot;, Writing = &quot;Writing Mean&quot;) %&gt;% gather(key = &quot;Test&quot;, value = &quot;Mean&quot;, &quot;Reading&quot;, &quot;Math&quot;, &quot;Writing&quot;) ggplot(tidydf, aes(Mean, `School Name`, color = Test)) + geom_point() + ggtitle(&quot;Schools are sorted alphabetically&quot;, sub = &quot;not the best option&quot;) + ylab(&quot;&quot;) + theme_dotplot 注意School Name是按照因子层次排序的，默认是首字母顺序。一个更好的选择是通过一个Test中的层次集来排序。通常最好是尝试通过不同的层次集来排序，然后观察出现的模式。 为了执行双重排序，就是先通过Test然后再通过Mean来排列School Name，我们会使用forcats::fct_reorder2()。这个函数会通过对两个矢量的排序来对.f(一个因子或者字符矢量)进行排序。对于这种图形，.x是彩色的点所表示的变量，.y是投射到y轴的连续变量。 假设我们希望通过阅读的平均分来对学校进行排序。我们可以在我们对Mean进行排序时，通过限制Test变量到“阅读”来完成该排序： ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test==&quot;Reading&quot;, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Reading mean&quot;) + ylab(&quot;&quot;) + theme_dotplot （非常感谢Zeyu Qiu对于直接在因子层次上对.x进行设置的小建议，这是一个更好的方式相较于我们下面讨论的，顺应fct_reorder2()的默认对因子层次进行重新排列。） 虽然这是首选方法，但有些情况下直接指明你希望通过对于第一个变量（Test）的第一个因子层次或者最后一个因子层次排序,而不用把它拼写出来会更加简单。 如果没有特别指明一个变量，fct_reorder2()默认会通过对.x的最后一个因子层组进行排序，这种情况下“写作”是Test的最后一个因子层次。 ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Writing mean&quot;) + ylab(&quot;&quot;) + theme_dotplot 如果你想要通过对.x的第一个因子层次进行排序，在这里是“数学”，你需要forcats的开发版本，你可以通过：devtools::install_github(\"tidyverse/forcats\")来安装。 把默认排序函数last2()改为first2()： ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .fun = first2, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Math mean&quot;) + ylab(&quot;&quot;) + theme_dotplot "],["translation-of-critical-covid-data-visualizations-paper.html", "Chapter 18 Translation of critical covid data visualizations paper", " Chapter 18 Translation of critical covid data visualizations paper Siqi Chen and Yaoling Wang My partner and I hoped to contribute to the community by translating the following paper into Chinese. This paper was first published on July 27, 2020 by Emily Bowe, Shannon Mattern and Erin Simmons. Translated paper was aimed to help people understand the importance of data visualization and how it exists in our daily lives, especially in this global emergency period. For instance, during the COVID-19 period, data visualization could be used to inform people of the symptoms of pandemics, and obtain the comuse time by dance models. In this way, people would stay home and stay safe. Meanwhile, we will also need to be aware of the limitations of data visualization and its potential subjectiveness when taking advantage of accessible data. Thinking critically about resources we get is more important than the way they are presented. My partner and I were inspired by this paper, not only because it was closely related to our recent circumstances, but also because it promotes critical thinking when using data visualization. This paper has been a useful resource for us, and we hope this translation would be helpful for our classmates as well. The original paper could be found on SAGE Journals. Our translation could be found on GitHub Repo. SAGE Journals link: https://journals.sagepub.com/doi/full/10.1177/2053951720939236 GitHub Repo link: https://github.com/siqichen99/STAT5702_CC_Group12.git "],["translate-ibm-website-about-design-language-into-chinese.html", "Chapter 19 Translate IBM Website about Design Language into Chinese", " Chapter 19 Translate IBM Website about Design Language into Chinese By Zhongsheng Chen The original passage is from the IBM website: https://www.ibm.com/design/language/data-visualization/overview/ The translated version in Chinese can be accessed in: https://github.com/zc2583/cc21/blob/main/5702%20CC.pdf "],["chinese-translation-exploratory-data-analysis-and-data-visualization.html", "Chapter 20 Chinese Translation Exploratory Data Analysis and Data Visualization", " Chapter 20 Chinese Translation Exploratory Data Analysis and Data Visualization Translators: Jiaxi Liu &amp; Ruoxi Li Original Author:Chong-ho Yu, Ph.Ds. You can find the pdf here: https://github.com/Ruoxiiiii/Translation-Script-Exploratory-data-analysis-and-Data-visualization/blob/main/EDA%26Visualization%20Translation%20Script.pdf 1.EDA: 探索侦察型工作 这是对探索性数据分析(EDA)和数据可视化的简要介绍。您将 遇到几个不熟悉的术语和图表，但是此时您不必完全理解它们。 本文的目的是让您知道哪些工具可用以及可以做什么。 EDA的 原理和特定技术将在进一步的阅读中介绍。 当有人声称他们的方法是探索性的时，实际上他们的意思是他们 不知道自己在做什么。不幸的是，经常以EDA的名义进行拙劣的 研究。在数据收集中，由于没有明确定义研究问题并且没有识别 出变量，研究人员用数百页的调查资料充斥了调查对象。确实， EDA不需要预先确定的假设就可以进行测试，但是它并不能证明 没有研究问题和定义不明确的变量，也无法证明每次尝试直到获 得显着的p值(p-hacking)(Jebb) ，Parrigon和Woo，2017年)。 EDA这门技术是丰富且结构合理的。探索性数据分析是对确认数 据分析(CDA)的补充，由约翰·图基(John Tukey)(1977，1980)建 立。 Tukey通常将EDA与侦探工作联系起来。在EDA中，研究人员的作用是以尽可能多的方式探索 数据，直到出现合理的数据“故事”为止。侦探不会收集任何信息。相反，他收集了与案件核心问题 有关的证据和线索。因此，从现在开始，您可以称我为“侦探俞”。 2.EDA的要素 Velleman和Hoaglin(1981)概述了探索性数据分析的四个基本要素，如下所示: ● 数据可视化 ● 残差分析 ● 数据转换或重新表达 ● 稳定性分析过程 数据可视化 数据可视化的基本原理是:“一张图片值一千字。”从图片检测数据模式比从数字输出检测数据模 式容易。一般而言，研究目标有六大类。所有这些人都可以利用制图技术来加深我们对数据的理 解: ● 发现异常值 ● 区分集群 ● 检查分配和其他假设 ● 检查关系 ● 比较均值差异 ● 观察基于时间的过程 以下是一些示例。这些图的解释非常复杂。只是获得可视化的想法，不要过多的在意细节。 发现异常值 在直方图等一维图表中很容易发现单变量离群值。在多变量情况下，旋转图很有用。请观看此动 画演示。 区分集群 通过可视化，我们可以将变量或主题聚类。此示例显示了如何使用笔刷对主题进行聚类以帮助进 行回归分析。请观看此动画演示。 检查分布和其他假设 许多参数测试都需要数据规范性。研究人员可以使用简 单的直方图来检查分布。一种更复杂的方法是使用正态 概率图检查数据。如果数据完全正常，则图形应显示对角 直线。与直线的偏离表示不正常的程度。 检查关系 当存在交互作用时，回归线在其他变量的所有级别上均 不一致。移动的网格表面描述了这种变化。如果动画使您 烦恼，请按浏览器上的“停止”按钮以冻结动画。这种类型 的数据可视化可以在Mathematica和DataDesk中执行。 比较小组差异 通常使用参数检验(例如t检验或F检验)来比 较均值差异。但是，可以使用图形来补充测试 统计信息。一个典型的例子是使用单元均值图 来检查主要作用和相互作用作用。用于比较差 异的高级图形示例包括菱形图和杠杆图。 残差分析 译文: EDA遵循数据=拟合+残差 或者 数据=模型+误差的模 型。拟合或模型是数据的期望值。残差或误差是偏 离该预期值的值。通过检查残差，研究人员可以评 估模型的适当性。一个简单的例子可以在回归分析 中找到。左侧的散点图显示了回归模型中的残差。 今天，不难理解为什么我们应该检查残差以检查数 据对模型的拟合程度。但是，“剩余”是一个现代概 念。几个世纪前，即使是训练有素的科学家对”残差” 这一概念的感受也很弱。 *不幸的是，目前，这个问 题仍然存在于一些倾向于将建模视为理所当然而忽 略残差的研究人员中。 过去，这种迭代过程是由分析人员手动执行的，例如 2-way fit方法。今天，机器学习算法可自动执行此过 程。 Boosting，也称为boosted树，是自动迭代的一 个很好的例子。 5.数据转换和重新表达 数据转换发生在我们的日常生活中:将美元转换为加元，将5分制的GPA转换为4分制的GPA。但是 ，这些示例属于线性变换，因此不影响数据的分布。在EDA中，通常使用非线性变换，从而改变数 据模式。数据重新表达本质上是探索性的，因为在进行转换之前，研究人员永远不知道哪种重新 表达方法可以达到理想的结果。 数据转换有四个主要目标: ● 标准化分布 :非常规数据违反了参数测试的假设，因此建议进行转换。常见的误解是将原 始分数转换为z分数会产生正态分布。实际上，原始到z的转换是线性转换。下图显示了从 原始到z的转换后，z得分的分布形状仍然类似于原始得分。适当的过程应该是自然对数变 换或逆概率变换。 ● 稳定差异:方差不相等的数据也不利于参数测试。方差稳定化变换的一个典型示例是平方 根变换:y * = sqrt(y)。 ● 线性化趋势: 回归分析需要线性假设。当数据显示曲线关系时，研究人员可以应用非线性 回归分析，也可以通过线性化变换对数据进行拉直。对数转换是后者的典型示例。 ● 正交化共线变量: 在多元回归中，预测变量之间缺乏独立性会使模型不稳定。就超空间而 言，代表这些变量的向量是非正交的。为了纠正这种情况，可以使用Gram–Schmidt过程或 其他转换技术，通过将分数居中来使变量正交。 但是，每种统计程序都有其局限性，应谨慎使用。数据转换也不例外。 Osborne(2002)建议应适 当使用数据转换。许多转换通过更改数据点之间的间距来减少非正态性，但是这在数据解释中提出了问题。如果正确完成了转换，则所有数据点应保持与转换之前相同的相对顺序，并且这不会 影响研究人员解释分数。但是，对于那些需要以直截了当的方式(例如年收入和年龄)来解释的 原始变量，数据转换就可能会出现问题。因为转换后，新变量的解释可能会变得更加复杂。 抵抗程序 参数测试基于均值估计，均值对异常值或偏斜分布敏感。在EDA中，通常使用鲁棒的估计器。例如 : ● 中位数:数据的中间点。 ● 三均值:根据第一四分位数，第三四分位数和中位数两次的算术平均值计算的集中趋势的 度量。 ● Winsorized均值:均值的可靠版本，其中极端得分被拉回到大部分数据中。 ● 切尾均值:没有异常值的均值 在您的第一节统计课程中，您就能了解到众数对异常值的抵抗力比中值大。您可能会问为什么使 用中位数而不是众数。确实，在大多数情况下，中位数和众数对异常值具有同样的鲁棒性。请观看 此动画演示。 重要的是要指出，尽管“抵抗性”和“鲁棒性”通常可以互换使用，但它们之间存在细微的差别。 EDA更加关注抵抗力，而假设检验则更加关注鲁棒性。抵抗力是关于不受异常值影响的，而鲁棒 性是关于不受违反原假设带来的影响。在前者中，目标是获得数据摘要，而在后者中，目标是进行 概率推断。 7.推荐用于EDA的软件 ● DataDesk DataDesk(数据描述公司，2008年)由John Tukey的学生Paul Velleman开发。 DataDesk是探索性 数据分析初学者的理想工具。 它具有丰富的功能和足够的灵活性，可以进行操作，但是对计算机 操作的了解很少。 例如，可以使用DataDesk中的各种转换函数来执行上述数据重新表达。 DataDesk具有一个称为Data Desk Plus的更丰富的版本，其中包含名为ActivStat的基于多媒体的 统计教程。 ● JMP JMP(SAS研究所，2016)是一个非常通用的统计程序。 JMP有两种变体，即JMP和JMP Pro。 顾名 思义，JMP Pro是一个专业版本，其中包含许多强大的过程。 但是对于大多数用户而言，JMP足以 满足EDA的要求。 像DataDesk一样，JMP具有内置的数据转换选项，如下所JMP的设计原理类似于苹果iPod的设计原理。 安装后，您无需阅读手册即可开始浏览数据。 除了直方图和箱线图之类的常用图形功能外，JMP中的“图形生成器”还为用户提供了地理信息系统( GIS)。 ● XLISP-STAT 如果您希望通过编程获得完全的控制权，则应考虑XLISP-STAT。例如，在数据可视化中，它涉及数 据平滑。通过编程，您可以查看不同详细程度的数据。 LISP代表列表处理。有人称其为“很多白痴和愚蠢的括号”。 LISP由John McCarthy在MIT于1956-62 年创建，用于非数值计算。后来，它专门用于人工智能的开发。 LISP有许多不同的版本，例如普 通Lisp，Franz LISP等XLISP是David Betz开发的许多方言之一。后来，LukeTierney(1990)开发了 用于统计可视化的XLISP-STAT。该软件包具有许多内置的统计图形功能。 Cook and Weisberg( 1994)基于XLISP-STAT，开发了一套称为R-code的回归绘图工具。另一个名为ViSta的综合EDA软 件包(Young，1999)也用XLISP-STAT编写。 XLISP-STAT是跨平台的。但是，它是一种解释性语言，而不是一种编译语言，因此，必须将编写的 程序加载到XLISP-STAT中才能运行它。 更多参考 Tukey(1977)的书被认为是EDA中的经典著作。 在他的时代，计算机资源不容易获得，但是如今， 他建议的大多数图形技术可在许多软件包中使用。 Behrens(1997)和Behrens&amp;Yu(2003)对于初学者和中级学习者都是必不可少的。 这两章都涵盖 了可视化，数据转换，残差分析和抵抗过程的详细信息，本课对此进行了简要介绍。 有关EDA的快速概述，请访问NIST工程统计手册。 尽管此站点提供了许多图形技术示例，但并未 告诉您哪些特定的软件包可以生成这些图形。 有关EDA的哲学基础，请咨询Yu(1994年4月，2006年)。 EDA是一种哲学/态度，而不是技的术集 合。 要更深入地了解数据可视化，请阅读Yu和Behrens(1995)和Yu(2010，2014)。 9.注解 *例如，被认为是现代遗传学创始人的格雷戈尔·孟德尔(Gregor Mendel，1824-1884年)，通过他的 科学发现确立了物种的物理特性受遗传影响的观念。孟德尔进行了一次受精实验，以证实他的信 念。在他的实验中，他跟踪了几代植物，观察特定基因如何从一代传到另一一代。尽管报告的数据 在很大程度上符合遗传假说，但R. A. Fisher(1936)质疑孟德尔研究的有效性。费舍尔指出孟，德 尔的数据“实在太好了”。通过卡方检验，费舍尔发现孟德尔的结果是如此接近预期，以至于这种协 议偶然发生的可能性少于万分之一。 另一个例子可以在约翰内斯·开普勒(Johnannes Kepler，1571-1630)的故事中找，到他是第一位提 出将地球和其他行星以椭圆形绕太阳公转的提议的天文学家，而不是像伽利略所相信的那样绕太 阳公转。开普勒在另一个著名的天文学家布拉赫(Brahe)的指导下工作，布拉赫收集了庞大的行 星轨道数据库。利用Brahe的数据，开普勒发现数据适合椭圆假设，而不是圆形假设。然而，将近 400年后，当威廉·多纳休(William Donahue)重新提出开普勒的计算结果时，他发现轨道数据和椭 圆模型并不完全吻合。 此外，有一个广泛的城市传说，英国物理学家亚瑟·爱丁顿(Arthur Eddington)通过观察19年19日食 期间恒星的位置，证实了爱因斯坦的广义相对论。但是，在1980年代，学者发现爱丁顿确实收集 了足够的数据来得出结论。相反，他扭曲了结果以使其符合理论(Swayer，2012)。 开普勒，孟德尔和爱丁顿不是仅有的三位未能接受数据和模型之间残差的科学家。威廉·哈维( William Harvey)，艾萨克·牛顿(Isaac Newton)和查尔斯·达尔文(CharleDsarwin)也有同样的问 题。这个清单不胜枚举。在回顾科学史上的这一现象时，一些学者谴责那些科学家犯有欺诈罪。 Press and Tanur(2001)用温和的语气说，问题是由“科学家的主观性”引起的。 我的观点是，这些科学家对残留物的意识较弱。他们以确认性方式进行科学，其中只能得出二分 法的答案。即使存在残差，它们也倾向于包含模型，因为通过承认任何不一致之处，整个模型都会 被拒绝。换句话说，他们接受了DATA = MODEL的概念。 修订日期:2017年5月 参考文献 ● Behrens，J.T。(1997)。探索性数据分析的原则和程序。心理方法，第2卷，第131-16页0 。 ● Behrens，J.T.，&amp;Yu，C.H.(2003)。探索性数据分析。在J. A. Schinka和W. F. Velicer(编辑) 中。心理学手册第2卷:心理学研究方法(第33-64页)。新泽西州:John Wiley&amp;Sons，Inc. ● Cook，D. R.和Weisberg，S.(1994)。回归图形简介。纽约:威利。 ● 数据描述公司(2008)。 DataDesk。 [在线]可用:http://www.datadesk.com ● Fisher，R.A.(1936年)。孟德尔的作品被重新发现了吗?科学年鉴，第1卷，第115-137页。 ● Jebb，A.，Parrigon，S.&amp;Woo，S.E.(2017年)。探索性数据分析是归纳研究的基 础。人力资 源管理评论，第27期，第265-276页。 ● Osborne，J。W.(2002)。有关使用数据转换的注意事项。实用评估，研究与评估，8(6)取 自:http://pareonline.net/getvn.asp?v=8&amp;n=6 ● Press，S.J。，和Tanur，J.M。(2001)。科学家的主观性和贝叶斯方法。纽约:约翰·威利父子( John Wiley&amp;Sons)。 ● SAS研究所。 (2016)。 JMP [计算机软件]。卡里，北卡罗来纳州:作者。 ● Swayer，R.K.(2012年)。解释创造力:人类创新的科学(第二版)。纽约，纽约:牛津大学出版 社。 ● Tierney，L.(1990)。 Lisp-Stat:用于统计计算和动态图形的面向对象的环境。纽约:威利。 ● Tukey，J.W。(1977)。探索性数据分析。马萨诸塞州雷丁:Addison-Wesley出版公司。 ● Tukey，J.W。(1980)。我们需要探索性和确认性。美国统计学家，第34页，第23-25页。 ● Velleman，P.F.和Hoaglin，D.C.(1981)。探索性数据分析的应用程序，基础知识和计算。马 萨诸塞州波士顿:达克斯伯里出版社。 ● Young，F.(1999)。 ViSta [计算机软件]。取自http://www.uv.es/prodat/ViSta/ ● Yu，C.H.(1994年4月)。就职?扣除?绑架? EDA有逻辑吗?该论文在路易斯安那州新奥 尔良举行的美国教育研究者协会年会上发表。 (ERIC文件复制服务，编号ED 376 173) ● Yu，C.H。和Behrens，J.T。(1995)。科学多元可视化在行为科学中的应用。行为研究方法， 仪器和计算机，第2卷，第264-271页。 ● Yu，C.H.(2010年)。在数据挖掘和重新采样的背景下进行探索性数据分析。国际心理研究 杂志，3(1)，9-22。取自 http://mvint.usbmed.edu.co:8002/ojs/index.php/web/article/download/455/460 [镜像]。 ● Yu，C.H.(2006年)。定量研究方法论的哲学基础。医学博士兰纳姆:美国大学出版社。 ● Yu，C.H.(2014年)。与数据共舞:数据可视化的艺术和科学。德国萨尔布吕肯(Saarbrucken ):LAP。 补充:关于数据可视化软件的更多介绍 Tableau: Tableau的真正目的是一个简单的数据可视化工具。开发该工具旨在洞察无法通过盯着电子表格 来快速回答的问题。自成立以来，它已发展成为地球上最流行的数据可视化和报告工具之一。使 用Tableau，用户可以以惊人的速度开发交互式仪表板和可视化文件。相较于其他可视化软件， Tableau有以下几点突出优势: (1) 快速创建交互式可视化:使用Tableau的拖放功能，用户可以在数分钟内创建非常互动的视觉效 果。该界面可以处理无尽的变化，同时还限制您创建违反数据可视化最佳做法的图表。您可以查 看Tableau Gallery上创建的一些惊人的视觉效果。 (2) 易于实施:Tableau中提供了许多不同类型的可视化选项，可以增强用户体验。而且，与Python ，Business Objects和Domo相比，Tableau非常易于学习，任何不具备编码知识的人都可以轻松学 习Tableau (3) 处理大量数据:Tableau可以轻松处理数百万行数据。可以使用大量数据创建不同类型的可视 化文件，而不会影响仪表板的性能。另外，Tableau中有一个选项，用户可以使它“实时”建立到不 同数据源(如SQL等)的连接。 (4) 使用其他脚本语言:为了避免性能问题并在Tableau中进行复杂的表计算，用户可以合并使用 Python或R。使用Python脚本可以通过对数据包执行数据清除任务来减轻软件的负担。但是， Python不是Tableau接受的本机脚本语言。因此，您可以导入一些视觉效果或包装。但是，您可以 看到使用Power BI的Python如何解决此问题。 R: 使用R提供的各种数据包, 仅需几行代码就可以创建具有视觉吸引力的数据可视化。常用的10大数 据可视化数据包有plotly, ggplot2, tidyquant, taucharts, ggiraph, geofacets, googleVis, RColorBrewer, dygraphs, shiny。 (1) Plotly软件包提供了在线互动图和质量图。该软件包扩展了JavaScript库 (2) ggplot2以其优雅和高质量的图形而闻名，这使其与其他可视化程序包区分开来。 (3) Tidyquant是用于执行定量财务分析的财务软件包。该软件包在tidyverse Universe下添加为财 务软件包，用于导入，分析和可视化数据。 (4) Taucharts提供了一个声明性接口，用于将数据字段快速映射到视觉属性。 (5) Ggiraph是允许我们创建动态ggplot图的工具。该软件包使我们可以在图形中添加工具提示， JavaScript操作和动画。 (6) Geofacets软件包为“ ggplot2”提供了地理标注功能。 Geofaceting将针对不同地理实体的一系 列绘图安排到保留某些地理方位的网格中。 (7) GoogleVis在R和Google的图表工具之间提供了一个界面。借助此软件包，我们可以基于R数据 框创建具有交互式图表的网页。 (8) RColorBrewer提供了由Cynthia Brewer设计的地图和其他图形的配色方案 (9) Dygraphs包是dygraphs JavaScript图表库的R接口。它提供了丰富的功能来绘制R中的时间序 列数据。 (10) Shiny使我们能够通过提供闪亮的程序包来开发交互式且美观的Web应用程序。该软件包提 供了HTML窗口小部件，CSS和JavaScript的各种扩展 Python: Python具有一些最具交互性的数据可视化工具。最基本的绘图类型在多个库之间共享，但是其他 类型仅在某些库中可用。数据科学家经常使用的数据可视化库是Matplotlib, Seaborn 和 Plotly。 (1) Matplotlib是最受欢迎的Python数据可视化库。它用于生成简单而强大的可视化。从初 学者到 经验丰富的数据科学专业人士，Matplotlib是最广泛使用的绘图库。 (2) Seaborn提供了多种可视化模式。与matplotlib相比，它与Pandas数据框的集成度更高。 Seaborn被广泛用于统计可视化，因为它具有一些内置的最佳统计任务。 (3) Plotly主要用于处理地理，科学，统计和财务数据。 "],["chinese-translation-of-stringr-package.html", "Chapter 21 Chinese translation of stringr package", " Chapter 21 Chinese translation of stringr package Siwen Xie library(stringr) 入门介绍：stringr package 资源链接：https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html 字符串不是R的迷人组成部分，但在许多数据清理和准备任务中确实发挥了重要作用。stringr提供了一组有凝聚力的功能，旨在使字符串的工作尽可能轻松。如果您不熟悉字符串，那么最好的起点是R for Data Science中有关字符串的章节。 stringr建立在stringi之上，stringi使用ICU C库提供常见字符串操作的快速，正确实现。 stringr专注于最重要且最常用的字符串操作函数，而stringi提供了涵盖几乎所有你可以想象的内容的全面集合。如果发现stringr缺少所需的功能，请尝试查找stringi。这两个软件包都具有相似的约定，因此，一旦掌握了stringr，就应该发现stringi同样易于使用。 stringr有四个主要的函数族： 字符操作：这些函数允许您操作字符向量中字符串中的单个字符。 用于添加、删除和操作空格的空格工具。 对区域设置敏感的操作，其操作因区域设置而异。 模式匹配功能。它们识别模式描述的四个引擎。最常见的是正则表达式，但还有三种其他工具。 获取和设置单个字符： 你用str_length()可以得到字符向量的长度 library(&quot;stringr&quot;) str_length(&quot;abc&quot;) ## [1] 3 这个函数和R的基础函数nchar() 是一样的。之前，我们需要解决nchar() 的一些问题，比如nchar(NA)会返回2。这个问题已经在3.3.0 版本的R 得到解决所以这个问题不是那么重要了。 你使用str_sub()可以得到单个字符。它需要三个参数：字符向量、起始位置和结束位置。任何一个位置都可以是从左开始计数的正整数，也可以是从右开始计数的负整数。这些位置包括在内，并且如果长于字符串，则将被自动截断。 x &lt;- c(&quot;abcdef&quot;, &quot;ghifjk&quot;) # The 3rd letter str_sub(x, 3, 3) ## [1] &quot;c&quot; &quot;i&quot; # The 2nd to 2nd-to-last character str_sub(x, 2, -2) ## [1] &quot;bcde&quot; &quot;hifj&quot; 你也可以使用str_sub()来修改字符串： str_sub(x, 3, 3) &lt;- &quot;X&quot; x ## [1] &quot;abXdef&quot; &quot;ghXfjk&quot; 要复制单个字符串，可以使用str_dup(): str_dup(x, c(2, 3)) ## [1] &quot;abXdefabXdef&quot; &quot;ghXfjkghXfjkghXfjk&quot; 空格 三个函数可添加，删除或修改空格： str_pad()通过在左侧，右侧或两侧添加额外的空格来将字符串填充到固定长度。 x &lt;- c(&quot;abc&quot;, &quot;defghi&quot;) str_pad(x, 10) # default pads on left ## [1] &quot; abc&quot; &quot; defghi&quot; str_pad(x, 10, &quot;both&quot;) ## [1] &quot; abc &quot; &quot; defghi &quot; （你可以使用pad参数填充其他字符。） str_pad()绝不会使字符串更短: str_pad(x, 4) ## [1] &quot; abc&quot; &quot;defghi&quot; 因此，如果要确保所有字符串的长度相同（通常对打印方法有用），请结合使用str_pad()和str_trunc(): x &lt;- c(&quot;Short&quot;, &quot;This is a long string&quot;) x %&gt;% str_trunc(10) %&gt;% str_pad(10, &quot;right&quot;) ## [1] &quot;Short &quot; &quot;This is...&quot; str_trim():是str_pad()的相反，它删除了前导和尾随空格： x &lt;- c(&quot; a &quot;, &quot;b &quot;, &quot; c&quot;) str_trim(x) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; str_trim(x, &quot;left&quot;) ## [1] &quot;a &quot; &quot;b &quot; &quot;c&quot; 你可以使用str_wrap()修改现有的空格，以换行一段文本，以使每行的长度尽可能相似。 jabberwocky &lt;- str_c( &quot;`Twas brillig, and the slithy toves &quot;, &quot;did gyre and gimble in the wabe: &quot;, &quot;All mimsy were the borogoves, &quot;, &quot;and the mome raths outgrabe. &quot; ) cat(str_wrap(jabberwocky, width = 40)) ## `Twas brillig, and the slithy toves did ## gyre and gimble in the wabe: All mimsy ## were the borogoves, and the mome raths ## outgrabe. 对区域的敏感性 少数几个字符串函数对语言环境敏感：它们在世界不同地区的执行方式会有所不同。 这些功能是大小写转换功能： x &lt;- &quot;I like horses.&quot; str_to_upper(x) ## [1] &quot;I LIKE HORSES.&quot; str_to_title(x) ## [1] &quot;I Like Horses.&quot; str_to_lower(x) ## [1] &quot;i like horses.&quot; str_to_lower(x, &quot;tr&quot;) ## [1] &quot;ı like horses.&quot; 字符串排序： x &lt;- c(&quot;y&quot;, &quot;i&quot;, &quot;k&quot;) str_order(x) ## [1] 2 3 1 str_sort(x) ## [1] &quot;i&quot; &quot;k&quot; &quot;y&quot; # In Lithuanian, y comes between i and k str_sort(x, locale = &quot;lt&quot;) ## [1] &quot;i&quot; &quot;y&quot; &quot;k&quot; 区域设置始终默认为英语，以确保默认行为在系统之间相同。语言环境始终包含两个字母的ISO-639-1语言代码（如英语为“ en”或中文为“ zh”），以及可选的ISO-3166国家/地区代码（如“ en_UK”与“ en_US”）。您可以通过运行stringi::stri_locale_list() 来查看可用语言环境的完整列表。 模式匹配 绝大多数的字符串函数都与模式一起使用。这些参数由它们执行的任务和它们匹配的模式类型进行参数化。 任务： 每个模式匹配函数都具有相同的前两个参数，要处理的字符串的字符向量和要匹配的单个模式。 stringr提供模式匹配功能，以检测，定位，提取，匹配，替换和拆分字符串。我将说明它们如何与一些字符串和正则表达式配合使用以匹配（美国）电话号码： strings &lt;- c( &quot;apple&quot;, &quot;219 733 8965&quot;, &quot;329-293-8753&quot;, &quot;Work: 579-499-7527; Home: 543.355.3679&quot; ) phone &lt;- &quot;([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})&quot; str_detect() 检测模式的存在或不存在，并返回逻辑向量（类似于grepl()）。 str_subset() 返回与正则表达式匹配的字符向量的元素（类似于 grep()，其值为TRUE）。 # Which strings contain phone numbers? str_detect(strings, phone) ## [1] FALSE TRUE TRUE TRUE str_subset(strings, phone) ## [1] &quot;219 733 8965&quot; ## [2] &quot;329-293-8753&quot; ## [3] &quot;Work: 579-499-7527; Home: 543.355.3679&quot; str_count()计算匹配的数量 # How many phone numbers in each string? str_count(strings, phone) ## [1] 0 1 1 2 str_locate() 定位模式的第一个位置，并返回带有列开始和结束的数字矩阵。 str_locate_all()查找所有匹配项，并返回数字矩阵列表。与regexpr()和gregexpr()类似。 # Where in the string is the phone number located? (loc &lt;- str_locate(strings, phone)) ## start end ## [1,] NA NA ## [2,] 1 12 ## [3,] 1 12 ## [4,] 7 18 str_locate_all(strings, phone) ## [[1]] ## start end ## ## [[2]] ## start end ## [1,] 1 12 ## ## [[3]] ## start end ## [1,] 1 12 ## ## [[4]] ## start end ## [1,] 7 18 ## [2,] 27 38 str_extract()提取与第一个匹配项对应的文本，并返回一个字符向量。 str_extract_all() 提取所有匹配项并返回字符向量列表。 # What are the phone numbers? str_extract(strings, phone) ## [1] NA &quot;219 733 8965&quot; &quot;329-293-8753&quot; &quot;579-499-7527&quot; str_extract_all(strings, phone) ## [[1]] ## character(0) ## ## [[2]] ## [1] &quot;219 733 8965&quot; ## ## [[3]] ## [1] &quot;329-293-8753&quot; ## ## [[4]] ## [1] &quot;579-499-7527&quot; &quot;543.355.3679&quot; str_extract_all(strings, phone, simplify = TRUE) ## [,1] [,2] ## [1,] &quot;&quot; &quot;&quot; ## [2,] &quot;219 733 8965&quot; &quot;&quot; ## [3,] &quot;329-293-8753&quot; &quot;&quot; ## [4,] &quot;579-499-7527&quot; &quot;543.355.3679&quot; str_match()从第一个匹配中提取由（）形成的捕获组。它返回一个字符矩阵，其中一列用于完全匹配，一列用于每个组。str_match_all()从所有匹配项中提取捕获组，并返回字符矩阵列表。与regmatches()类似。 # Pull out the three components of the match str_match(strings, phone) ## [,1] [,2] [,3] [,4] ## [1,] NA NA NA NA ## [2,] &quot;219 733 8965&quot; &quot;219&quot; &quot;733&quot; &quot;8965&quot; ## [3,] &quot;329-293-8753&quot; &quot;329&quot; &quot;293&quot; &quot;8753&quot; ## [4,] &quot;579-499-7527&quot; &quot;579&quot; &quot;499&quot; &quot;7527&quot; str_match_all(strings, phone) ## [[1]] ## [,1] [,2] [,3] [,4] ## ## [[2]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;219 733 8965&quot; &quot;219&quot; &quot;733&quot; &quot;8965&quot; ## ## [[3]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;329-293-8753&quot; &quot;329&quot; &quot;293&quot; &quot;8753&quot; ## ## [[4]] ## [,1] [,2] [,3] [,4] ## [1,] &quot;579-499-7527&quot; &quot;579&quot; &quot;499&quot; &quot;7527&quot; ## [2,] &quot;543.355.3679&quot; &quot;543&quot; &quot;355&quot; &quot;3679&quot; str_replace()替换第一个匹配的模式并返回一个字符向量。str_replace_all()替换所有匹配项。类似于sub() 和 gsub()。 str_replace(strings, phone, &quot;XXX-XXX-XXXX&quot;) ## [1] &quot;apple&quot; ## [2] &quot;XXX-XXX-XXXX&quot; ## [3] &quot;XXX-XXX-XXXX&quot; ## [4] &quot;Work: XXX-XXX-XXXX; Home: 543.355.3679&quot; str_replace_all(strings, phone, &quot;XXX-XXX-XXXX&quot;) ## [1] &quot;apple&quot; ## [2] &quot;XXX-XXX-XXXX&quot; ## [3] &quot;XXX-XXX-XXXX&quot; ## [4] &quot;Work: XXX-XXX-XXXX; Home: XXX-XXX-XXXX&quot; str_split_fixed() 根据模式将字符串分成固定数量的段，并返回一个字符矩阵。str_split() 将字符串分成可变数量的段，并返回字符向量列表。 str_split(&quot;a-b-c&quot;, &quot;-&quot;) ## [[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; str_split_fixed(&quot;a-b-c&quot;, &quot;-&quot;, n = 2) ## [,1] [,2] ## [1,] &quot;a&quot; &quot;b-c&quot; 引擎 stringr可以使用四个主要引擎来描述模式： 正则表达式是默认值，如上所示，并在vignette(“regular-expressions”)中进行了描述。 使用fixed()进行固定的按字节匹配。 区分语言环境的字符匹配，使用coll() 使用boundary()进行文本边界分析。 固定匹配 fixed(x) 仅匹配x指定的确切字节序列。这是一个非常有限的“模式”，但是该限制可以使匹配更快。当心对非英语数据使用fixed() 。这是有问题的，因为通常有多种方式表示同一角色。例如，有两种定义“á”的方法：作为单个字符或作为“a”加重音符号： a1 &lt;- &quot;\\u00e1&quot; a2 &lt;- &quot;a\\u0301&quot; c(a1, a2) ## [1] &quot;á&quot; &quot;á&quot; a1 == a2 ## [1] FALSE 它们的渲染方式相同，但是由于定义不同，fixed() 找不到匹配项。相反，您可以使用下面解释的coll()来遵守人类字符比较规则： str_detect(a1, fixed(a2)) ## [1] FALSE str_detect(a1, coll(a2)) ## [1] TRUE 归类搜索 coll(x)使用人工语言的排序规则来寻找与x的匹配项，如果您要进行不区分大小写的匹配，则它尤其重要。排序规则在世界各地都不同，因此您还需要提供一个语言环境参数。 i &lt;- c(&quot;I&quot;, &quot;İ&quot;, &quot;i&quot;, &quot;ı&quot;) i ## [1] &quot;I&quot; &quot;İ&quot; &quot;i&quot; &quot;ı&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE)) ## [1] &quot;I&quot; &quot;i&quot; str_subset(i, coll(&quot;i&quot;, ignore_case = TRUE, locale = &quot;tr&quot;)) ## [1] &quot;İ&quot; &quot;i&quot; coll()的缺点是速度慢。由于识别哪些字符相同的规则很复杂，因此与regex() 和fixed()相比，coll()相对较慢。注意，当fixed()和regex()都具有ignore_case参数时，它们执行的比较比coll()简单得多。 边界分析 boundary()匹配字符，行，句子或单词之间的边界。它对str_split()最为有用，但可以与所有模式匹配功能一起使用： x &lt;- &quot;This is a sentence.&quot; str_split(x, boundary(&quot;word&quot;)) ## [[1]] ## [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; str_count(x, boundary(&quot;word&quot;)) ## [1] 4 str_extract_all(x, boundary(&quot;word&quot;)) ## [[1]] ## [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;sentence&quot; 按照惯例，“ ”被视为boundary(character)界限区分符： str_split(x, &quot;&quot;) ## [[1]] ## [1] &quot;T&quot; &quot;h&quot; &quot;i&quot; &quot;s&quot; &quot; &quot; &quot;i&quot; &quot;s&quot; &quot; &quot; &quot;a&quot; &quot; &quot; &quot;s&quot; &quot;e&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;n&quot; &quot;c&quot; &quot;e&quot; &quot;.&quot; str_count(x, &quot;&quot;) ## [1] 19 函数总结 "],["creating-r-packages.html", "Chapter 22 Creating R Packages", " Chapter 22 Creating R Packages Sam Gabor Creating custom R packages allows users to exchange code logic and data effortlessly. In addition, even if R packages are not shared with others, they still provide utility for the R user because they group related functionality into clean, easily maintained and tested units. I created a tutorial that illustrates the creation of a non-trivial R package covering all the steps that constitute a good work flow. It culminates in publishing the created package on GitHub where it can be shared with other users using devtools::install_github(\"atsats/PCD\"). The finished package as well as the Tutorial markdown, html and pdf files can be found here: https://github.com/atsats/PCD "],["introduction-to-quandl.html", "Chapter 23 Introduction to Quandl() 23.1 INTRODUCTION", " Chapter 23 Introduction to Quandl() Aiqi Zhou (az2638) Quandl::Quandl() 23.1 INTRODUCTION Quandl is a source of financial, economic and alternative data. It is acquired by Nasdaq in 2018 and has 20+ million data sets available. Users can acquire free data, purchase data or sell data at Quandl. You can find data on a large variety of types of data from company data to the demographics data of a country. More can be explored on Quandl’s website. https://www.quandl.com/search The data is readily available in json, csv, xml formats and can be loaded in MatLab, R, Python, etc. All the data can be loaded in R through the package Quandl. 23.1.1 Installation : install.packages(“Quandl”) 23.1.2 Load Package library(Quandl) library(Quandl) 23.1.3 Set API key Quandl allows 50 calls a day for anonymous users. Sign up for a free account to make unlimited calls every day. Set up your own account and copy your api_key into the following function Quandl.api_key(‘your_key’) 23.1.4 Loading Data All available data can be found and viewed on https://www.quandl.com/search. This package includes many free datasets, however, some datasets requires an account or is for paid use only. To call on a data set in R. You must find the Quandl code of the data set. The codes can be found in the documentation page of the data set. 23.1.4.1 For example The follow dataset is called the “Federal Reserve Economic Data”, which has the code FRED. This dataset has 335,000+ time-series data. To call on certain aspect like the GDP of this data set, the Quandl code is “FRED/GDP”. mydata = Quandl(&quot;FRED/GDP&quot;) head(mydata) ## Date Value ## 1 2020-10-01 21487.90 ## 2 2020-07-01 21170.25 ## 3 2020-04-01 19520.11 ## 4 2020-01-01 21561.14 ## 5 2019-10-01 21747.39 ## 6 2019-07-01 21540.33 To call on another aspect like disposable personal income, the Quandl code is “FRED/DSPI” mydata= Quandl(&quot;FRED/DSPI&quot;) head(mydata) ## Date Value ## 1 2021-01-01 19217.7 ## 2 2020-12-01 17254.5 ## 3 2020-11-01 17151.4 ## 4 2020-10-01 17398.9 ## 5 2020-09-01 17546.8 ## 6 2020-08-01 17430.4 23.1.5 Type The data can be called in different formats. The default of the data is data frame, which can have an argument type = “raw” “raw” “ts” “zoo” “xts” “timeSeries” mydata = Quandl(&quot;FRED/GDP&quot;, type=&quot;xts&quot;) head(mydata) ## [,1] ## 1947 Q1 243.164 ## 1947 Q2 245.968 ## 1947 Q3 249.585 ## 1947 Q4 259.745 ## 1948 Q1 265.742 ## 1948 Q2 272.567 mydata = Quandl(&quot;FRED/GDP&quot;, type=&quot;ts&quot;) head(mydata) ## [1] 243.164 245.968 249.585 259.745 265.742 272.567 mydata = Quandl(&quot;FRED/GDP&quot;, type=&quot;zoo&quot;) head(mydata) ## 1947 Q1 1947 Q2 1947 Q3 1947 Q4 1948 Q1 1948 Q2 ## 243.164 245.968 249.585 259.745 265.742 272.567 23.1.6 Transform There are some preprocessing can be done when loading the data. By adding a transform argument in Quandl(), the data can have the following types of transformations. \"\" = Default original data “diff” = row on row change in value “rdiff” = row on row percentage change “normalize” = scale to start at 100 “cumul” = cumulative sum “rdiff_from” = latest value as % increment 23.1.6.1 For example the percentage change of GDP by year mydata = Quandl(&quot;FRED/GDP&quot;,transform = &quot;rdiff&quot; ) head(mydata) ## Date Value ## 1 2020-10-01 0.015004262 ## 2 2020-07-01 0.084535264 ## 3 2020-04-01 -0.094662207 ## 4 2020-01-01 -0.008564474 ## 5 2019-10-01 0.009613086 ## 6 2019-07-01 0.009866349 23.1.7 Order The data can be ordered by date upon it is loaded. Default is descending order by date. - “desc” - “asc” mydata = Quandl(&quot;FRED/GDP&quot;, order = &quot;asc&quot;) head(mydata) ## Date Value ## 1 1947-01-01 243.164 ## 2 1947-04-01 245.968 ## 3 1947-07-01 249.585 ## 4 1947-10-01 259.745 ## 5 1948-01-01 265.742 ## 6 1948-04-01 272.567 23.1.8 collapse Since all data have a time associated. Quandl allows you to preprocess the data by specifying the frequency of the data. ’’ ‘daily’ ‘weekly’ ‘monthly’ ‘quarterly’ ‘annual’ mydata = Quandl(&quot;FRED/GDP&quot;, collapse = &quot;quarterly&quot;) head(mydata) ## Date Value ## 1 2020-12-31 21487.90 ## 2 2020-09-30 21170.25 ## 3 2020-06-30 19520.11 ## 4 2020-03-31 21561.14 ## 5 2019-12-31 21747.39 ## 6 2019-09-30 21540.33 23.1.9 Slicing data 23.1.9.1 Rows Because the data we are using is time series data, we can specify the range of row of the data set, using the start and end date arguments. mydata = Quandl(&quot;FRED/GDP&quot;, start_date=&quot;2001-12-31&quot;, end_date=&quot;2005-12-31&quot;) head(mydata) ## Date Value ## 1 2005-10-01 13332.32 ## 2 2005-07-01 13142.87 ## 3 2005-04-01 12910.02 ## 4 2005-01-01 12761.34 ## 5 2004-10-01 12522.42 ## 6 2004-07-01 12303.34 23.1.9.2 Columns To get multiple columns of data from Quandl, the data can be called in the format: Quandl(c(“col_1”, “col_2”)) mydata = Quandl(c(&quot;FRED/GDP&quot;, &quot;FRED/DSPI&quot;),start_date=&quot;2001-12-31&quot;, end_date=&quot;2005-12-31&quot;) head(mydata) ## Date FRED.GDP - Value FRED.DSPI - Value ## 1 2002-01-01 10788.95 7962.4 ## 2 2002-02-01 NA 7981.9 ## 3 2002-03-01 NA 8003.6 ## 4 2002-04-01 10893.21 8066.8 ## 5 2002-05-01 NA 8099.5 ## 6 2002-06-01 NA 8127.2 23.1.10 Finding a dataset in Quandl You can look for certain data sets within R using Quandl.search() Format: Quandl.search(query = “search keyword”, page = # , source = “source to search from if known”, silent = TRUE/FALSE) query: mandatory argument that you want to search for page: which page of search result you want. Default page = 1. source: specific source you want to search from. silent: print the results when FALSE. Nothing prints when true. 23.1.10.1 Example search for Japan Quandl.search(query = &quot;Japan&quot;, page = 1 , silent = FALSE) ## Japanese Intervention: Japanese Bank purchases of USD against JPY ## Code: FRED/JPINTDUSDJPY ## Desc: 100 Million Yen Not Seasonally Adjusted, (+) numbers mean purchases of the USD (sell Yen), (-) numbers mean sales of USD (buy Yen). Unpublished data. ## Freq: daily ## Cols: DATE | VALUE ## ## NASDAQ Japan Retail JPY Index (NQJP5300JPY) ## Code: NASDAQOMX/NQJP5300JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP5300JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP5300JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Media JPY Index (NQJP5500JPY) ## Code: NASDAQOMX/NQJP5500JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP5500JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP5500JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Tech JPY Index (NQJP9000JPY) ## Code: NASDAQOMX/NQJP9000JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP9000JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP9000JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Telecom JPY Index (NQJP6000JPY) ## Code: NASDAQOMX/NQJP6000JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP6000JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP6000JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Utilities JPY Index (NQJP7000JPY) ## Code: NASDAQOMX/NQJP7000JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP7000JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP7000JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Banks JPY Index (NQJP8300JPY) ## Code: NASDAQOMX/NQJP8300JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP8300JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP8300JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Inds JPY Index (NQJP2000JPY) ## Code: NASDAQOMX/NQJP2000JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP2000JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP2000JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Ins JPY Index (NQJP8500JPY) ## Code: NASDAQOMX/NQJP8500JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP8500JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP8500JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value ## ## NASDAQ Japan Financials JPY Index (NQJP8000JPY) ## Code: NASDAQOMX/NQJP8000JPY ## Desc: For detailed information, see &lt;a href=https://indexes.nasdaqomx.com/Index/Overview/NQJP8000JPY&gt;https://indexes.nasdaqomx.com/Index/Overview/NQJP8000JPY&lt;/a&gt; ## Freq: daily ## Cols: Trade Date | Index Value | High | Low | Total Market Value | Dividend Market Value You can pick which data set to use from the search result. 23.1.11 Example The following is a quick example of getting data from the data set United Nations Industrial Commodities. From the documentation of this data set, we can learn what we are interested in. https://www.quandl.com/data/UINC-United-Nations-Industrial-Commodities Here I want to plot the time series data of Beer production of Germany. The code for this information is “UINC/BEER_DEU”. I want to plot the time series, then the data need to be in the form of “ts” beer = Quandl(&quot;UINC/BEER_DEU&quot;,type = &quot;ts&quot;) head(beer) ## Beer (Mil. USD) Beer (Thousand hectolitres) ## [1,] 10021.504 111875.4 ## [2,] 9468.029 108937.5 ## [3,] 8258.888 108729.5 ## [4,] 7873.553 106993.0 ## [5,] 7625.868 107479.3 ## [6,] 6420.827 106877.4 beer = Quandl(&quot;UINC/BEER_DEU&quot;,type = &quot;ts&quot;) plot.ts(beer) If I want to see the yearly percentage change of the data, we need to add a transform argument “rdiff” beer_per_change = Quandl(&quot;UINC/BEER_DEU&quot;,type = &quot;ts&quot;,transform = &#39;rdiff&#39;) plot.ts(beer_per_change) "],["getting-data-using-apis.html", "Chapter 24 Getting data using APIs", " Chapter 24 Getting data using APIs Melissa Bischoff For my community contribution project I created a tutorial on how to use APIs to get data. API integrations are an easy and useful way to pull data from the internet before resorting to web scraping. A lot of companies and services have APIs available, some are free for public use and some are for paying customers. In my tutorial I did three examples of getting data using APIs - one in R and two in Python. The first example is in R and uses the httr package. It accesses the github job posting API, which does not require authentication. The second example is the same as the first but in Python and uses the requests package. The third example is also in Python with the requests package but uses the Spotify API which does require authentication. The tutorial includes information about what APIs are focusing on the GET and POST method, examples of getting data from APIs with and without authentication, and helpful tricks along the way. Link to tutorial pdf Link to Python code Link to R code "],["introdution-to-time-series-analysis.html", "Chapter 25 Introdution to Time Series Analysis 25.1 Overview 25.2 Introduction 25.3 Simple Time Series 25.4 Measures of Dependence 25.5 Reference", " Chapter 25 Introdution to Time Series Analysis Yiyuan Xu library(astsa) library(TSA) 25.1 Overview This text briefly talks about basic ideas of time series analysis. Meanwhile, several simple time series are roughly described and shown in pictures. Finally, we explored a few tools such as mean function to analyze and predict time series. 25.2 Introduction The analysis of data that have been observed at different time points causes new problems in statistical modeling and inference. These data points taken over time may have an internal structure such as autocorrelation, trend or seasonal variation, which restricts the applicability of some conventional statistical models that assume adjacent observations are independent. Time series analysis, however, is a systematic approach to solve mathematical and statistical problems posed by the internal structure. 25.2.1 Example plot(jj, type=&quot;o&quot;, main = &quot;Average global temperature deviations (1880–2015) in degrees Celsius&quot;,ylab=&quot;Quarterly Earnings per Share&quot;) The data are the global land–ocean temperature deviations (1880-2015), measured in degrees centigrade, from the 1951-1980 average. From the data, we can see that there is an obvious upward trend during the later twentieth century. This observation has been used as one of arguments for the global warming hypothesis. Except the overall upward trend, there are also some other peaks that we can question and analyze. Time series analysis provides tools and models to answer those questions of interest exposed by real experimental data taken from different subject areas. 25.3 Simple Time Series The main goal of time series analysis is to construct mathematical models to provide plausible descriptions for observed data. There are many time series models we can use and fit to understand the underlying structures of observed data and do forecasting, monitoring or feedback and feedforward control. Here, some simple time series are listed below. 25.3.1 Definition Before looking at different series, a clear definition of time series may be helpful. A time series is an ordered sequence of random variables obtained in time. For example, in a time series \\(x_1, x_2, x_3\\), …, the random variable \\(x_1\\) represents the value at the first time period, \\(x_2\\) represents the value at the second time period and so on. In this text, we will use \\(\\{x_t\\}\\) to denote a time series where t is time here and typically discrete (\\(t = 0, ±1, ±2\\), …, or some subset of integers). 25.3.2 White Noise White noise, \\(w_t\\), is a collection of uncorrelated random variables with mean 0 and variance \\(\\sigma^{2}_w\\). This process can be denoted as \\(w_t \\sim wn(0,\\sigma^{2}_w)\\). Sometimes the noise is independent and identically distributed (iid) with mean 0 and variance \\(\\sigma^{2}_w\\). We call this white independent noise or iid noise and \\(w_t \\sim iid(0,\\sigma^{2}_w)\\). There is also a useful white noise series named Gaussian white noise where \\(w_t\\) are independent normal random variables with mean 0 and variance \\(\\sigma^{2}_w\\) i.e. \\(w_t \\sim iid N(0,\\sigma^{2}_w)\\). The graph below shows a collection of 100 independent normal random variables with mean 0 and variance \\(\\sigma^{2}_w=1\\). w = rnorm(100,0,1) plot.ts(w, main=&quot;white noise&quot;) 25.3.3 Moving Average of White Noise We can replace the white noise \\(w_t\\) by a moving average smoothing the series. For instance, a moving average of white noise using three terms (current and immediate neighbors in the past and future) is \\[v_t=\\frac{1}{3}(w_{t-1}+w_t+w_{t+1})\\] The graph below uses the Gaussian white noise before by taking the average of three items and shows a smoother version of the white noise one. v = filter(w, sides=2, filter=rep(1/3,3)) plot.ts(v, ylim=c(-3,3), main=&quot;moving average&quot;) 25.3.4 Random Walk with Drift A random walk with drift is given by \\[x_t= δ+x_{t-1}+w_t\\] for t = 1, 2, …, with \\(x_0 = 0\\) and \\(w_t\\) is white noise. \\(δ\\) here is the drift and when it is zero, this series is simply called a random walk. Note that when \\(δ=0\\), the value at time t is just the value at time t-1 plus a complete random movement determined by \\(w_t\\). We can also rewrite the equation as a cumulative sum of white noise \\[x_t= δt+\\sum^{t}_{j=1}w_j \\] w = rnorm(100) x = cumsum(w) wd = w +.2 xd = cumsum(wd) plot.ts(xd, ylim=c(-20,30), main=&quot;random walk&quot;, ylab=&#39;&#39;) lines(x, col=4) This graph depicts two random walks with drift \\(δ=0.2\\) (black line) and \\(δ=0\\) (blue line). Here,\\(\\sigma^2_w =1\\) for the white noise. 25.3.5 AR(1) An autoregressive model of order 1 (AR(1)) is of the form \\[x_t= \\phi x_{t-1}+w_t\\] where \\(w_t \\sim wn(0,\\sigma^{2}_w)\\), \\(|\\phi|&lt;1\\) is a constant and \\(x_t\\) is weakly stationary i.e. its mean function is constant and its autocovariance function depends only on the difference between two time period \\(t_1\\) and \\(t_2\\). The definitions of mean and autocovariance will be clarified later. We can also write this equation as \\[x_t= \\phi x_{t-1}+w_t=\\phi(\\phi x_{t-2}+w_{t-1})+w_t \\\\ = ϕ^2 x_{t-2}+ϕw_{t-1}+ w_t=⋯\\\\ = \\phi^k x_{t-k}+ \\sum_{j=0}^{k-1}\\phi^j w_{t-j}= \\sum_{j=0}^{\\infty} \\phi^j w_{t-j}\\] Here, graphs of AR(1) with \\(\\phi=±0.9\\), \\(\\sigma_w^2=1\\) is provided. plot(arima.sim(list(order=c(1,0,0), ar=.9), n=100), ylab=&quot;x&quot;,main=(expression(AR(1)~~~phi==+.9))) plot(arima.sim(list(order=c(1,0,0), ar=-.9), n=100), ylab=&quot;x&quot;,main=(expression(AR(1)~~~phi==-.9))) The series with \\(\\phi=.9\\) is smoother than the series with \\(\\phi = −.9\\). We will talk about the reasons why these two have different levels of smoothness later when we go into ACF. 25.3.6 MA(1) A moving average model of order 1 (MA(1)) is of the form \\[ x_{t}=w_{t}+\\theta w_{t-1} \\] where \\(w_t \\sim wn(0,\\sigma^{2}_w)\\) and \\(\\theta\\) is a parameter. Notice that \\(x_t\\) here is stationary for any values of \\(\\theta\\). Here, graphs of MA(1) with \\(\\theta=±0.7\\), \\(\\sigma_w^2=1\\) is provided. plot(arima.sim(list(order=c(0,0,1), ma=.7), n=100), ylab=&quot;x&quot;, main=(expression(MA(1)~~~theta==+.7))) plot(arima.sim(list(order=c(0,0,1), ma=-.7), n=100), ylab=&quot;x&quot;, main=(expression(MA(1)~~~theta==-.7))) Similar to AR(1), the series with \\(\\theta=.7\\) is smoother than the series with \\(\\theta = −.7\\). 25.4 Measures of Dependence There are many informative measures to describe data and make people know more about the internal structure and correlation of the data. In this text, we will talk about mean function, autocovariance function and autocorrelation function (ACF). 25.4.1 Mean Function For a time series \\(x_t\\), its mean function is defined as \\[\\mu_{x_t}= E(x_t )= ∫_{-∞}^∞ xf_t (x)dx\\] 25.4.2 Autocovariance Function The autocovariance function is defined as \\[ \\gamma_{x}(s, t)=\\operatorname{cov}\\left(x_{s}, x_{t}\\right)=E\\left[\\left(x_{s}-\\mu_{s}\\right)\\left(x_{t}-\\mu_{t}\\right)\\right] \\] where s,t are two time points. The autocovariance function measures the linear dependence between values at two different time points in the same time series. If \\(\\gamma_x (s,t)=0\\), \\(x_s\\) and \\(x_t\\) are not linearly related. Also, very smooth time series will tend to have a large autocovariance function. 25.4.3 Autocorrelation Function (ACF) The autocorrelation function is defined as \\[ \\rho(s, t)=\\frac{\\gamma(s, t)}{\\sqrt{\\gamma(s, s) \\gamma(t, t)}} \\] The ACF shows the linear predictability of the value at time t using the value at time s in the time series. The value of ACF is between -1 and 1. For example, if \\(x_t=β_0+β_1 x_s\\) and we can perfectly predict \\(x_t\\) from \\(x_s\\), ACF will be +1 when \\(β_1&gt;0\\) and -1 when \\(β_1&lt;0\\). In this way, we can roughly forecast \\(x_t\\) from \\(x_s\\). 25.4.4 Three Measures for Simple Time Series Above 25.4.4.1 White Noise \\(w_t \\sim wn(0,\\sigma^{2}_w)\\) 25.4.4.1.1 Mean Function: \\[E(w_t)=0\\] 25.4.4.1.2 Autocovariance function: \\[ \\gamma(s, t)=\\left\\{\\begin{array}{c} 0, \\text { if } t \\neq s \\\\ \\sigma_{w}^{2}, \\text { if } t=s \\end{array}\\right. \\] 25.4.4.1.3 ACF: \\[ \\rho(s, t)=\\left\\{\\begin{array}{ll} 0, &amp; \\text { if } t \\neq s \\\\ 1, &amp; \\text { if } t=s \\end{array}\\right. \\] 25.4.4.2 Moving Average of White Noise Using Three Terms \\(v_t=\\frac{1}{3}(w_{t-1}+w_t+w_{t+1})\\) 25.4.4.2.1 Mean Function: \\[E(v_t)=0\\] 25.4.4.2.2 Autocovariance function: \\[ \\gamma(s, t)=\\left\\{\\begin{array}{ll} \\frac{3}{9} \\sigma_{w}^{2}, &amp; \\text { if } s=t \\\\ \\frac{2}{9} \\sigma_{w}^{2}, &amp; \\text { if }|s-t|=1 \\\\ \\frac{1}{9} \\sigma_{w}^{2}, &amp; \\text { if }|s-t|=2 \\\\ 0, &amp; \\text { if }|s-t|&gt;2 \\end{array}\\right. \\] 25.4.4.2.3 ACF: \\[ \\rho(s, t)=\\left\\{\\begin{array}{ll} 1, &amp; \\text { if } s=t \\\\ \\frac{2}{3}, &amp; \\text { if }|s-t|=1 \\\\ \\frac{1}{3}, &amp; \\text { if }|s-t|=2 \\\\ 0, &amp; \\text { if }|s-t|&gt;2 \\end{array}\\right. \\] 25.4.4.3 Random Walk with Drift \\(x_t= δt+\\sum_{j=1}^t w_j\\) 25.4.4.3.1 Mean Function: \\[E(x_t )= δt\\] 25.4.4.3.2 Autocovariance function with drift = 0: \\[ \\gamma(s, t)=\\min (s, t) \\sigma_{w}^{2} \\] 25.4.4.3.3 ACF with drift = 0: \\[ \\rho(s, t)=\\frac{\\min (s, t)}{\\sqrt{s t}} \\] 25.4.4.4 AR(1) \\(x_t= \\phi x_{t-1}+w_t\\) 25.4.4.4.1 Mean Function: \\[E(x_t)=0\\] 25.4.4.4.2 Autocovariance function: \\[ \\gamma(h)=\\frac{\\sigma_{w}^{2} \\phi^{h}}{1-\\phi^{2}}, h \\geq 0 \\] Since the time series is weakly stationary, \\(γ(0,h)= γ(h,0)= γ(t,t+h)= γ(s,s+h)=γ(h)\\). 25.4.4.4.3 ACF: \\[ \\rho(h)=\\phi^{h}, h=0,1,2, \\ldots \\] Take a look at the ACF of AR(1) when \\(ϕ= 0.9\\) ar = arima.sim(list(order=c(1,0,0), ar=.9), n=100) acf(ar,lag.max = 10) Take a look at the ACF of AR(1) when \\(ϕ= -0.9\\) ar = arima.sim(list(order=c(1,0,0), ar=-.9), n=100) acf(ar,lag.max = 10) From these two graphs, we can find out that when \\(\\phi\\) is positive, data at contiguous time points will be positively correlated with each other and close to each other. When \\(\\phi\\) is negative, data at contiguous time points will be negatively correlated with each other. If one observed data is positive, then the next observed data will be negative. This sample path will be choppy. This is the reason why we will have a smoother sample path when \\(\\phi\\) is positive than when \\(\\phi\\) is negative before. 25.4.4.5 MA(1) \\(x_{t}=w_{t}+\\theta w_{t-1}\\) 25.4.4.5.1 Mean Function: \\[E(x_t)=0\\] 25.4.4.5.2 Autocovariance function: \\[ \\gamma(h)=\\left\\{\\begin{array}{ll} \\left(1+\\theta^{2}\\right) \\sigma_{w}^{2} &amp; h=0 \\\\ \\theta \\sigma_{w}^{2} &amp; h=1 \\\\ 0 &amp; h&gt;1 \\end{array}\\right. \\] 25.4.4.5.3 ACF: \\[ \\rho(h)=\\left\\{\\begin{array}{ll} 1 &amp; h = 0\\\\ \\frac{\\theta}{\\left(1+\\theta^{2}\\right)} &amp; h=1 \\\\ 0 &amp; h&gt;1 \\end{array}\\right. \\] Take a look at the ACF of MA(1) when \\(\\theta= 0.7\\) ma = arima.sim(list(order=c(0,0,1), ma=.7), n=100) acf(ma, lag.max = 10) Take a look at the ACF of MA(1) when \\(\\theta= -0.7\\) ma = arima.sim(list(order=c(0,0,1), ma=-.7), n=100) acf(ma, lag.max = 10) We can see that ACF will be around zero when \\(h&gt;1\\). The only difference is that when \\(h=1\\), the value of ACF is positive if \\(\\theta &gt; 0\\) and negative if \\(\\theta &lt; 0\\). 25.4.5 Application These measures are good ways to analyze data and find suitable time series models that fit the data. Here is a simple example. The data shown in the image below records the annual precipitation (in inches) in Los Angeles from 1878 to 1992 and its ACF. data(&quot;larain&quot;) plot(larain, main = &quot;Annual Rainfall/Inches in Los Angeles 1878-1992&quot;) acf(larain) The ACF is 1 only when \\(h=0\\), and for other values of h, the values of ACF are near zero. It is very similar to the ACF of white noise with mean 0 and variance 1. We then create a white noise time series with 100 data and take a look at the ACF. w = rnorm(n=100,0,1) acf(w) Comparing the ACF of white noise and the ACF of the LA rain data, we can get a rough assumption that the LA rain time series may be a random and normal time series just like the white noise one. Although other measures need to be calculated to prove this data is white noise, the ACF values here do give us some ideas about how to analyze data. Next, if it is white noise, we can use a predict function to forecast future data though it does not have much meaning to do predictions by fitting the white noise model since each white noise data point is iid and random. 25.5 Reference https://online.stat.psu.edu/stat510/lesson/1/1.2 https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc4.htm Shumway, R. H., &amp; Stoffer, D. S. (2017). Time series analysis and its applications: With R examples. Cham, Switzerland: Springer. "],["intro-to-tableau-with-r.html", "Chapter 26 Intro to Tableau with R", " Chapter 26 Intro to Tableau with R Daniel Schmidle and Pranav Gopal With an increasing number of job listings requesting experience with Tableau for Data Scientist, Analyst and ML Engineer positions, it is becoming increasingly likely that we might find ourselves needing this tool as part of our repertoire. Since 2003, Tableau has been helping people see and understand data to create actionable insights from their analysis. And without a doubt, Tableau has easy and extensive data visualization capabilities. However, as we travel through the Tableau-verse in wonderment of all the beautiful graphs and charts, we still might feel as if something is missing: the computational, data manipulating, statistical power of R we have grown to love. Fortunately, through the Tableau Desktop and the Tableau Server we can still harness the power of R to create incredible data visualizations and analysis. Thus, here is how to start utilizing R in Tableau: Setup First, you will need R installed on your device (https://www.r-project.org/ ) and Tableau Desktop through the Tableau homepage (free for students: https://www.tableau.com/academic/students) In order to have Tableau to connect to R, the Rserve package must be installed and implemented in the RGui or RStudio Console: Tableau will now be able to find and connect to R through a socket server. Now, on the Tableau Desktop go to Help → Settings and Performance → Manage Analytics Extension Connection.. Set the extension Rserve, and set the Server: Localhost and Port: 6311 You can check the connection by the Test Connection And now you are all set up to use R in Tableau. So, let’s do one easy example to show how to use R functions in the Tableau Desktop. Creating Your First R Function To begin using the computational power of R we must first understand how Tableau implements R. Here, the common Tableau built-in Sample - Superstore is used for the base dataset in the example. Start, by creating a new calculated field (one easy way is to right click the Data panel on the left hand side and choose Create Calculated Field) Now, here is where the R magic happens: Inside the Calculated Field window, we can start by renaming our field, here we name it “First R Example (expenses)” Next, inside the function field we need to declare to Tableau what data type we want the R function to return. The types are SCRIPT_BOOL , SCRIPT_INT , SCRIPT_REAL , SCRIPT_STR . The functions list window on the right will allow you to select the data types / Tableau functions and provide further details for each: In the above example SCRIPT_REAL is chosen since we want floating point numbers to be returned. Next, Inside the SCRIPT_REAL(“ #Your R function goes here”,…) with each argument that you want to pass in being represented by .arg1 , .arg2 ,… , argN for N arguments. In this example the variable x is calculating the difference of .arg1 and .arg2 Next, the final part of the function SCRIPT_REAL(“ Your R function goes here”, specified data arguments for (.arg1, .arg2, etc) go here ). In this example SUM(Sales) is being passed to .arg1 and SUM([Profit]) is being passed to .arg2 so that x = SUM(Sales) - SUM([Profit]) Attention! Tableau will only allow aggregate data to be passed to the SCRIPT function, and thus R function. Therefore, the arguments must be of the form SUM() or AVG() etc. and not in vector format. This limits the normal R functionality however, the limitations become more versatile as aggregate manipulation occurs in Tableau. So, to finish off our first example we can now use the R output to visualize Sales on the First R Example (calculated expenses) by dragging the calculated expenses to the columns section, sales to the Rows, and Sub-Category to the Color sections: And now we have used R to create a graph in Tableau! Now, this simple calculation can be done in Tableau easily and the call to R is not needed in this case but, let’s take a look at another example where the power R really starts to show. Example 2 (getting some more use out of R) To get a little deeper with an R function let us create a linear model for predictions. Still using the Sample - Superstore data from the previous example, we create a new calculated field: Here, we are using the lm() function from R to create a linear model of SUM([Profit]) on AVG(Quantity), AVG(Sales), and from our first example [First R Example (calculated expenses)]. This linear model will estimate an expected profit for a category we choose. So, plotting Profit on Expected Profit with aggregated color by Sub-Category creates a nice visualization of the expected vs real outcomes from the model. To note here, a scatter plot was chosen to represent the model outcomes, but the possibilities of other plot types are just a click away in tableau. From this example we can start to see the possibilities of R computation within Tableau. Being able to utilize R in Tableau creates a more dynamic analytical environment however, Tableau’s use of R as a calculated field is limited in many ways. First, as noted above, the SCRIPT Function in Tableau can only take aggregate input, such as the SUM or AVG, and will not accept a vector for computation. So, when using R inside Tableau think more point calculations than Monte Carlo simulations. There is however a way around these limitations that uses R to calculate what you want first, then utilize the newly formed dataset to create a more dynamic integration in Tableau. Example 3 (The R Tableau Tango) A great example of how easy Tableau is to create dynamic, visually stunning graphs is no better demonstrated than with geospatial data. Tableau can create amazing, interactive visualizations for any region with just a couple clicks of the mouse and the right data. For this example we used a Covid-19 dataset taken from the JHU CSSE COVID-19 Data as well as The New York Times. https://data.world/covid-19-data-resource-hub/covid-19-case-counts Tableau easily creates an interactive time series visualizing the growth of positive cases by country. Using the Tableau generated latitude and longitude creates the geospatial map, selecting Report Date on the Pages tab creates the time series for cases per day, and adding the SUM(People Positive Cases Count) and the Country Alpha 3 Code to the Marks options creates the visualization for case growth. Another benefit of Tableau is the ease of formatting: changing color patterns, background colors, and plot styles which can be seen in the plot below. But, say we wanted to do a point estimate for the Death Rate parameter of Covid-19 by region. And let’s say we wanted to use bootstrapping from the daily reported data. This sort of computation is where R shines. So, since this type of simulation would not be possible from the Tableau Desktop, using R separately and feeding the result back to Tableau would create the perfect team for all your statistical needs. So, lets begin: Using the ‘Covid-19 Activity.csv’ data in Rmarkdown, the first pre-processing step is to calculate the Death Rate per day for each country. We then eliminated all data that had a Death Rate of NaN or Infinity using the tidyverse and dplyr R packages. Looping through every country in the dataframe, we used the base R replicate and sample functions to bootstrap the Death Rate data for each country, taking 100 samples over 1000 repetitions and calculating the mean point estimated Death Rate per country. Writing this calculated pre-processed dataset back to Tableau and plotting the new data in conjunction with the geospatial plot produces the image below: library(tidyverse) library(dplyr) covid &lt;- read.csv(&quot;data/COVID-19 Activity.csv&quot;) covid_clean &lt;- select(covid, 1, 4, 10, 13) covid_clean$Death_Rate &lt;- covid_clean$PEOPLE_DEATH_COUNT/covid_clean$PEOPLE_POSITIVE_CASES_COUNT covid_clean &lt;- do.call(data.frame, lapply(covid_clean, function(x) replace(x, is.infinite(x), NA))) covid_clean &lt;- covid_clean %&gt;% drop_na(&quot;Death_Rate&quot;) countries &lt;- unique(covid_clean$COUNTRY_SHORT_NAME) Est_Death_Rate &lt;- rep(NA,length(countries)) death_tbl &lt;- data.frame(countries, Est_Death_Rate) for (c in countries){ country &lt;- covid_clean[covid_clean$COUNTRY_SHORT_NAME == c,] c_death &lt;- replicate(1000, mean(sample(country$Death_Rate, 100, replace = TRUE))) death_tbl$Est_Death_Rate[death_tbl$countries == c] &lt;- mean(c_death) } #write.csv(death_tbl,&quot;data/death_tbl.csv&quot;) It is clear from the above examples that Tableau is extremely user-friendly in data visualization. It allows users to create beautiful and colorful plots with no coding knowledge at all. Through it’s drag and drop environment, users can easily change color palettes, filter plots by certain variables, and resize plots instead of having to write time-consuming R code to achieve the same result. However, Tableau’s limited ability for data manipulation makes it much better suited for simple calculations and plotting data that is already pre-processed. Example 2 showcased the ability to use R within Tableau for more advanced capabilities such as linear regression and other machine learning models. Additionally, Example 3 dealt with statistical analysis of bootstrapping Covid-19 data to calculate a point estimated Death Rate per country. This is where combining the packages available in R to pre-process the data, and then passing it back to Tableau creates a powerhouse for statistical analysis and data visualization. All three of these examples showcase the capabilities of using Tableau and R together. Combining the analytical ability of R packages with the drag and drop data visualization of Tableau will make you a better data scientist, and hopefully will help you land the job of your dreams! Sources https://www.tableau.com/learn/whitepapers/using-r-and-tableau https://www.youtube.com/watch?v=E092x4a9_Y4 "],["sentiment-analysis-and-wordcloud.html", "Chapter 27 Sentiment analysis and wordcloud 27.1 Read and explore the text 27.2 Sentiment analysis 27.3 Wordcloud", " Chapter 27 Sentiment analysis and wordcloud Yuao Zhao library(qdap) library(tidyverse) library(dplyr) library(tidyr) library(stringr) library(tidytext) library(ggplot2) library(ggthemes) library(wordcloud) Sentiment analysis is an important part of the emotion computing and wordcloud is a fancy way of text visualization. Combining the two, we can reveal and display people’s attitude and perspectives through their comments or articles. This tutorial will go through the basic process of these two by analysing the video game comments dataset. About the Data This dataset is based on a set of Amazon review of video games downloaded from Prof Julian McAuley’s website (http://jmcauley.ucsd.edu/data/amazon/). Since this dataset is very large, we only included data from Aug 2013 to July 2014 which gives us a year of data to work with. The data we are going to use includes the following fields: * id: A unique identifier for each review * review: Text of review posted on Amazon * review_rating: Each review on Amazon is rated by others using a five-star scale (presumably based on helpfulness of review) 27.1 Read and explore the text videogame = read.csv(&#39;resources/sentiment_analysis_and_wordcloud/videogame_review.csv&#39;,stringsAsFactors = F) head(videogame) ## id ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## review ## 1 1st shipment received a book instead of the game.2nd shipment got a FAKE one. Game arrived with a wrong key inside on sealed box. I got in contact with codemasters and send them pictures of the DVD and the content. They said nothing they can do its a fake DVD.Returned it good bye.! ## 2 I still haven&#39;t figured this one out. Did everything as instructed but the game never installed. Strange. Since I don&#39;t like to rate on something I couldn&#39;t get to work, I&#39;ll just say that I didn&#39;t like it because I couldn&#39;t get it to work :P For those that did manage to get it installed, I&#39;m sure it&#39;s a great game. I had the first Dirt and it was a scream! Too bad this one bombed. Maybe a bad disc... ## 3 I bought this and the key didn&#39;t work. It was a gift, and the recipient wasn&#39;t able to solve the problem. It might have been a good game, but I never found out because the key failed. ## 4 I love it! Use it all the time. Really works perfectly with all games that you need a mic for. ## 5 my grandkids (and their parents) love playing with the microphone along with the guitar we got.this really is a lot of fun for everyone. would totally recommend getting this if someone has an interest in music (singing)... ## 6 The greatest bait and switch betrayal of a fanbase in videogaming history and most hated game of all time.To begin I will provide Amazon&#39;s customer base the &amp;#34;tale of the turd&amp;#34; as I call it in relation to this abomination&#39;s &amp;#34;quality&amp;#34; or rather lackthereof:Gameplay: Utterly challengeless. A lobotomy patient, a two year old drooling baby in a feces filled diaper or even an individual with a severe case of down syndrome could easily complete this game in a single sitting with minimal effort. Just mash buttons or &amp;#34;spam&amp;#34; the controls as PC gamers would say and you&#39;ll eventually complete this atrocious slap in the face Disney level Aonuma dreck.Graphics: Hideous, vomit inducing, ridiculous, teletubbies-esque, bait and switch from what we were originally promised at Spaceworld 2000 (realistic style epic, cool and beautiful graphics with a decidedly Lord Of The Rings style flare) Also there are ugly and gaudy white lines that appear all throughout Wand Wacker indicating Nintendo was not able to polish this turd before release. Oh what a shame.Sound: The only redeemable inspiring thing about this Aonuma dud is the ocean music likely composed by Koji Kondo. The rest is detestable Saturday morning cartoon level kiddy drivel. Soccer moms and grandparents should approve. Real Zelda fans can only vomit at this level of disrespect Nintendo has stooped to in order to insult us.Theme: Near total rip off of Kevin Costner&#39;s Water World to such a great extent I fully believe Costner could probably win a lawsuit against Nintendo for royalties if he so desired. Needless to say this watery environment is not what true Zelda fans look forward to when we go to pick up a new Zelda game. No, what we&#39;re looking for is the green forests and fields of Hyrule proper, not some watery grave that sits atop it for reasons likely relating to Aonuma&#39;s child learning how to bathe for the first time and Aonuma getting inspired to make Zelda into a giant bathing simulator via Wand Wacker. I&#39;ve said it before and I&#39;ll say it again Nintendo needs to fire Eiji Aonuma and his entire Zelda team and get normal people developing the Zelda series again for it to return to health.Commentary: The online game journalists also known as professional liars can attempt to re-write history all they like but The Legend Of Celda: Wand Wacker will go down as the most hated videogame of all time and the greatest betrayal of a fanbase in gaming history.This was Nintendo, Miyamoto and Eiji Aonuma&#39;s Vince Mcmahon esque &amp;#34;Screw you!&amp;#34; to the true Zelda fanbase and by true Zelda fanbase I&#39;m not referring to the scumbag sycophants that populate online gaming message forums but the market in general who determines whether videogames become massive successes or massive failures and the true Zelda fans spoke loud and clear not once but twice.Two whole times on that Wand Wacker piece of dog excrement first with its original Gamecube release and second with its Wii U HD re-release that this is not the way that they envisioned the Legend Of Zelda series, this is not the way they wanted it to go and as a result Wand Wacker flopped mightily BOTH times as such a Benedict Arnold or Rosenbergs esque betrayal is destined to do.To rephrase a quote from Ridley Scott&#39;s Gladiator: &amp;#34;Marcus Aurelius had a dream that was Zelda and this is not it, THIS IS NOT IT!&amp;#34;We were lied to and led to believe that the epic, cool, badass, Spaceworld 2000 realistically presented Zelda would be what we would soon be getting on the Gamecube but then much later the bait and switch hideously deformed Celda Wand Wacker made its debut to boos and hisses worldwide and massive hate on the internet was poured it and Nintendo&#39;s way (before the damage control brainwashing took effect which has left a dishonest internet forum gaming community in its wake who pretends to love Wand Wacker in order to shill and run damage control for Nintendo while real Zelda fans have abandoned Nintendo due to its betrayal of them for the other consoles or PC gaming if they haven&#39;t simply upped and left gaming in disgusting over this nasty unnecessary debacle and betrayal of Zelda&#39;s true fans.Eiji Aonuma the hack no talent Jimmy Fallon Of Japan enabled by the out of touch Shigeru Miyamoto has been ruining the Zelda series since he took up its reigns to create the embarrassment known as Majora&#39;s Mask.Aonuma&#39;s first game &amp;#34;Marvelous&amp;#34; failed (MARVELOUSLY!) and he had no credentials at all to be given the Zelda series but insanely Nintendo gave it to him and ever since he&#39;s been driving Zelda along with it&#39;s sales figures into the ground making one boring puzzle filled bad NPC filled faux Zelda game after another.When the truth of the matter is Zelda is supposed to be an arcade action adventure sword and sorcery game with light RPG elements thrown in for good measure.Aonuma completely butchers this formula in favor of the boring garbage gameplay elements he prefers and the market lets him know what it thinks of his efforts by making sure his version of Zelda ends up in the bomba discount gaming bins where such low quality trash belongs.In anycase to re-iterate so as to set this very important point in stone once more:Wand Wacker offers up literally no gameplay challenge whatsoever and can be completed by a lobotomy patient (as is typical of Aonuma offerings).Conclusion: My rating of this turd is 1 star since I can&#39;t give it lower due to how Amazon&#39;s system is set up. ## review_rating ## 1 1 ## 2 2 ## 3 1 ## 4 5 ## 5 5 ## 6 1 27.1.1 Explore Ratings of Reviews videogame %&gt;% summarize(average_rating = mean(review_rating), median_rating = median(review_rating)) ## average_rating median_rating ## 1 4.423333 5 The median_rating is 5 and the average of the rating is 4.42 27.1.2 Distribution of Ratings ggplot(data=videogame,aes(x=review_rating))+ geom_bar(fill=&#39;sienna&#39;)+ theme_economist() As we can see from the chart, few people gave the extreme low scores. 27.1.3 Explore the reviews Regular expressions (regex) is a framework for teaching a computer how to recognize patterns of text. In order to count sentences, one needs to define what a sentence is. If we defined a sentence as “a set of characters or punctuation (comma, quote) or spaces that end with one or more period, question mark, exclamation mark or combination of them”, the following reqular expression would match the sentence: [A-Za-z,;’\"]+[^.!?]*[.?!] # Characters across all reviews videogame%&gt;% summarize(mean_character = mean(nchar(review)), median_character = median(nchar(review))) ## mean_character median_character ## 1 582.81 191.5 # Words across all reviews videogame%&gt;% summarize(mean_words = mean(str_count(string = review,pattern = &#39;\\\\S+&#39;)), median_words = median(str_count(string = review,pattern = &#39;\\\\S+&#39;))) ## mean_words median_words ## 1 106.7833 37 # Sentences across all reviews videogame%&gt;% summarize(mean_sentences = mean(str_count(string = review,pattern = &quot;[A-Za-z,;&#39;\\&quot;\\\\s]+[^.!?]*[.?!]&quot;)), median_sentences = median(str_count(string = review,pattern = &quot;[A-Za-z,;&#39;\\&quot;\\\\s]+[^.!?]*[.?!]&quot;))) ## mean_sentences median_sentences ## 1 5.84 3 On average, there are 583 characters, 106 words, 6 sentences across all reviews. 27.1.4 Most common words most_common_word = freq_terms(text.var=videogame$review,top=25,stopwords = c(Top200Words,&quot;game&quot;,&quot;games&quot;,&quot;playing&quot;,&quot;it&#39;s&quot;,&quot;i&#39;ve&quot;,&quot;i&#39;m&quot;,&quot;s&quot;,&quot;d&quot;)) ggplot(most_common_word,aes(x = fct_reorder(WORD, FREQ, .desc=FALSE),y=FREQ))+ geom_bar(stat = &quot;identity&quot;,fill = &quot;steelblue&quot;)+ xlab(&quot;Word&quot;)+ ylab(&quot;Frequency&quot;)+ theme_classic()+ coord_flip() By examining the top 25 words, we can conclude that the most popular games are Mario and Zelda. Also most words are good-comment-word, such as fun, love, best. 27.2 Sentiment analysis Let us classify the words to gain a better understanding of the reviews. For this, we will use the dplyr and tidytext packages. We will make use of the unnest_tokens function from tidytext to tokenize the reviews and the following dplyr functions to organize the data: select, group_by, ungroup and count. Later on, we will also use the tidyr package to reshape the data. 27.2.1 Words in reviews videogame %&gt;% select(id,review) %&gt;% unnest_tokens(output = word,input=review) %&gt;% group_by(id) %&gt;% count() %&gt;% head() ## # A tibble: 6 x 2 ## # Groups: id [6] ## id n ## &lt;int&gt; &lt;int&gt; ## 1 1 54 ## 2 2 80 ## 3 3 37 ## 4 4 20 ## 5 5 37 ## 6 6 949 27.2.2 Word lexicons —- Bing There are a number of word lexicons that can be used to classify words as being positive or negative. The bing lexicon categorizes words as being positive and negative. pos_neg = videogame%&gt;% select(id,review)%&gt;% unnest_tokens(output = word, input = review)%&gt;% inner_join(get_sentiments(&#39;bing&#39;)) head(pos_neg) ## id word sentiment ## 1 1 fake negative ## 2 1 wrong negative ## 3 1 fake negative ## 4 1 good positive ## 5 2 strange negative ## 6 2 like positive ggplot(pos_neg,aes(x=sentiment,fill=sentiment))+ geom_bar()+ guides(fill = F)+ theme_economist() The amount of positive words is almost as twice as the amount of negative words. rating_sentiment = videogame %&gt;% select(id,review,review_rating)%&gt;% unnest_tokens(output=word,input=review)%&gt;% inner_join(get_sentiments(&#39;bing&#39;))%&gt;% group_by(review_rating,sentiment)%&gt;% summarize(amount = n())%&gt;% mutate(proportion = amount/sum(amount)) rating_sentiment %&gt;% ggplot(aes(x=review_rating,y=proportion,fill=sentiment))+ geom_col()+ theme_economist() The proportion of positive words increases as the review_rating growth. Ratings of 3 and 4 are barely the same in terms of the positive-negative structure of people’s comments. 27.2.3 Word lexicons —- nrc A word may reflect more than just valence. The ‘nrc’ lexicon categorizes words by emotion. Please note that the next two code chunks must be run interactively since they require the user to consent to the license agreement for using the nrc lexicon. See here for more detail: https://github.com/EmilHvitfeldt/textdata/issues/19 Emotion in reviews videogame %&gt;% select(id,review) %&gt;% unnest_tokens(output = word, input = review) %&gt;% inner_join(get_sentiments(&#39;nrc&#39;)) %&gt;% group_by(sentiment) %&gt;% count() Let’s visualize videogame %&gt;% select(id,review)%&gt;% unnest_tokens(output = word, input = review)%&gt;% inner_join(get_sentiments(&#39;nrc&#39;))%&gt;% group_by(sentiment)%&gt;% count()%&gt;% ggplot(aes(x=reorder(sentiment,X = n),y=n,fill=sentiment))+ geom_col()+ guides(fill=F)+ coord_flip()+ theme_wsj() 27.3 Wordcloud Wordclouds sometime could offer little insight into the data, yet they tend to be very good at capturing interest of non-technical audiences. 27.3.1 Simple Word Cloud Visualization wordcloudData1 = videogame%&gt;% select(id,review)%&gt;% unnest_tokens(output=word,input=review)%&gt;% anti_join(stop_words)%&gt;% group_by(word)%&gt;% summarize(freq = n())%&gt;% arrange(desc(freq))%&gt;% ungroup()%&gt;% data.frame() set.seed(617) wordcloud(words = wordcloudData1$word,wordcloudData1$freq,scale=c(2,0.5),max.words = 100,colors=brewer.pal(9,&quot;Spectral&quot;)) 27.3.2 Comparison Word Cloud wordcloudData2 = videogame%&gt;% select(id,review)%&gt;% unnest_tokens(output=word,input=review)%&gt;% anti_join(stop_words)%&gt;% inner_join(get_sentiments(&#39;bing&#39;))%&gt;% count(sentiment,word,sort=T)%&gt;% spread(key=sentiment,value = n,fill=0)%&gt;% data.frame() rownames(wordcloudData2) = wordcloudData2[,&#39;word&#39;] wordcloudData2 = wordcloudData2[,c(&#39;positive&#39;,&#39;negative&#39;)] set.seed(617) comparison.cloud(term.matrix = wordcloudData2,scale = c(2,0.5),max.words = 200, rot.per=0) Hope you can enjoy this tutorial and explore more on text mining. "],["twitter-sentiment-analysis-in-r.html", "Chapter 28 Twitter sentiment analysis in R 28.1 Loading all the required R libraries 28.2 Sentiment Analysis 28.3 Twitter authorization to extract tweets: 28.4 Extracting Global Warming tweets: 28.5 Frequency of Tweets 28.6 Estimating Sentiment Score 28.7 Histogram of sentiment scores: 28.8 Barplot of sentiment type: 28.9 Wordcloud: 28.10 Word Frequency plot: 28.11 Network Analysis 28.12 References:", " Chapter 28 Twitter sentiment analysis in R Shivani Modi and Sriram Dommeti 28.1 Loading all the required R libraries library(twitteR) library(ROAuth) library(hms) library(lubridate) library(tidytext) library(tm) library(wordcloud) library(igraph) library(glue) library(networkD3) library(rtweet) library(plyr) library(stringr) library(ggplot2) library(ggeasy) library(plotly) library(dplyr) library(hms) library(lubridate) library(magrittr) library(tidyverse) library(janeaustenr) library(widyr) 28.2 Sentiment Analysis Sentiment analysis gives us insight into the things that automate mining of attitudes, opinions, views and emotions from text, speech, tweets and database sources. However, to fully explore the possibilities of this text analysis technique, we need data visualization tools to help organize the results. Visually representing the content of a text document is one of the most important tasks in the field of text mining. However, there are some gaps between visualizing unstructured (text) data and structured data. Many text visualizations do not represent the text directly, they represent an output of a language model. In this post, we will use tweets extracted using Twitter API, store tweets as text data, classify opinions in text into categories like positive, or negative or neutral, create a function to calculate the score of each type of opinion in the text and try to explore and visualize as much as we can, using R libraries. Tweets can be imported into R using Twitter API, then the text data has to be cleaned before analysis, for example removing emoticons, removing URLs, etc. 28.3 Twitter authorization to extract tweets: As a first step, we need to get authorized credentials from Twitter to use the API for extracting the tweets. Steps involve creating a Twitter developer account, creating an app and then we have necessary credentials. Reference for obtaining access tokens: https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html #Note: Replace below with your credentials following above reference api_key &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxx&quot; api_secret &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; access_token &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; access_token_secret &lt;- &quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot; #Note: This will ask us permission for direct authentication, type &#39;1&#39; for yes: setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret) ## [1] &quot;Using direct authentication&quot; 28.4 Extracting Global Warming tweets: # extracting 4000 tweets related to global warming topic tweets &lt;- searchTwitter(&quot;#globalwarming&quot;, n=4000, lang=&quot;en&quot;) n.tweet &lt;- length(tweets) # convert tweets to a data frame tweets.df &lt;- twListToDF(tweets) tweets.txt &lt;- sapply(tweets, function(t)t$getText()) # Ignore graphical Parameters to avoid input errors tweets.txt &lt;- str_replace_all(tweets.txt,&quot;[^[:graph:]]&quot;, &quot; &quot;) ## pre-processing text: clean.text = function(x) { # convert to lower case x = tolower(x) # remove rt x = gsub(&quot;rt&quot;, &quot;&quot;, x) # remove at x = gsub(&quot;@\\\\w+&quot;, &quot;&quot;, x) # remove punctuation x = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, x) # remove numbers x = gsub(&quot;[[:digit:]]&quot;, &quot;&quot;, x) # remove links http x = gsub(&quot;http\\\\w+&quot;, &quot;&quot;, x) # remove tabs x = gsub(&quot;[ |\\t]{2,}&quot;, &quot;&quot;, x) # remove blank spaces at the beginning x = gsub(&quot;^ &quot;, &quot;&quot;, x) # remove blank spaces at the end x = gsub(&quot; $&quot;, &quot;&quot;, x) # some other cleaning text x = gsub(&#39;https://&#39;,&#39;&#39;,x) x = gsub(&#39;http://&#39;,&#39;&#39;,x) x = gsub(&#39;[^[:graph:]]&#39;, &#39; &#39;,x) x = gsub(&#39;[[:punct:]]&#39;, &#39;&#39;, x) x = gsub(&#39;[[:cntrl:]]&#39;, &#39;&#39;, x) x = gsub(&#39;\\\\d+&#39;, &#39;&#39;, x) x = str_replace_all(x,&quot;[^[:graph:]]&quot;, &quot; &quot;) return(x) } cleanText &lt;- clean.text(tweets.txt) # remove empty results (if any) idx &lt;- which(cleanText == &quot; &quot;) cleanText &lt;- cleanText[cleanText != &quot; &quot;] 28.5 Frequency of Tweets tweets.df %&lt;&gt;% mutate( created = created %&gt;% # Remove zeros. str_remove_all(pattern = &#39;\\\\+0000&#39;) %&gt;% # Parse date. parse_date_time(orders = &#39;%y-%m-%d %H%M%S&#39;) ) tweets.df %&lt;&gt;% mutate(Created_At_Round = created%&gt;% round(units = &#39;hours&#39;) %&gt;% as.POSIXct()) tweets.df %&gt;% pull(created) %&gt;% min() ## [1] &quot;2021-03-20 14:55:43 UTC&quot; tweets.df %&gt;% pull(created) %&gt;% max() ## [1] &quot;2021-03-26 06:34:05 UTC&quot; plt &lt;- tweets.df %&gt;% dplyr::count(Created_At_Round) %&gt;% ggplot(mapping = aes(x = Created_At_Round, y = n)) + theme_light() + geom_line() + xlab(label = &#39;Date&#39;) + ylab(label = NULL) + ggtitle(label = &#39;Number of Tweets per Hour&#39;) plt %&gt;% ggplotly() 28.6 Estimating Sentiment Score There are many resources describing methods to estimate sentiment. For the purpose of this tutorial, we will use a very simple algorithm which assigns sentiment score of the text by simply counting the number of occurrences of “positive” and “negative” words in a tweet. Hu &amp; Liu have published an “Opinion Lexicon” that categorizes approximately 6,800 words as positive or negative, which can be downloaded from this link: http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 28.6.1 Loading sentiment word lists: positive = scan(&#39;resources/twitter_sentiment_analysis/positive-words.txt&#39;, what = &#39;character&#39;, comment.char = &#39;;&#39;) negative = scan(&#39;resources/twitter_sentiment_analysis/negative-words.txt&#39;, what = &#39;character&#39;, comment.char = &#39;;&#39;) # add your list of words below as you wish if missing in above read lists pos.words = c(positive,&#39;upgrade&#39;,&#39;Congrats&#39;,&#39;prizes&#39;,&#39;prize&#39;,&#39;thanks&#39;,&#39;thnx&#39;, &#39;Grt&#39;,&#39;gr8&#39;,&#39;plz&#39;,&#39;trending&#39;,&#39;recovering&#39;,&#39;brainstorm&#39;,&#39;leader&#39;) neg.words = c(negative,&#39;wtf&#39;,&#39;wait&#39;,&#39;waiting&#39;,&#39;epicfail&#39;,&#39;Fight&#39;,&#39;fighting&#39;, &#39;arrest&#39;,&#39;no&#39;,&#39;not&#39;) 28.6.2 Sentiment scoring function: score.sentiment = function(sentences, pos.words, neg.words, .progress=&#39;none&#39;) { require(plyr) require(stringr) # we are giving vector of sentences as input. # plyr will handle a list or a vector as an &quot;l&quot; for us # we want a simple array of scores back, so we use &quot;l&quot; + &quot;a&quot; + &quot;ply&quot; = laply: scores = laply(sentences, function(sentence, pos.words, neg.words) { # clean up sentences with R&#39;s regex-driven global substitute, gsub() function: sentence = gsub(&#39;https://&#39;,&#39;&#39;,sentence) sentence = gsub(&#39;http://&#39;,&#39;&#39;,sentence) sentence = gsub(&#39;[^[:graph:]]&#39;, &#39; &#39;,sentence) sentence = gsub(&#39;[[:punct:]]&#39;, &#39;&#39;, sentence) sentence = gsub(&#39;[[:cntrl:]]&#39;, &#39;&#39;, sentence) sentence = gsub(&#39;\\\\d+&#39;, &#39;&#39;, sentence) sentence = str_replace_all(sentence,&quot;[^[:graph:]]&quot;, &quot; &quot;) # and convert to lower case: sentence = tolower(sentence) # split into words. str_split is in the stringr package word.list = str_split(sentence, &#39;\\\\s+&#39;) # sometimes a list() is one level of hierarchy too much words = unlist(word.list) # compare our words to the dictionaries of positive &amp; negative terms pos.matches = match(words, pos.words) neg.matches = match(words, neg.words) # match() returns the position of the matched term or NA # we just want a TRUE/FALSE: pos.matches = !is.na(pos.matches) neg.matches = !is.na(neg.matches) # TRUE/FALSE will be treated as 1/0 by sum(): score = sum(pos.matches) - sum(neg.matches) return(score) }, pos.words, neg.words, .progress=.progress ) scores.df = data.frame(score=scores, text=sentences) return(scores.df) } 28.6.3 Calculating the sentiment score: analysis &lt;- score.sentiment(cleanText, pos.words, neg.words) # sentiment score frequency table table(analysis$score) ## ## -5 -4 -3 -2 -1 0 1 2 3 4 ## 2 14 81 191 591 2259 674 128 52 8 28.7 Histogram of sentiment scores: analysis %&gt;% ggplot(aes(x=score)) + geom_histogram(binwidth = 1, fill = &quot;lightblue&quot;)+ ylab(&quot;Frequency&quot;) + xlab(&quot;sentiment score&quot;) + ggtitle(&quot;Distribution of Sentiment scores of the tweets&quot;) + ggeasy::easy_center_title() Analysis: From the Histogram of Sentiment scores, we can see that around half of the tweets have sentiment score as zero i.e. Neutral and overall as expected, the distribution depicts negative sentiment in the tweets related to global warming, since it is a major issue of concern. 28.8 Barplot of sentiment type: neutral &lt;- length(which(analysis$score == 0)) positive &lt;- length(which(analysis$score &gt; 0)) negative &lt;- length(which(analysis$score &lt; 0)) Sentiment &lt;- c(&quot;Positive&quot;,&quot;Neutral&quot;,&quot;Negative&quot;) Count &lt;- c(positive,neutral,negative) output &lt;- data.frame(Sentiment,Count) output$Sentiment&lt;-factor(output$Sentiment,levels=Sentiment) ggplot(output, aes(x=Sentiment,y=Count))+ geom_bar(stat = &quot;identity&quot;, aes(fill = Sentiment))+ ggtitle(&quot;Barplot of Sentiment type of 4000 tweets&quot;) Analysis: It is also clear from this barplot of sentiment type that around half of the tweets have sentiment score as zero i.e. Neutral and there are more negative sentiment tweets than that of positive sentiment. This barplot helps us to identify overall opinion of the people about global warming. 28.9 Wordcloud: text_corpus &lt;- Corpus(VectorSource(cleanText)) text_corpus &lt;- tm_map(text_corpus, content_transformer(tolower)) text_corpus &lt;- tm_map(text_corpus, function(x)removeWords(x,stopwords(&quot;english&quot;))) text_corpus &lt;- tm_map(text_corpus, removeWords, c(&quot;global&quot;,&quot;globalwarming&quot;)) tdm &lt;- TermDocumentMatrix(text_corpus) tdm &lt;- as.matrix(tdm) tdm &lt;- sort(rowSums(tdm), decreasing = TRUE) tdm &lt;- data.frame(word = names(tdm), freq = tdm) set.seed(123) wordcloud(text_corpus, min.freq = 1, max.words = 100, scale = c(2.2,1), colors=brewer.pal(8, &quot;Dark2&quot;), random.color = T, random.order = F) Analysis: Wordcloud helps us to visually understand the important terms frequently used in the tweets related to global warming, here for example, “climate change”, “environmental”, “temperature”, “emissions”, etc. 28.10 Word Frequency plot: ggplot(tdm[1:20,], aes(x=reorder(word, freq), y=freq)) + geom_bar(stat=&quot;identity&quot;) + xlab(&quot;Terms&quot;) + ylab(&quot;Count&quot;) + coord_flip() + theme(axis.text=element_text(size=7)) + ggtitle(&#39;Most common word frequency plot&#39;) + ggeasy::easy_center_title() Analysis: we can infer that the most frequently used terms in the tweets related to global warming are, “climate”, “climatechange”, “since”, “biggest”, “hoax”, etc. 28.11 Network Analysis We are using a weighted network (graph) to describe how to encode and visualize text data. In this section we are counting pairwise relative occurrence of words. 28.11.1 Bigram analysis and Network definition Bigram counts pairwise occurrences of words which appear together in the text. #bigram bi.gram.words &lt;- tweets.df %&gt;% unnest_tokens( input = text, output = bigram, token = &#39;ngrams&#39;, n = 2 ) %&gt;% filter(! is.na(bigram)) bi.gram.words %&gt;% select(bigram) %&gt;% head(10) ## bigram ## 1 rt wemeantoclean ## 2 wemeantoclean we&#39;re ## 3 we&#39;re expanding ## 4 expanding our ## 5 our nursery ## 6 nursery of ## 7 of nativetree ## 8 nativetree with ## 9 with reuse ## 10 reuse of extra.stop.words &lt;- c(&#39;https&#39;) stopwords.df &lt;- tibble( word = c(stopwords(kind = &#39;es&#39;), stopwords(kind = &#39;en&#39;), extra.stop.words) ) Next, we filter for stop words and remove white spaces. bi.gram.words %&lt;&gt;% separate(col = bigram, into = c(&#39;word1&#39;, &#39;word2&#39;), sep = &#39; &#39;) %&gt;% filter(! word1 %in% stopwords.df$word) %&gt;% filter(! word2 %in% stopwords.df$word) %&gt;% filter(! is.na(word1)) %&gt;% filter(! is.na(word2)) Finally, we group and count by bigram. bi.gram.count &lt;- bi.gram.words %&gt;% dplyr::count(word1, word2, sort = TRUE) %&gt;% dplyr::rename(weight = n) bi.gram.count %&gt;% head() ## word1 word2 weight ## 1 globalwarming amp 439 ## 2 stop globalwarming 418 ## 3 wants 2 416 ## 4 2 help 415 ## 5 amp destruction 415 ## 6 earth wants 415 Let us plot the distribution of the weightvalues: bi.gram.count %&gt;% ggplot(mapping = aes(x = weight)) + theme_light() + geom_histogram() + labs(title = &quot;Bigram Weight Distribution&quot;) Note that it is very skewed, for visualization purposes it might be a good idea to perform a transformation, eg log transform: bi.gram.count %&gt;% mutate(weight = log(weight + 1)) %&gt;% ggplot(mapping = aes(x = weight)) + theme_light() + geom_histogram() + labs(title = &quot;Bigram log-Weight Distribution&quot;) In order to define weighted network from a bigram count we used the following structure. Each word is going to represent a node. Two words are going to be connected if they appear as a bigram. The weight of an edge is the number of times the bigram appears in the corpus. 28.11.2 Network visualization threshold &lt;- 50 # For visualization purposes we scale by a global factor. ScaleWeight &lt;- function(x, lambda) { x / lambda } network &lt;- bi.gram.count %&gt;% filter(weight &gt; threshold) %&gt;% mutate(weight = ScaleWeight(x = weight, lambda = 2E3)) %&gt;% graph_from_data_frame(directed = FALSE) plot( network, vertex.size = 1, vertex.label.color = &#39;black&#39;, vertex.label.cex = 0.7, vertex.label.dist = 1, edge.color = &#39;gray&#39;, main = &#39;Bigram Count Network&#39;, sub = glue(&#39;Weight Threshold: {threshold}&#39;), alpha = 50 ) We can even improvise the representation by setting the sizes of the nodes and the edges by the degree and weight respectively. V(network)$degree &lt;- strength(graph = network) # Compute the weight shares. E(network)$width &lt;- E(network)$weight/max(E(network)$weight) plot( network, vertex.color = &#39;lightblue&#39;, # Scale node size by degree. vertex.size = 2*V(network)$degree, vertex.label.color = &#39;black&#39;, vertex.label.cex = 0.6, vertex.label.dist = 1.6, edge.color = &#39;gray&#39;, # Set edge width proportional to the weight relative value. edge.width = 3*E(network)$width , main = &#39;Bigram Count Network&#39;, sub = glue(&#39;Weight Threshold: {threshold}&#39;), alpha = 50 ) We can go a step further and make our visualization more dynamic using the networkD3 library. threshold &lt;- 50 network &lt;- bi.gram.count %&gt;% filter(weight &gt; threshold) %&gt;% graph_from_data_frame(directed = FALSE) # Store the degree. V(network)$degree &lt;- strength(graph = network) # Compute the weight shares. E(network)$width &lt;- E(network)$weight/max(E(network)$weight) # Create networkD3 object. network.D3 &lt;- igraph_to_networkD3(g = network) # Define node size. network.D3$nodes %&lt;&gt;% mutate(Degree = (1E-2)*V(network)$degree) # Define color group network.D3$nodes %&lt;&gt;% mutate(Group = 1) # Define edges width. network.D3$links$Width &lt;- 10*E(network)$width forceNetwork( Links = network.D3$links, Nodes = network.D3$nodes, Source = &#39;source&#39;, Target = &#39;target&#39;, NodeID = &#39;name&#39;, Group = &#39;Group&#39;, opacity = 0.9, Value = &#39;Width&#39;, Nodesize = &#39;Degree&#39;, # We input a JavaScript function. linkWidth = JS(&quot;function(d) { return Math.sqrt(d.value); }&quot;), fontSize = 12, zoom = TRUE, opacityNoHover = 1 ) In this blog, we explored how to extract data and insights from Twitter. We presented how to clean text data and perform sentiment analysis. Secondly, we saw how pairwise word counts give information about the relations of the input text. Lastly, we studied how to use networks to represent bigram count measures. 28.12 References: Bing Liu, Minqing Hu and Junsheng Cheng. “Opinion Observer: Analyzing and Comparing Opinions on the Web.” Proceedings of the 14th International World Wide Web conference (WWW-2005), May 10-14, 2005, Chiba, Japan. "],["introduction-to-pyecharts.html", "Chapter 29 Introduction to pyecharts", " Chapter 29 Introduction to pyecharts Shihang Wang and Jingyuan Wang Pyecharts is a powerful data visualization tool which combines ‘python’ and ‘echarts’.This project includes a pdf version and jupyter notebook to explain detailed usage rules for pyecharts. The links are below: https://github.com/MichelleeeWang/STAT5702/blob/main/5702-community%20contribution.ipynb https://github.com/MichelleeeWang/STAT5702/blob/main/Community%20Contribution.pdf "],["up-and-running-quickly-with-leaflet.html", "Chapter 30 Up and Running (Quickly) with Leaflet 30.1 What is leaflet? 30.2 Why should we use leaflet on R? 30.3 Why another how-to post on Leaflet? 30.4 First: Creating a base map 30.5 Second: Loading feature data 30.6 Last: Saving the map", " Chapter 30 Up and Running (Quickly) with Leaflet Kenny Lee Last updated: 2021-03-20 30.1 What is leaflet? According to Wikipedia: Leaflet is an open source JavaScript library used to build web mapping applications. First released in 2011, it supports most mobile and desktop platforms, supporting HTML5 and CSS3. Among its users are FourSquare, Pinterest and Flickr. Leaflet allows developers without a GIS background to very easily display tiled web maps hosted on a public server, with optional tiled overlays. It can load feature data from GeoJSON files, style it and create interactive layers, such as markers with popups when clicked. It is developed by Vladimir Agafonkin, who joined Mapbox in 2013. 30.2 Why should we use leaflet on R? The leaflet package is regularly maintained and is an easy way to create leaflet maps in R. See Leaflet for R. Mapping tools for R had been around prior to the introduction of this package. For example, ggplot2 supports mapping with the help of other packages such as maptools. However, there are clear advantages over these tools that leaflet can offer. See below. 30.2.1 Reason why you should use leaflet It is light and easy to use. It supports interactivity on the web, without worrying about JavaScript. It can be easily embedded into R Markdown documents and Shiny apps. There are many visually appealing templates created by third parties. That said, leaflet is not perfect. One drawback is a lack of total freedom. Consider D3 for total control of form and interactivity. 30.3 Why another how-to post on Leaflet? The leaflet package is relatively well documented, with numerous pages of technical details on everything one can do with it, but all this information can feel daunting., especially for those who lack experience in working with geospatial data. In this note, I focus on helping others produce their first map quickly without perusing all the pages in the documentation. Follow the steps below to get up and running with leaflet. 30.4 First: Creating a base map leaflet uses tiles to create maps. One just needs a few lines of code to get started. The function leaflet() returns a leaflet map widget. After that, addTiles() loads a base map. Here’s our first map with Leaflet. library(leaflet) mymap &lt;- leaflet() %&gt;% addTiles() # Default tiles are from OpenStreetMap mymap Let us zoom into New York City. setView() will set the focal point for a map. lng and lat indicate longitude and lattitude of the center of the map. zoom defines the zoom level. mymap %&gt;% addTiles() %&gt;% setView(lng = -73.9, lat = 40.7, zoom = 10) It is easy add tiles from third parties. The following shows a satellite image of the US on in Feb 15 2021, in the midst of severe winter storms that impacted most of the country. The image is provide by NASA’s Global Imagery Browse Services (GIBS). leaflet() %&gt;% setView(-93.65, 42.0285, zoom = 3) %&gt;% addProviderTiles(&quot;NASAGIBS.ModisTerraTrueColorCR&quot;, options = providerTileOptions(time = &quot;2021-02-15&quot;)) Currently, third-party tiles cannot be supported on bookdown. This issue is discussed here: Satellite imagery from GIBS 30.5 Second: Loading feature data 30.5.1 What does geospatial data look like? Geospatial data are often represented in vectors (points, lines and polygons). A shapefile that contains this information usually holds a single type of these vector data. Most common formats for geospatial data are GeoJSON, TopoJSON and .shp shapefiles. leaflet makes it easy to take spatial features to maps. The data often come in the form of SpatialLines, SpatialLinesDataFrame, Lines, and Line objects (from the sp package), SpatialPolygons, SpatialPolygonsDataFrame, Polygons, and Polygon objects (from the sp package), and many others. Here’s a real example using Lines. First, these packages are required. library(rgdal) # for readOGR and others library(sp) # for spatial objects library(leaflet) # for interactive maps library(tidyverse) # for working with data frames and ggplot2 library(jsonlite) # for downloading and interacting with Geo JSON file later library(RCurl) # same as above library(httr) # to deal with https problems when getting data from Github library(zip) # to unpack downloaded compressed data In this example, I plot all roads in Manhattan from the US Census Bureau’s TIGER database. Read on below to find out more about the TIGER database. addPolylines() function from leaflet can interpret SpatialLinesDataFrame and plot lines on a map. # Load the shape file base_url &lt;- &quot;https://www2.census.gov/geo/tiger/TIGER2020/ROADS/tl_2020_36061_roads.zip&quot; download.file(URLencode(base_url), &quot;tl_2020_36061_roads.zip&quot;, quiet=TRUE) unzip(&quot;tl_2020_36061_roads.zip&quot;) # &quot;dsn&quot; is the path of the data and the &quot;layer&quot; is the name of the shapefile without any extension road_shape_ny &lt;- readOGR(dsn = &quot;.&quot;, layer = &quot;tl_2020_36061_roads&quot;, verbose = FALSE) leaflet() %&gt;% addTiles() %&gt;% setView(lng = -73.98928, lat = 40.75042, zoom = 12) %&gt;% addPolylines(data = road_shape_ny, weight = 1) Next, I present an example on plotting SpatialPolygonsDataFrame. A polygon is made of 3 or more points connected by vertices. Polygons are often used to delineate an area such as lakes, states or nations. A great source of geographical data is the US Census Bureau. It provides high-quality data on economic and demographic information across all regions in the nation. This information is based on American Community Survey (ACS), which is a nationwide survey designed to provide communities with reliable and timely social, economic, housing, and demographic data every year. To support ACS and of course the Decennial Census, the Census Bureau developed the Topologically Integrated Geographic Encoding and Referencing (TIGER) database. The TIGER files provide geospatial and map data down to geographical regions as small as census tracts (subdivisions of counties with hundreds to thousands of residents). Below, I use the function addPolygons() to plot Polygons objects that represent census tracts in New York City. Instead of using TIGER files directly, I am showing a modified version by the city authorities as the boundaries of some tracts go over the body of water surrounding the city. The GeoJSON file can be found here. readOGR() function from the rgdal package is used to read the spatial data. # Geo JSON base_url &lt;- &quot;https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Census_Tracts_for_2010_US_Census/FeatureServer/0/query?where=1=1&amp;outFields=*&amp;outSR=4326&amp;f=pgeojson&quot; geo_data &lt;- getURL(URLencode(base_url)) NYCtract &lt;- readOGR(geo_data, &#39;OGRGeoJSON&#39;, verbose = F) NYCtract %&gt;% leaflet() %&gt;% addTiles() %&gt;% setView(lng = -73.98928, lat = 40.75042, zoom = 10) %&gt;% addPolygons(weight = 1) In the example above, default inputs for parameters were enough to display the census tracts on the map. Only weight parameter was given to set the width of the boundaries. Next, we can make this map more meaningful by combining it with a new dimension. The COVID-19 pandemic has shed light on growing economic and health disparity in our society. The latest ACS data can show economic disparity within our own city. From the latest 2019 vintage of ACS 5-year estimates, I pulled data on the share of population below the poverty line as defined by the federal government across census tracts. The subject table can be found here. The codes below pull in those data and create a sub-sample for New York City. Afterwards, we combine the ACS data with the SpatialPolygonsDataFrame we created above. # 2019 ACS 5-year data by census tract (subject table S1701) base_url &lt;- &quot;https://raw.githubusercontent.com/kennylee15/leaflet_for_r_101/main/s1701.csv&quot; data &lt;- read.csv(base_url, stringsAsFactors = FALSE) # Subset tracts belonging to NYC data &lt;- data[grep(&quot;US36005|US36047|US36061|US36081|US36085&quot;, data$geoid),] # Sub-sample the tracts with population above 150 # Remove the tract for Rendall&#39;s Island (the data seem unrealistic) data &lt;- mutate(data, id = str_sub(geoid,start=10,end=20)) data &lt;- data %&gt;% filter(pop&gt;=150 &amp; id!=&quot;36061024000&quot;) # Create a unique key for census tracts by combining with county FIPS codes. # Census tract ids are only unique within a specific county. NYCtract@data$CountyCode &lt;- recode(NYCtract@data$BoroCode, &quot;1&quot; = &quot;36061&quot;, &quot;2&quot; = &quot;36005&quot;, &quot;3&quot;=&quot;36047&quot;, &quot;4&quot;=&quot;36081&quot;, &quot;5&quot;=&quot;36085&quot;) NYCtract@data &lt;- within(NYCtract@data, CT2010Unique &lt;- paste(CountyCode, CT2010, sep=&quot;&quot;)) mapdata &lt;- NYCtract mapdata@data &lt;- left_join(NYCtract@data, data, by=c(&quot;CT2010Unique&quot;=&quot;id&quot;)) Finally, we create a leaflet object and plot the data. The SpatialPolygonsDataFrame, which was combined with the ACS data is loaded with the addPolygons() function from leaflet. This function will automatically interpret polygons and add a new layer to the map. # This is for a data pop-up that will appear upon click popup &lt;- paste0(&quot;Area Name (NTA): &quot;, mapdata$NTAName, &quot;&lt;br&gt;&quot;, mapdata$name, &quot;&lt;br&gt;&quot;, &quot;Percent of Population under Poverty: &quot;, mapdata$pvty) # Function for color palette pal &lt;- colorNumeric( palette = &quot;YlGnBu&quot;, domain = mapdata$pvty ) map_leaflet &lt;- leaflet() %&gt;% addTiles() %&gt;% setView(lng = -73.9, lat = 40.7, zoom = 10) %&gt;% addPolygons(data = mapdata, fillColor = ~pal(pvty), color = &quot;#444444&quot;, # use hex colors fillOpacity = 0.6, weight = 0.5, smoothFactor = 0.2, popup = popup, highlight = highlightOptions(weight = 4, color = &quot;blue&quot;, bringToFront = TRUE)) %&gt;% addLegend(pal = pal, values = mapdata$pvty, position = &quot;bottomright&quot;, title = &quot;Percent of Population&lt;br&gt;under Poverty&quot;, labFormat = labelFormat(suffix = &quot;%&quot;)) map_leaflet Among parameters for aesthetics, the ones used most often are: weight: the thickness of the boundary lines in pixels, fillColor: the color of the polygons, highlight: options to highlight a polygon on hover. 30.6 Last: Saving the map The whole point of making an interactive map is to share it on a webpage. A leaflet map can be saved in an html form using the htmlwidgets package. The saveWidget() function can produce a file that we can store on a server. library(htmlwidgets) saveWidget(map_leaflet, file=&quot;map_leaflet.html&quot;, selfcontained=FALSE) "],["introduction-to-geographic-chart-in-r.html", "Chapter 31 Introduction to geographic chart in R", " Chapter 31 Introduction to geographic chart in R Zihao Liu Our data in the real world often contains geographic location information, so it is inevitable to draw maps involving geographic coordinate systems. The geographic coordinate system uses a three-dimensional sphere to define the position of the earth’s surface, so as to realize a coordinate system that references points on the earth’s surface through latitude and longitude. In this part, I will introduce some methods to draw geographic chart by using spatial data in R. Spatial data refers to data defined in a three-dimensional space with geographic location information. Map projection is a particularly important key technology when getting data. The most basic step of map information visualization is map projection, which converts the geographic coordinate information on the non-expandable surface to a two-dimensional plane, which is equivalent to surface parameterization. Since some data is in .shp or .shx which are data of projection of maps which can not be achieved by urls, I just wrote that in a pdf and uploaded it to my github alone with all the raw data for everyone to download. The link of tutorial: https://github.com/Lzhntjs/edav_community_contribution/blob/master/Introduction%20to%20geographic%20chart%20in%20R.pdf "],["interview-questions-for-data-visualization.html", "Chapter 32 Interview questions for data visualization", " Chapter 32 Interview questions for data visualization Ivy Liu library(gridExtra) library(ggplot2) 32.0.1 General Questions 32.0.1.1 Question1: What is data modeling? Data modeling is the analysis of data objects that are used in a business or other context and the identification of the relationships among these data objects. Data modeling is the first step in performing object-oriented programming. 32.0.1.2 Question2: What are the steps involved in a data analysis process} Data exploration Data preparation Data modeling Validation Implementation of model and tracking 32.0.1.3 Question3: Why is data cleansing important for data visualization? Data cleansing is used for identifying and removing errors and inconsistencies from data in order to enhance the quality of data. This process is crucial and emphasized because wrong data can lead to poor analysis. This step ensures the quality of the data is met to prepare data for visualization. 32.0.1.4 Question4: Explain what should be done with suspected or missing data? Prepare a validation report that gives information of all suspected data. It should give information like validation criteria that it failed and the date and time of occurrence. Experience personnel should examine the suspicious data to determine their acceptability. Invalid data should be assigned and replaced with a validation code. To work on missing data use the best analysis strategy like deletion method, single imputation methods, mean/median/mode imputation, model based methods, etc. 32.0.1.5 Question5: What are some important features of a good data visualization? The data visualisation should be light and must highlight essential aspects of the data; looking at important variables, what is relatively important, what are the trends and changes. Besides, data visualisation must be visually appealing but should not have unnecessary information in it. One can answer this question in multiple ways: from technical points to mentioning key aspects, but be sure to remember saying these points: Data positioning Bars over circle and squares Use of colour theory Reducing chart junk by avoiding 3D charts and eliminating the use of pie charts to show proportions why sunburst visualization is more effective for hierarchical plots 32.0.1.6 Question6: What is a scatter plot? For what type of data is scatter plot usually used for? A scatter plot is a chart used to plot a correlation between two or more variables at the same time. It’s usually used for numeric data. 32.0.1.7 Question7: What features might be visible in scatterplots? Correlation: the two variables might have a relationship, for example, one might depend on another. But this is not the same as causation. Associations: the variables may be associated with one another. Outliers: there could be cases where the data in two dimensions does not follow the general pattern. Clusters: sometimes there could be groups of data that form a cluster on the plot. Gaps: some combinations of values might not exist in a particular case. Barriers: boundaries. Conditional relationships: some relationship between the variables rely on a condition to be met. 32.0.1.8 Question8: What type of plot would you use if you need to demonstrate “relationship” between variables/parameters? When we are trying to show the relationship between 2 variables, scatter plots or charts are used. When we are trying to show “relationship” between three variables, bubble charts are used. 32.0.1.9 Question9: When will you use a histogram and when will you use a bar chart? Explain with an example.} Both plots are used to plot the distribution of a variable. Histograms are usually used for a categorical variable, while bar charts are used for a categorical variable. 32.0.1.10 Question10: What is an outlier? The outlier is a commonly used terms by analysts referred for a value that appears far away and diverges from an overall pattern in a sample. There are two types of outliers: univariate and multivariate. 32.0.1.11 Question11: What type of data is box-plots usually used for? Why? Boxplots are usually used for continuous variables. The plot is generally not informative when used for discrete data. 32.0.1.12 Question12: What information could you gain from a box-plot? Minimum/maximum score Lower/upper quartile Median The Interquartile Range Skewness Dispersion Outliers 32.0.1.13 Question13: When do you use a boxplot and in what situation would you choose boxplot over histograms.} Boxplots are used when trying to show a statistical distribution of one variable or compare the distributions of several variables. It is a visual representation of the statistical five number summary. Histograms are better at determining the probability distribution of the data; however, boxxplots are better for comparison between datasets and they are more space efficient. 32.0.1.14 Question14: When analyzing a histogram, what are some of the features to look for? Asymmetry Outliers Multimodality Gaps Heaping/Rounding: Heaping example: temperature data can consist of common values due to conversion from Fahrenheit to Celsius. Rounding example: weight data that are all multiples of 5. Impossibilities/Errors 32.0.1.15 Question15: What type of data is histograms usually used for? Continuous data 32.0.1.16 Question16: What is the difference between count histogram, relative frequency histogram, cumulative frequency histogram and density histogram? Count: the vertical (y) axis shows the frequency/count of data that falls within each bin Relative frequency: the vertical (y) axis shows the relative frequency of data that falls within each bin. Relative frequency can be calculated by dividing the frequency by the total frequency (total count). Hence, the height of the bars sum up to 1. Cumulative frequency: it shows the accumulation of the frequencies. Each vertical (y) axis shows the frequency of data in the corresponding bin and all the previous ones. Density: the vertical (y) axis is calculated by dividing the relative frequency by the bin width. Hence, the area of the bars sum up to 1. 32.0.1.17 Question17: What is nominal data and ordinal data? Explain with examples.} Nominal data is data with no fixed categorical order. For example, the continents of the world (Europe, Asia, North America, Africa, South America, Antarctica, Oceania). Ordinal data is data with fixed categorical order. For example, customer satisfactory rate (Very dissatisfied, dissatisfied, neutral, satisfied, very satisfied). 32.0.1.18 Question18: What are some advantages of using cleveland dot plot versus bar chart? Cleveland plot takes up less space for data with many categories. Within a given space, one can fit more dots than bars. Cleveland plot can show two different sets of values on the same line. 32.0.1.19 Question19: How do you determine the color palette in your plots? Depends on whether the data is discrete or continuous, I would choose different color scheme. For example, if the data is nominal, I would chose qualitative palette where there is no progression, but if the data is continuous, I would choose a sequential or perceptually uniform color palette. To avoid using colors that causes confusion or unintended emphasis on part of the data, I usually use the existing palettes in the software packages. For example in R, there is RColorBrewer. When I am presenting my data visualizations, I would try to make my graphs color vision decency (CVD) friendly. 32.0.2 R Questions 32.0.2.1 Question20: List 3 libraries in R that can be used for data visualization.} ggplot2, Lattice, Leaflet, Highcharter, RColorBrewer, Plotly, sunburstR, RGL, dygraphs 32.0.2.2 Question21: How do you make multiple plots to a single page layout in R? Use the function \\(par(mfrow=c(n,m))\\). For example, \\(par(mfrow=c(2,2))\\) can be used to capture a 2 X 2 plot in a single page. par(mfrow=c(2,2)) plot(wt,mpg, main=&quot;Scatterplot of wt vs. mpg&quot;) plot(wt,disp, main=&quot;Scatterplot of wt vs disp&quot;) hist(wt, main=&quot;Histogram of wt&quot;) boxplot(wt, main=&quot;Boxplot of wt&quot;) 32.0.2.3 Question22: What is the lattice package in R used for? Lattice is mainly used for multivariate data and relationships. It supports trellis graphs, which displays a variable or the relationship between variables, conditioned on one or more other variables. 32.0.2.4 Question23: Can pots be exported as image files or other file formats in R? Yes, plots could be saved as images directly from R using an editor such as RStudio. This way of saving, however, does not provide much flexibility. If we want to customize our images, we need to have an approach as to how to export plots from the R code itself. We can use “ggsave” function to accomplish this. We can save the plots in different formats such as jpeg, tiff, pdf, svg etc. We can also use various parameters to change the size of the image prior to exporting it or saving it in a path or location. Saving as jpeg format \\[ggsave(filename = “PlotName1.jpeg”, plot=Image\\_plot )\\] Saving as tiff format \\[ggsave(filename = “PlotName1.tiff”, plot=Image\\_plot )\\] Saving as pdf format \\[ggsave(filename = “PlotName1.pdf”, plot=Image\\_plot )\\] Saving as tiff format with change in size \\[ggsave(filename = “PlotName1.tiff”, plot=Image\\_plot , width=14, height=10, units=”cm”)\\] 32.0.2.5 Question24: What are the key components or grammar for the visualization in the ggplot2 library in R? Every visualization in ggplot2 package in R comprises of the following key aspects: Data – The raw material of your visualization Layers – What you can see or visualize on plots (i.e. lines, points, maps etc.) Scales – Maps the data to graphical output Coordinates – This is from the visualization perspective (i.e. grids, tables etc.) Faceting – Provides “visual drill-down” into the data Themes – Controls the details of the display (i.e. fonts, size, colour etc.) 32.0.2.6 Question25: Why is it important to tidy data? Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:\\ Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Tidy data makes it easy for an analyst or a computer to extract needed variables because it provides a standard way of structuring a dataset. Compare the different versions of the classroom data: in the messy version you need to use different strategies to extract different variables. This slows analysis and invites errors. (source: https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) 32.0.2.7 Question26: How do you import data in R} One can import data from a text file, csv, excel, SPSS, SAS, etc. in R. R base functions that can be used include: \\(read.table()\\), \\(read.delim()\\), \\(read.csv()\\), \\(readcsv2()\\). We could also use the \\(readr\\) package to fast read data. Reading from excel requires either \\(readxl\\) or \\(xlsx\\) package, and for SPSS and SAS, one could use the \\(Hmisc\\) package. 32.0.2.8 Question27: How missing values and impossible values are represented in R ? NaN (Not a Number) is used to represent impossible values whereas NA (Not Available) is used to represent missing values. Usually, simply deleting missing values is not a good idea because the probable cause for missing value could be some problem with data collection or programming or the query. It is good to find the root cause of the missing values and then take necessary steps handle them. 32.0.2.9 Question28: What are the building blocks for graphing with ggplot2? Layers: a plot of the dataset Scale: normal, logarithmic, etc. Coord: coordinate system Facet: multiple plots Theme: the looks of the overal graph 32.0.2.10 Question29: Two students plotted histograms for the exact same data in R with the same bin width and boundary values, however their plots have completely different shape. What could be the cause of this? This is probably due to the fact that the one plot is right closed, the other is right open, so that the data points that fall on the boundaries are put into different bins. This kind of difference can be eliminated by choosing boundary values that cannot occur in the dataset; for example, we can use higher precision decimal values. Example data: 780, 1100, 940, 900, 1170, 900, 950, 905, 1340, 1122, 900, 970, 1009, 1157, 1151, 1009, 1217, 1080, 896, 958, 1153, 900, 860, 1070, 800, 1070, 909, 1100, 940, 1110, 940, 1122, 1100, 1300, 1070, 890, 1106, 704, 500, 500, 620, 1500, 1100, 833, 1300, 1011, 1100, 1140, 610, 990, 1058, 700, 1069, 1170, 700, 900, 700, 1150, 1500, 950 32.0.2.11 Question30: In R, we can use the \\(read\\_html\\) function to easily scrape data from websites. What are some of the caveats of web scraping (ethically and legally)? Think and investigate legality of scraping the data Think about whether the use of the data is ethical Limit bandwidth use Scrape only what you need 32.0.2.12 Question32: What are factors in R and why is it useful? Factors are the data objects which are used to categorize the data and store it as levels. They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like “Male,”Female\" and True, False etc. They are useful in data analysis for statistical modeling. (source: https://www.tutorialspoint.com/r/r\\_factors.html) 32.0.2.13 Question32: What is Rmarkdown? What is the use of it? RMarkdown is a tool provided by R to create dynamic documents and reports that contain shiny widgets and outputs from R. An R Markdown document is written in markdown (an easy-to-write plain text format) and contains chunks of embedded R code. 32.0.2.14 Question33: What are some common functions in the “dplyr” package? filter, select, mutate, arrange and count. 32.0.3 Python Questions 32.0.3.1 Question34: What is pandas? Pandas is a Python open-source library that provides high-performance, powerful and flexible data structures and data analysis tools. It is built on top of Python programming language. 32.0.3.2 Question35: What libraries do data scientists use to plot data in Python? Matplotlib is the main library used for plotting data in Python. However, the plots created with this library need lots of fine-tuning to look shiny and professional. For that reason, many data scientists prefer Seaborn, which allows you to create appealing and meaningful plots with only one line of code. 32.0.3.3 Question36: Name a few libraries in Python used for Data Analysis and Scientific computations.? NumPy: It is used for scientific computing and performing basic and advanced array operations. It offers many handy features performing operations on n-arrays and matrices in Python. It helps to process arrays that store values of the same data type and makes performing math operations on arrays (and their vectorization) easier. SciPy: This useful library includes modules for linear algebra, integration, optimization, and statistics. Its main functionality was built upon NumPy, so its arrays make use of this library. Pandas: This is a library created to help developers work with “labeled” and “relational” data intuitively. It’s based on two main data structures: “Series” (one-dimensional, like a list of items) and “Data Frames” (two-dimensional, like a table with multiple columns). SciKit: Scikits is a group of packages in the SciPy Stack that were created for specific functionalities – for example, image processing. Scikit-learn uses the math operations of SciPy to expose a concise interface to the most common machine learning algorithms. Matplotlib: This is a standard data science library that helps to generate data visualizations such as two-dimensional diagrams and graphs (histograms, scatterplots, non-Cartesian coordinates graphs). Seaborn: Seaborn is based on Matplotlib and serves as a useful Python machine learning tool for visualizing statistical models – heatmaps and other types of visualizations that summarize data and depict the overall distributions. Plotly: This web-based tool for data visualization that offers many useful out-of-box graphics – you can find them on the Plot.ly website. The library works very well in interactive web applications. 32.0.3.4 Question37: Which library would you prefer for plotting in Python language: Seaborn or Matplotlib? Matplotlib is the python library used for plotting but it needs lot of fine-tuning to ensure that the plots look shiny. Seaborn helps data scientists create statistically and aesthetically appealing meaningful plots. The answer to this question varies based on the requirements for plotting data. 32.0.4 Tableau Questions 32.0.4.1 Question38: What does Tableau do? Tableau is a visual analytics engine that makes it easier to create interactive visual analytics in the form of dashboards. It empowers people and organizations to make the most of their data and solve problems. 32.0.4.2 Question39: In Tableau, how is the Context Filter different from other Filters? When we create a Context Filter, Tableau will create a temporary table for this particular Filter set and other Filters will be applied on the Context Filter data like cascade parameters. Suppose, we have created a Context Filter on countries, USA and India, Tableau will create a temporary table for these two countries’ data and if we have any other Filters other will be applied on these two countries’ data if we don’t have any Context Filter, each record will check for all Filters. (source: https://datavizguru.com/interview-questions/) 32.0.4.3 Question40: In Tableau, what is the disadvantage of Context Filters? The Context Filter is not frequently changed by the user—if the Filter is changed, the database must be recomputed and the temporary table has to be rewritten, slowing performance. When we set a dimension to context, Tableau creates a temporary table that will require a reload each time the view is initiated. For Excel, Access, and text data sources, the temporary table created is in an Access table format. For SQL Server, MySQL, and Oracle data sources, we must have permission to create a temporary table on our server. For a multidimensional data source, or cubes, temporary tables are not created, and Context Filters defined which Filters are independent and which are dependent. (source: https://datavizguru.com/interview-questions/) 32.0.4.4 Question41: What are Filters? How many types of Filters are there in Tableau? The filters can be applied in a worksheet to restrict the number of records present in a dataset, especially for unnecessary data; and it shows the exact data we want. There are various types of filters available in Tableau. Extract Filters: Extract filters are used to apply filter on extracted data from the data source. For this filter, data is extracted from the data source and placed into the Tableau data repository. Datasource Filters: Using the data source filters in tableau, we can directly apply our filter conditions on the data source. It filters the data at the data source and then uploads only the remaining data in the Tableau worksheet. But, the only difference is it works with both live and extract connection. Context Filters: By using context filters in tableau, we can apply a general context to our overall analysis of Tableau. Dimension Filters: The dimension filters are the filters that we apply on individual dimensions. Dimensions are not aggregated and so, we can select distinct values from the list of field values. Measure Filters: The measure filters are the filters that we apply using the measure field values. Measure values are always aggregated like sum, average, median, etc. (source: https://data-flair.training/blogs/tableau-filters/) 32.0.4.5 Question42: What is the difference between heat map and treemap in Tableau? A heat map is a great way to compare categories using color and size. In this, we can compare two different measures. A treemap is used for illustrating hierarchical (tree-structured) data and for visualizing a part of or a whole relationship. 32.0.4.6 Question43: What are the main features of Tableau? Tableau Dashboard: Tableau Dashboards provide a wholesome view of your data by the means of visualizations, visual objects, text, etc. Dashboards are very informative as they can present data in the form of stories, enable the addition of multiple views and objects, provide a variety of layouts and formats, enable the users to deploy suitable filters. Collaboration and Sharing: Tableau provides convenient options to collaborate with other users and instantly share data in the form of visualizations, sheets, dashboards, etc. in real-time. It allows you to securely share data from various data sources such as on-premise, on-cloud, hybrid, etc. Instant and easy collaboration and data sharing help in getting quick reviews or feedback on the data leading to a better overall analysis of it. Live and In-memory Data: Tableau ensures connectivity to both live data sources or data extraction from external data sources as in-memory data. This gives the user the flexibility to use data from more than one type of data source without any restrictions. You can use data directly from the data source by establishing live data connections or keep that data in-memory by extracting data from a data source as per their requirement. Data Sources in Tableau: Tableau offers a myriad of data source options you can connect to and fetch data from. Data sources ranging from on-premise files, spreadsheets, relational databases, non-relational databases, data warehouses, big data, to on-cloud data are all available on Tableau. Advanced Visualizations (Chart Types): In Tableau, you can make visualizations as basic as a bar chart or pie chart. You can also make more advanced visualization as histogram, gantt chart, bullet chart, motion chart, treemap and boxplot. Maps: Tableau has a lot of pre-installed information on maps such as cities, postal codes, administrative boundaries, etc. This makes the maps created on Tableau very detailed and informative. Trend Lines and Predictive Analysis: Easy creation of trend lines and forecasting is possible due to Tableau’s powerful backend and dynamic front end. You can easily get data predictions such as a forecast or a trend line by simply selecting some options and drag-and-drop operations using your concerned fields. (source : https://data-flair.training/blogs/tableau-features/) 32.0.4.7 Question44: What are Histograms in Tableau? What is its use in analysis? Histograms show or graphically represent the distribution of values in intervals or bins of equal sizes. These charts are specifically used to represent the distribution of continuous data. Histogram in Tableau takes the continuous measure values and places them into bins of equal sizes. 32.0.4.8 Question45: Explain Bar charts in Tableau. What are the different kinds of Bar Charts? Bar charts represent data in categories by using rectangular bars, whose height is proportional to the value that bar is representing. In Tableau, we can make different kinds of bar charts such as; Segmented bar chart, Stacked bar chart, Side-by-side bar chart etc. Horizontal/ Vertical bar chart: A horizontal or vertical bar graph is a simple graph having bars of vertical or horizontal orientation. Segmented bar chart: A segmented bar chart is a bar chart where a bar chart contains more than one set of bars. Each set of bars belongs to a particular segment. For instance, we can have a sales bar graph for three or four different segments all seen in a single view. Stacked bar chart: A stacked bar chart has a single bar divided into smaller parts. For instance, a single bar for the year 2019 can show sales for different countries or regions or cities. We can also set a color scheme for the subdivisions in a bar as we can see in a stacked bar chart below. Side-by-side bar chart: A side-by-side bar chart will have multiple bars standing next to each other for a single segment. "],["introduction-to-supervised-machine-learning.html", "Chapter 33 Introduction to Supervised Machine Learning", " Chapter 33 Introduction to Supervised Machine Learning Tomislav Galjanic In this project a video module on Introduction to Supervised Machine Learning (ML) was created for the Northeast Big Data Hub based on a previously prepared PowerPoint presentation deck and a companion Jupyter Notebook. There are five videos in the module covering introductory topics on supervised ML, decision trees, and Random Forrest. The examples discussed in the first four videos are also explored in more detail programmatically in the last video by going through the Python code in the companion Jupyter Notebook. The link to the Google Drive folder with the video lectures is: https://drive.google.com/drive/folders/1RyR4Qj0waQHysydW3dXnSeM6i9aq7bXw?usp=sharing "],["data-acquisition-and-wrangling.html", "Chapter 34 Data acquisition and wrangling", " Chapter 34 Data acquisition and wrangling Renyin Zhang I volunteered to create a video to present one of the few introductory data science based modules that Northeast Big Data Hub provided. Using the PowerPoint file as a material, I prepared the lecture and recorded a 30-minute lecture on how to acquire and clean the data. The whole lecture is split into several small videos, each lasting less than 15 minutes, to avoid overwhelming viewers with too much information to intake, and to have breaks in between. Here are the links to my videos: part 1 part 2 part 3 Reminder: Log in with your LionMail to have access for the videos. "],["complete-eda-process-and-visualizations-in-python.html", "Chapter 35 Complete EDA Process and Visualizations in Python", " Chapter 35 Complete EDA Process and Visualizations in Python Shaurya Malik, Rahul Agarwal, Keertan Krishnan Identifying Python as another widely used language for data science jobs in the industry, we have created a one-stop-shop for visualizations and EDA techniques in Python. Taking EDAV.info as motivation, we have added self-written and consolidated code snippets with appropriate references. Please find the link to the website below, along with its code in the GitHub repository. Final Website URL: https://rtg8055.github.io/DataScience/ To explore our project, check out our GitHub repository below: https://github.com/RTG8055/DataScience https://github.com/RTG8055/DataScience/tree/gh-pages "],["data-transformation-with-dplyr-video-tutorial.html", "Chapter 36 Data Transformation with Dplyr Video Tutorial", " Chapter 36 Data Transformation with Dplyr Video Tutorial Brian Hernandez Part of the R Tidyverse, Dplyr is one of the most powerful tools out there for data transformation. Learn all about it with this video tutorial. The video is intended as a quick introduction for newcomers and a quick reference for those who are already experienced. Enjoy! "],["data-science-ethics.html", "Chapter 37 Data science ethics", " Chapter 37 Data science ethics Abhishek Sinha The Northeast Big Data Innovation Hub (http://nebigdatahub.org/) is a collaboration hub for data science innovation in the Northeast United States. One of its initiatives, the Northeast Student Data Corps (https://nebigdatahub.org/nsdc/) aims at teaching data science fundamentals to students across the region, with a special focus on underserved institutions. As a part of my community contribution, I aided the Northeast Big Data Innovation Hub by recording a few videos teaching data science concepts for this purpose. My videos are on the topic of Data Science ethics. Researching the slides, and then shooting the videos (2 videos, took me a few takes for each) took about 3-4 hours. I also aided by gathering more volunteers from the class and coordinating between them and the Hub. This included logistical aid and vetting the videos other volunteers submitted as well. (I got 4 more volunteers from our class and vetted 6 videos). I also collaborated with a few students of our class who organised/hosted a session on Data for Social Good on March 9th, by presenting this volunteering opportunity to the other attendees and talking about previous work with data for social good. The link to my recordings for Data Science Ethics: https://drive.google.com/drive/folders/16JyGYLriAGCWG6iNIZGmLTe7EIB3-CZN?usp=sharing The videos will be edited by the Hub and put up on their website as well as their youtube channel. The timeline for this has not been finalized but I would be happy to share that as well as and when it comes up. "],["d-plot-in-r.html", "Chapter 38 3D plot in R 38.1 Introduction 38.2 Static 3D Plots 38.3 Interactive 3D Plots 38.4 Conclusion 38.5 Works Cited", " Chapter 38 3D plot in R Xinzhe Qi If using rgl library for interative plots, we need to set up options to display the graph inside html. library(plot3D) library(plotly) 38.1 Introduction Usually, we tend to use 2-D graphs for data visualization. However, sometimes we do want to show the spatial relationship in 3-D. As a result, it would be helpful for some to use 3-D visuals in R. This post will explore these methods in R. 38.2 Static 3D Plots 38.2.1 3D scatterplot x = sep.l &lt;- iris$Sepal.Length y = pet.l &lt;- iris$Petal.Length z = sep.w &lt;- iris$Sepal.Width scatter3D(x = sep.l, y = pet.l, z = sep.w, # define x, y, z axes clab = c(&quot;Sepal&quot;, &quot;Width (cm)&quot;) # clab is used to change the title of the color legend. ) 38.2.1.1 Change background “f”: full box “b”: default value. Only the back panels are visible “b2”: back panels and grid lines are visible “g”: grey background with white grid lines “bl”: black background “bl2”: black background with grey lines “u”: means that the user will specify the arguments col.axis, col.panel, lwd.panel, col.grid, lwd.grid manually “n”: no box will be drawn. This is the same as setting box = FALSE # full box scatter3D(x, y, z, bty = &quot;f&quot;, colkey = FALSE, main =&quot;bty= &#39;f&#39;&quot;) # back panels and grid lines scatter3D(x, y, z, bty = &quot;g&quot;, colkey = FALSE, main =&quot;bty= &#39;g&#39;&quot; ) 38.2.1.2 Change viewing direction theta is the azimuthal direction and phi is the co-latitude. scatter3D(x, y, z, theta = 15, phi = 20) scatter3D(x, y, z, phi = 0, bty =&quot;g&quot;) 38.2.2 3D texts We can put texts into a 3D space using text3D. This text in the graph shows the state name, with dimensions of Rape, Murder, and Assault. The color of each name indicates the population of each state. data(USArrests) with(USArrests, text3D(Murder, Assault, Rape, labels = rownames(USArrests), colvar = UrbanPop, # add columns col = gg.col(100), theta = 60, phi = 20, xlab = &quot;Murder&quot;, ylab = &quot;Assault&quot;, zlab = &quot;Rape&quot;, main = &quot;USA arrests&quot;, cex = 0.6, bty = &quot;g&quot;, # background ticktype = &quot;detailed&quot;, # axis tick type d = 2, clab = c(&quot;Urban&quot;,&quot;Pop&quot;), adj = 0.5, font = 2)) We can even add points and columns into the text graph. “The adj parameter determines the position of the text relative to the specified coordinate. Use adj = c(0, 0) to place the left bottom corner at (x, y, z), adj = c(0.5, 0.5) to center the text there, and adj = c(1, 1) to put the right top corner there. The optional second coordinate for vertical adjustment defaults to 0.5. Placement is done using the”advance\" of the string and the “ascent” of the font relative to the baseline, when these metrics are known.\" # Plot texts with(USArrests, text3D(Murder, Assault, Rape, labels = rownames(USArrests), colvar = UrbanPop, col = gg.col(100), theta = 60, phi = 20, xlab = &quot;Murder&quot;, ylab = &quot;Assault&quot;, zlab = &quot;Rape&quot;, main = &quot;USA arrests&quot;, cex = 0.6, bty = &quot;g&quot;, ticktype = &quot;detailed&quot;, d = 2, clab = c(&quot;Urban&quot;,&quot;Pop&quot;), adj = 0.5, font = 2)) # Add points with(USArrests, scatter3D(Murder, Assault, Rape - 1, colvar = UrbanPop, col = gg.col(100), type = &quot;h&quot;, pch = &quot;.&quot;, add = TRUE)) 38.2.3 3D Histogram Here we can plot a 3-D histogram. This example visualizes the matrix, where the heights represent the number of deaths. data(VADeaths) hist3D(z = VADeaths, scale = FALSE, expand = 0.01, bty = &quot;g&quot;, phi = 20, col = &quot;#0072B2&quot;, border = &quot;black&quot;, shade = 0.2, ltheta = 90, space = 0.3, ticktype = &quot;detailed&quot;, d = 2) 38.3 Interactive 3D Plots 38.3.1 rgl library data &lt;- iris # Add a new column with color mycolors &lt;- c(&#39;royalblue1&#39;, &#39;darkcyan&#39;, &#39;oldlace&#39;) data$color &lt;- mycolors[ as.numeric(data$Species) ] plot3d( x=data$`Sepal.Length`, y=data$`Sepal.Width`, z=data$`Petal.Length`, col = data$color, type = &#39;s&#39;, radius = .1, xlab=&quot;Sepal Length&quot;, ylab=&quot;Sepal Width&quot;, zlab=&quot;Petal Length&quot;) rglwidget() 38.3.2 plotly library Other than rgl, we can also draw an interative scatter plot, using plotly. 38.3.2.1 3D Scatter Plot mtcars$am[which(mtcars$am == 0)] &lt;- &#39;Automatic&#39; mtcars$am[which(mtcars$am == 1)] &lt;- &#39;Manual&#39; mtcars$am &lt;- as.factor(mtcars$am) fig &lt;- plot_ly(mtcars, x = ~wt, y = ~hp, z = ~qsec, color = ~am, # define color on param colors = c(&#39;#BF382A&#39;, &#39;#0C4B8E&#39;)) fig &lt;- fig %&gt;% add_markers() fig &lt;- fig %&gt;% layout(scene = list(xaxis = list(title = &#39;Weight&#39;), yaxis = list(title = &#39;Gross horsepower&#39;), zaxis = list(title = &#39;1/4 mile time&#39;))) fig 38.4 Conclusion If we only need to plot a static 3D graph, library plot3D is pretty easy to use. For interative plots, rgl and plotly are great choices. There are also line plots and surface plots in plotly library, but since they are not very frequently-used, they’re not included in this tutorial. 38.5 Works Cited http://www.sthda.com/english/wiki/impressive-package-for-3d-and-4d-graph-r-software-and-data-visualization https://plotly.com/r/3d-scatter-plots/ https://www.r-graph-gallery.com/3d_scatter_plot.html "],["ben-shneidermans-visualization-mantra.html", "Chapter 39 Ben Shneiderman’s Visualization Mantra 39.1 Introduction: 39.2 Need for Visualization: 39.3 Ben Shneiderman’s Visual Information Seeking Mantra 39.4 References:", " Chapter 39 Ben Shneiderman’s Visualization Mantra Mohammed Aqid Khatkhatay and Sanket Gokhale 39.1 Introduction: Data Visualization today has emerged as a very important aspect in any career. It has not only made comprehending data easy for the human brain but also gaining insights [1]. Visualization has become easier with the improvements in hardware and software over the years. Colors have been useful in deciding the information to be conveyed [2]. Good communication is the key to good understanding. Similarly good visualization is the key to good insights. Every data visualization has 3 C’s namely Content, Context and Construction. Content basically refers to the data to be plotted. Context refers to the specifications about data, usually stored in a data dictionary. Construction is the phase where the visuals are built from the content and context. 39.2 Need for Visualization: Firstly the need for any kind of data or information visualization is to gain insights into the data and not just pictures or graphs. Moreover visualization allows us to detect faulty or missing data or inconsistencies within the data. Anomalies can also be found and corrected. Visualization is multidisciplinary, that is any person can use it to get a deeper understanding of data. It can help higher level professionals convert raw insights into business problems. Colors add different perceptions and can help think differently. Visualization should penetrate the visual domain of an individual. Too much visualization can cause problems and staggering of data. It can also hide important insights. For this purpose we need fixed guidelines on how to apply visualizations and what to show and what to hide. 39.3 Ben Shneiderman’s Visual Information Seeking Mantra In his paper titled “The eyes have it: a task by data type taxonomy for information visualizations” [3], Ben Schneiderman suggested the following 7 principles for visualization of data. 39.3.1 Step 1-&gt; Overview: The first step of the Holy Trinity of Visual Information Seeking is ‘Overview’. In this step, it is recommended to visualize the given data set in a simple way without going into too much detail. We do not want to lose any information in this step, thus it is recommended to not apply any filters and just visualize the given data to get an idea about it. For explaining the different steps of Visual Information Seeking, we take the example of Covid-19 statistics from 1 January to 15 January of USA. The data set has 3 columns: (i) date (ii) daily_new_cases (iii) daily_deaths As the first step, we will visualize the Covid-19 data set using simple bar graph. The x-axis represents the dates and the bar height shows the daily_new_cases for each date. library(ggplot2) library(reshape2) library(scales) daily_new_cases = c(225974, 239642, 201613, 199506, 234864, 264914, 280785, 308301, 258373, 221676, 220023, 230597, 239313, 235643, 249623) daily_deaths = c(2304, 2109, 1427, 2003, 3806, 4115, 4160, 4066, 3278, 1883, 2078, 4513, 4097, 4161, 3821) dat &lt;- data.frame(date=c(&quot;Jan 01&quot;,&quot;Jan 02&quot;,&quot;Jan 03&quot;,&quot;Jan 04&quot;,&quot;Jan 05&quot;,&quot;Jan 06&quot;,&quot;Jan 07&quot;,&quot;Jan 08&quot;,&quot;Jan 09&quot;,&quot;Jan 10&quot;,&quot;Jan 11&quot;,&quot;Jan 12&quot;,&quot;Jan 13&quot;,&quot;Jan 14&quot;,&quot;Jan 15&quot;),daily_new_cases=c(daily_new_cases), daily_deaths=c(daily_deaths)) ggp &lt;- ggplot(dat, aes(x=date, y=daily_new_cases)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 60)) + scale_y_continuous(labels = comma) + ggtitle(&quot;Daily New Cases&quot;) + theme(plot.title = element_text(hjust = 0.5)) ggp 39.3.2 Step 2(and 3) -&gt; Zoom and Filter: After understanding the basic distribution of the data set in the first step, the next steps that follow are ‘Zoom’ and ‘Filter’. Here, we proceed to focus on the part of the data set that is of value to us. Zooming and Filtering help highlight the objects of interest using various techniques. Continuing on our example, there are two ways we can zoom in on the data: Suppose we want to find the days where the daily_new_cases is between 220,000 and 310,000. Here we will perform a zoom on the Y-axis so that we can only see bar graphs for days that have daily_new_cases between the 220,000 and 310,000. We can also perform zoom on specific days. In this example, we want to find out the daily_new_cases from Jan 1 to Jan 5. ggp1 &lt;- ggplot(dat, aes(x=date, y=daily_new_cases, fill = daily_deaths)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 15)) ggp1 + coord_cartesian(ylim = c(220000, 310000)) + ggtitle(&quot;Daily New Cases&quot;) + theme(plot.title = element_text(hjust = 0.5)) ggp2 &lt;- ggplot(dat, aes(x=date, y=daily_new_cases, fill = daily_deaths)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 0)) ggp2 + xlim(&quot;Jan 01&quot;, &quot;Jan 03&quot;, &quot;Jan 04&quot;, &quot;Jan 05&quot;) + scale_y_continuous(labels = comma)+ ggtitle(&quot;Daily New Cases and Daily Deaths Overview&quot;) + theme(plot.title = element_text(hjust = 0.5)) In this data set, we can filter to show only the dates of interest. Suppose we want to find the daily_new_cases for dates which have observed more than 4000 deaths. Here we can use the filter() function in R as follows: library(ggplot2) library(plotly) p &lt;-filter(dat, dat$daily_deaths&gt;=4000) ggp2 &lt;- ggplot(p, aes(x=date, y=daily_new_cases, fill = daily_deaths)) + geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 60)) + scale_y_continuous(labels = comma) ggp2+ ggtitle(&quot;Daily New Cases filtered by Daily Deaths&quot;) + theme(plot.title = element_text(hjust = 0.5)) 39.3.3 Step 4 -&gt; Details On Demand We identify areas of interest in the first step(Overview), and dig deeper into the data using zooming and filtering in the second step, the next step naturally should be to find exact details which will help us find interesting facts from the data set. The final step of the Holy Trinity is details on demand. Here we use the full power of interactivity that are provided by the libraries that we use. For example, for the Covid-19 dataset, we can use the library ‘plotly’ to make the graph we produced above, more interactive. When we hover our cursor above each bar in this graph, we get the details for that particular day. ggplotly(ggp2+ ggtitle(&quot;Daily New Cases filtered by Daily Deaths with details on Hover&quot;)+ theme(plot.title = element_text(hjust = 0.5))) In 1996, Shneiderman offered a taxonomy for visual information seeking. The taxonomy divides general visual information seeking into seven data types and seven tasks. This taxonomy is one of the earliest and most influential contributions to the information visualization field. Seven Data Types: • one-dimensional data: This refers to the linear type of data. For example texts and numbers. • two-dimensional data: This refers to the multi-layer type of data. For example maps and contour plots. • three-dimensional data: This refers to the object type of data. For example buildings, street-view or mechanical instruments. • temporal data: This refers to the time based type of data. For example time charts and time series. • multidimensional data: This refers to the statistical and relational data containing n-attributes in n-dimensions. For example multidimensional histogram or multidimensional scatters. • tree data: This refers to data having parent-child type structure. For example Decision tree and Family tree. • network data: This refers to data in which attributes have relationships among themselves. For example social networks. The additional tasks other than the Holy Trinity include the following: • relate • history • extract 39.3.4 Step 5 -&gt; Relate Relate allows users to view relationships between data points. This mantra allows users to further explore networks or maps that help in determining the further course of actions. We can demonstrate this using a simple blood transfusion dataset which tells us the blood transfusion capability between different blood groups. We first plot the network diagram. Each arrow head in this indicates a valid blood transfusion relation. library(GGally) library(network) library(ggnetwork) library(geomnet) data(blood, package = &quot;geomnet&quot;) # plot with ggnet2 (Figure 2a) ggplot(data = blood$edges, aes(from_id = from, to_id = to)) + geom_net(colour = &quot;darkred&quot;, layout.alg = &quot;circle&quot;, labelon = TRUE, size = 12, directed = TRUE, vjust = 0.6, labelcolour = &quot;grey80&quot;, arrowsize = 1, linewidth = 0.5, arrowgap = 0.02, selfloops = TRUE, ecolour = &quot;Blue&quot;) +theme_net() + ggtitle(&quot;Relationship between Blood Types&quot;) + theme(plot.title = element_text(hjust = 0.5)) 39.3.5 Step 6 -&gt; History This mantra enables the developer to create a history of user actions in order to let the user undo or redo any action taken while visualizing the data. This works on the assumption that the user is bound to make mistake or the desired goal of an activity is not obtained in the first attempt. Hence it is always desirable to give the user flexibility to undo or redo the necessary steps. Since it is difficult to demonstrate this in the form of an RMD we shall skip this mantra. 39.3.6 Step 7 -&gt; Extract This mantra allows users to visualize a part of the graph in order to focus only on the data that is necessary for immediate use. In the below example we plot a graph showing effectiveness vs dose of phase 1 and phase 2 trials of a very small sample set. On hovering over the bar graphs we can hide/un-hide the necessary/unnecessary data. This is a good way to extract insights without getting hindered from data not required. library(highcharter) df &lt;- data.frame(dose=c(0.5, 1, 2), effectiveness=c(4.2, 10, 29.5)) df2 &lt;- data.frame(supp=rep(c(&quot;Phase 1&quot;, &quot;Phase 2&quot;), each=3), dose=rep(c(0.5, 1, 2),2), effectiveness=c(6.8, 15, 33, 4.2, 10, 29.5)) hc &lt;- df2 %&gt;% hchart(&#39;column&#39;, hcaes(x = &#39;dose&#39;, y = &#39;effectiveness&#39;, group = &#39;supp&#39;))%&gt;% hc_title( text = &quot;&lt;i&gt;Effectiveness vs dose of phase 1 and phase 2 trials&lt;/i&gt;&quot;,margin = 20,align = &quot;left&quot;,style = list(useHTML = TRUE)) hc 39.4 References: [1] https://medium.com/multiple-views-visualization-research-explained/the-purpose-of-visualization-is-insight-not-pictures-an-interview-with-visualization-pioneer-ben-beb15b2d8e9b [2] Lyn Bartram, Abhisekh Patra, and Maureen Stone. 2017. Affective Color in Visualization. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). Association for Computing Machinery, New York, NY, USA, 1364–1374. DOI:https://doi.org/10.1145/3025453.3026041 [3] B. Shneiderman, “The eyes have it: a task by data type taxonomy for information visualizations,” Proceedings 1996 IEEE Symposium on Visual Languages, Boulder, CO, USA, 1996, pp. 336-343, doi: 10.1109/VL.1996.545307. "],["choose-your-adventure-how-to-choose-the-right-visualization-for-your-data.html", "Chapter 40 Choose Your Adventure - How to choose the right visualization for your data 40.1 Introduction 40.2 The Flow Chart 40.3 An Example Use Case 40.4 Conclusion", " Chapter 40 Choose Your Adventure - How to choose the right visualization for your data Gee Hyun Kwon (gk2575) &amp; Angela Simei Li (sl4525) library(readxl) library(ggplot2) library(lubridate) library(tidyverse) 40.1 Introduction Graphs and charts help us understand data better, but it isn’t always clear exactly how to choose the right visualization. We have developed a 3-step process “choose your adventure” to help you navigate through your next data visualization problem at hand. 40.1.1 Step 1. What would you like to show? We have developed a flowchart to help you decide the type of graph you might need. We believe that there are four types of goals in visualizing a dataset. Comparison : Comparing the sizes against several datasets How many apples versus pears were sold by the same farmer? Composition : Understanding the makeup of a dataset How many college freshmen, sophomores, juniors, and seniors make up an undergraduate college? Relationship : Discovering the relationship between variables Is there a correlation between the city’s temperature and ice cream sales? Distribution : Checking how a group falls into different categories How many students in an exam made &lt;60, 60-70, 70-80, 80-90, 90-100? Based on the types, we recommend the following flowchart to pick the right visualization. 40.1.2 Step 2 It is often not enough to simply pick only one type of graph for complex questions. Here are some additional aspects to consider once you’ve picked the direction of the visualization through the flowchart. Is my data categorical or quantitative? This often plays a role in the ordering of the axis. Categorical data can often be placed alphabetically, whereas numerical data can often be displayed increasingly. Is faceting a good strategy for the graphs? Faceting means placing same x-y axis graphs repeatedly in a table format, but each graph has a different subject. An example use of faceting is to show the growth height of all 6 vegetables in a vegetable garden through the same time period, and the 6 graphs of x-axis being time and y-axis being height are being placed in a 2x3 table format. 40.1.3 Step 3 Is overlaying two different types of graphs a good idea? Overlaying two different graphs could bring clarity to the visualization. However, unnecessarily overlaying could lead to confusion and clutter. A good example of overlaying two graphs is drawing a regression line through a chaotic scatter plot to highlight the trend of the dots. Some common details to make-or-break a graph include: Title Legend of the graph Color of the graph, including using alpha blending to adjust transparency Displaying numbers over the graph or displaying numbers only on the axes Using normal scale or log scale for the axes 40.2 The Flow Chart 40.3 An Example Use Case We will show an example use case of our flow chart using COVID statistics data. # Data is from here: # https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide url &lt;- &quot;https://opendata.ecdc.europa.eu/covid19/casedistribution/csv/data.csv&quot; covid_data &lt;- as.data.frame(read_csv(url)) covid_data$dateRep &lt;- parse_date(covid_data$dateRep, &quot;%d/%m/%Y&quot;) df &lt;- covid_data glimpse(df) ## Rows: 61,900 ## Columns: 12 ## $ dateRep &lt;date&gt; 2020-12-… ## $ day &lt;dbl&gt; 14, 13, 1… ## $ month &lt;dbl&gt; 12, 12, 1… ## $ year &lt;dbl&gt; 2020, 202… ## $ cases &lt;dbl&gt; 746, 298,… ## $ deaths &lt;dbl&gt; 6, 9, 11,… ## $ countriesAndTerritories &lt;chr&gt; &quot;Afghanis… ## $ geoId &lt;chr&gt; &quot;AF&quot;, &quot;AF… ## $ countryterritoryCode &lt;chr&gt; &quot;AFG&quot;, &quot;A… ## $ popData2019 &lt;dbl&gt; 38041757,… ## $ continentExp &lt;chr&gt; &quot;Asia&quot;, &quot;… ## $ `Cumulative_number_for_14_days_of_COVID-19_cases_per_100000` &lt;dbl&gt; 9.013779,… For this example, let’s pretend that the world is made up of only the following 5 European countries. We will be looking at data from the week 2020-04-06 ~ 2020-04-12. names(df)[names(df) == &#39;countriesAndTerritories&#39;] &lt;- &#39;country&#39; countries &lt;- c(&quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Sweden&quot;, &quot;Romania&quot;) df = df[df$country %in% countries, ] df = df[(df$dateRep&gt;= &quot;2020-04-06&quot; &amp; df$dateRep&lt;= &quot;2020-04-12&quot;),] df_dcc &lt;- df %&gt;% select(dateRep, cases, country) Let’s say that we are interested in comparing each countries in their total number of COVID infections. Our path through the flowchart would be the following: Comparison -&gt; Time Variant? YES -&gt; Line Chart g &lt;- ggplot(data = df_dcc, mapping = aes(x = dateRep, y = cases, color = country)) + geom_line() + ggtitle(&quot;Line graph of cases per country&quot;) + labs (x = &quot;&quot;, y = &quot;# of cases&quot;) + theme_grey(16) + theme(legend.title = element_blank()) g This graph answers the following questions (and many more, of similar format): On April 2020, which country had the most number of COVID infections? Which one had the least? Which country in general seems to be leading in the number of COVID infections? Let’s say instead that you want to see the total number of infections, and what percentage each country contributes to that amount. Our path through the flowchart would be the following: Composition -&gt; Time variant? YES -&gt; Stacked Bar Chart ggplot(df_dcc, aes(fill=country, y=cases, x=dateRep)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) According to the flowchart, we should use a stacked bar chart that shows the number of infected through each segment of the bar. This graph would answer the following questions: How many people in total were infected in a particular date ex) 2020-04-11? Which country contributed the most to the number of people infected? When did the total number of people infected worldwide reach its maximum during the given time period? 40.4 Conclusion Our example here shows that even when using the same data, we can create multiple visualizations that answer different questions. Our flow chart makes this question-asking process more streamlined and helps create meaningful visualization. After all, if a visualization does not answer a question, what purpose does it serve? "],["common-errors-in-r.html", "Chapter 41 Common Errors in R 41.1 could not find function 41.2 Error in if 41.3 Object not found 41.4 unexpected … 41.5 condition has length &gt; 1 41.6 $ operator is invalid for atomic vectors 41.7 argument is of length zero 41.8 unimplemented type ‘list’ 41.9 undefined columns selected 41.10 replacement has length zero 41.11 replacement has 10 rows, data has 5 41.12 ‘by’ must specify a uniquely valid column 41.13 incorrect number of subscripts on matrix 41.14 subscript out of bounds 41.15 cannot open the connection 41.16 non-numeric argument to binary operator 41.17 need finite ‘xlim’ values 41.18 unused argument 41.19 Tips for Debugging 41.20 References", " Chapter 41 Common Errors in R Emily Jennings-Dobbs 41.1 could not find function ymd(10-7-19) ## Error in ymd(10 - 7 - 19): could not find function &quot;ymd&quot; This error happens most often when the correct packages aren’t loaded. The above error would be fixed by including “library(lubridate)” in the line before the code. If this happens with a function you’ve defined yourself, make sure the function is defined either in a chunk before it is called or in the same chunk but before the code. 41.2 Error in if c &lt;- NA if(c &gt; 3){ c =c+1 } ## Error in if (c &gt; 3) {: missing value where TRUE/FALSE needed This error will appear whenever you try to send a NULL or NA variable into an if statement. Check any previous manipulations of the variable to see if something went wrong. 41.3 Object not found temp ## Error in eval(expr, envir, enclos): object &#39;temp&#39; not found The variable you have used isn’t defined in the code. Check for spelling or capitalization errors and make sure you’ve defined the variable before the call and not later in the code. 41.4 unexpected … c &lt;- 1 if(c&gt;1)) ## Error: &lt;text&gt;:2:8: unexpected &#39;)&#39; ## 1: c &lt;- 1 ## 2: if(c&gt;1)) ## ^ This one is pretty simple, somewhere in your code is an extra parenthesis. x 2 ## Error: &lt;text&gt;:1:3: unexpected numeric constant ## 1: x 2 ## ^ Usually when this error appears you’ve forgotten an operator (such as + or =) or you’ve called a function that requires multiple variables and haven’t put a comma between them. 41.5 condition has length &gt; 1 f&lt;-function(t) { if (t&lt;0){ f&lt;-0 return(f) }else{ f&lt;-(2*t)/((1+t^2)^2) return(f) } } f(c(12,13)) ## Warning in if (t &lt; 0) {: the condition has length &gt; 1 and only the first element ## will be used ## [1] 0.001141498 0.000899654 This happens when you pass a variable with multiple elements like a list or vector to a function that expects an input of only one integer or object. Try using apply() if you need to evaluate the function for multiple values or recheck the declaration of your variables if there is opnly supposed to be one element. 41.6 $ operator is invalid for atomic vectors testlist &lt;- c(1,2,3) testlist$s ## Error in testlist$s: $ operator is invalid for atomic vectors This happens with lists, the objects in the list are unnamed and therefore it’s impossible to access them with $. Instead use brackets to find the correct subset. testlist[1] ## [1] 1 41.7 argument is of length zero df = read.table(text = &#39; A B C D 1 2 34 4 11 2 4 34 13 22&#39;, header=TRUE) for (i in 1:nrow(df)) if(df[i, &quot;A&quot;] == df[i, &quot;E&quot;]) c=0 ## Error in if (df[i, &quot;A&quot;] == df[i, &quot;E&quot;]) c = 0: argument is of length zero This happens when you call a column name that doesn’t exist. This could be fixed by changing “E” in the if staement to a valid column name like “D.” 41.8 unimplemented type ‘list’ test &lt;- list(1,2,3) order(test) ## Error in order(test): unimplemented type &#39;list&#39; in &#39;orderVector1&#39; This error happens when you send a list to function that requires a vector. It can be fixed by using as.vector() in some cases, but may require you to redeclare the object as a vector using vector(). 41.9 undefined columns selected df[,5] ## Error in `[.data.frame`(df, , 5): undefined columns selected This one is almost the same error as our example in “argument is of length zero”, we are trying to access a column that doesn’t exist. To check what your columns are called check names(), and to check the dimensions use dim(). 41.10 replacement has length zero x = rep(1, 10) for (i in 1:10){ x[i] = x[i]+x[i-1] } ## Error in x[i] &lt;- x[i] + x[i - 1]: replacement has length zero This happens when you try to access the zeroth component (x[0]) of a vector which has a value of numeric(0). Check how you are indexing your variables. 41.11 replacement has 10 rows, data has 5 foo &lt;- data.frame(n = c(1:5), d = letters[1:5]) foo$test &lt;- c(1:10) ## Error in `$&lt;-.data.frame`(`*tmp*`, test, value = 1:10): replacement has 10 rows, data has 5 a &lt;- c(30:50) foo &lt;- c(1:30) foo[1:10] &lt;- a This error happens when you attempt to store an amount of objects in a space that is either too big or too small to hold them. Check dimensions using dim() and reevaluate what you are trying to do. 41.12 ‘by’ must specify a uniquely valid column df1 = read.table(text = &#39; A1 B1 C1 D 1 2 34 4 11 2 4 34 13 22&#39;, header=TRUE) merge(x = df, y = df1, by = &quot;C&quot;) ## Error in fix.by(by.y, y): &#39;by&#39; must specify a uniquely valid column This happens when you try to merge two dataframes by a column that isn’t in both dataframes. This would only work if we chose to use by=“D” because it is the only column that appears in both dataframes. 41.13 incorrect number of subscripts on matrix x = rep(1, 10) for(i in 1:10){x[i,]=5} ## Error in x[i, ] &lt;- 5: incorrect number of subscripts on matrix This happens when you try to index an object that isn’t a matrix as though it was. In other words x is not declared as a matrix and by trying to index it using x[i,] you get an error. If this happens tryusing as.matrix(), or just index x using x[i] instead of x[i,]. 41.14 subscript out of bounds a &lt;- 3 foo &lt;- matrix(NA, nrow = 2, ncol = 2) foo[a,1] &lt;- &#39;oops&#39; ## Error in `[&lt;-`(`*tmp*`, a, 1, value = &quot;oops&quot;): subscript out of bounds This happens when you try to assign an object to an index that doesn’t exist. In other words, since there is no third row we can’t try to store anything in it. 41.15 cannot open the connection library(readr) write_csv(as.data.frame(foo), path = &#39;the/long/road/to/nowhere.csv&#39;) ## Error in open.connection(file, &quot;wb&quot;): cannot open the connection This one is pretty simple but can be quite annoying if you don’t know what it means. Your path just doesn’t exist, check for spelling or punctuation errors or look for another path. 41.16 non-numeric argument to binary operator a &lt;- &#39;7&#39; 6 + a ## Error in 6 + a: non-numeric argument to binary operator Also pretty simple, the function used expected a numeric argument to be thrown to it like 7 instead of ‘7’. The easiest fix is to use as.numeric(), but depending on the situation you may need to re-declare the variable. 41.17 need finite ‘xlim’ values x &lt;- rep(NA, 5) y &lt;- c(4, 1, 6, 5, 3) plot(x, y) ## Error in plot.window(...): need finite &#39;xlim&#39; values This happens when you plot NA or NULL values, check your variables and determine what to do with NA/NULL values. 41.18 unused argument my_fun1 &lt;- function(x) { x^2 } my_fun1(x = 1, y = 2) ## Error in my_fun1(x = 1, y = 2): unused argument (y = 2) This happens when you send a variable to a function that is unneeded and unusable. Above code can be fixed by removing “y=2”. 41.19 Tips for Debugging Check for matching brackets and parenthesis Check for discrepancies in spelling and capitalization remove variables with rm() when you’re done with them If you can’t find where your error takes place use traceback() Print out your variables to make sure they are of the right type and have the correct dimensions (you can check these using typeof() and dim()) The internet is your friend, Google your problem and more often than not you’ll find someone stuck with the exact same error. Don’t forget to check Stack Overflow. Check if you’ve loaded all of the correct packages Clean up your environment using the little broomstick button in the environment window on the upper right of RStudio Try updating your packages Restart R Unistall and then Reinstall R and/or RStudio If all else fails, take a break for a few minutes and come back with fresh eyes. Staring at the same code for long enough will always tire you out and make it harder to find small mistakes. 41.20 References Clayvelle, T. (2017). Common errors and debugging R code. Retrieved March 01, 2021, from https://www.tylerclavelle.com/code/2018/debug/ Handling errors &amp; warnings in r: List of typical messages &amp; how to solve. (2021, February 26). Retrieved March 04, 2021, from https://statisticsglobe.com/errors-warnings-r Huynh, Y. (2019, August 06). R for graduate students. Retrieved March 04, 2021, from https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html Programming, R. (2016, June 06). Common r programming errors faced by beginners. Retrieved March 02, 2021, from https://www.r-bloggers.com/2016/06/common-r-programming-errors-faced-by-beginners/ Silaparasetty, V. (2020, March 05). Common errors in r and debugging techniques. Retrieved March 04, 2021, from https://medium.com/analytics-vidhya/common-errors-in-r-and-debugging-techniques-f11af3f1c7d3 "],["collaborating-on-rstudio-with-github.html", "Chapter 42 Collaborating on RStudio with GitHub 42.1 Objective 42.2 What is Git? 42.3 Collaborating on GitHub 42.4 Getting started with Git on RStudio 42.5 Merging Conflicts", " Chapter 42 Collaborating on RStudio with GitHub Jee sun Yun and Yena Lee 42.1 Objective Collaborating on RStudio is not feasible, especially when working on a group project on R. Using GitHub is not instantaneous like Google docs where team members can collaborate and get real-time updates. However, it can be a powerful tool once you have it set up. Just by the fact that you can avoid the nuisance of mailing each other files and copy and pasting your teammates’ code makes this attractive. This ducumentation will mainly focus on collaborating on GitHub, assuming that Git and GitHub is set up. 42.2 What is Git? Git is a software for version control. You can track changes, keep the history, or set markers to pull up certain versions of the file. You can use Terminal to type in Git commands, but the RStudio interface makes it easier to commit, push, pull and track changes. 42.3 Collaborating on GitHub 42.3.1 Create a repository On your GitHub account, create a new repository by clicking on + New Repository Add a simple name for the repository (i.e. 21_edav), description of the repo Create a README file if necessary Choose to add a .gitignore to add documents to identify files that you want GitHub to ignore. (I would recommend to have this, the MacOS automatically creates a .DS_Store file which stores config informations but this is rarely relevant) Stage .DS_Store, and on commit, click on ignore on top of the screen and this will add the file to .gitignore By signing in with you Columbia mail address, you can choose your repository to be private. 42.3.2 Branching Branching is useful, because you can safely make changes to your work without affecting the main branch. Think of it as a parallel universe. You can merge different branches to make changes in the main branch later on. You can create a branch by typing in a new name. 42.3.3 Add Collaborators Add collaborators to your repository, allowing them access to push to the repository. Go to Settings &gt; Manage Access &gt; Invite a collaborator. Your teammate should accept the invite in his/her mail. 42.3.4 Tracking changes on GitHub clicking on ###commits allows you to see commit history click on commits to see what was changed code in red: deleted code in green: added 42.3.5 Blame When going to a file, you can see Blame on the right. This shows you who is the author line by line. 42.3.6 Issues Issues can be found next to &lt;&gt;Code at the top, right below your repository name. Issues can be used to track todos, bugs, and other comments to leave in the repository for others to see. You can tag your collaborator for them to be alerted with @. If you have trouble figuring something out, or want somethnig to be resolved, others can see and help you resolve your problem. 42.4 Getting started with Git on RStudio 42.4.1 Cloning a repository to a new project Step 0: Create a github folder to organize and keep all GitHub repositories. Mac users: Users [username] Windows Users: Users [username] Step 1: Copy GitHub URL Click Code &gt; Copy httpsformat to clipboard Or you can install GitHub Desktop and clone your repository there Step 2: Create a new project on RStudio Create New project &gt; Version Control &gt; Git &gt; paste repository URL The repository will be saved on your local machine Open the new project on a “new session”: this will allow you to keep different projects separated and be organized; have different projects open at the same time Step 3: Push the .Rproj to GitHub Pull first, and check everything is up to date Stage: select files you want to commit Commit: commit the files to GitHub mostly, for first commit write “initial commit” for commit description or anything else adequate for further commit, write description of what changes you made if there is no change in description, you can amend previous comment check on GitHub page by refreshing to see if .Rproj was added 42.4.2 Cloning a repository set up by someone else If your teammate or collaborator already set up a github repo and commited an .Rproj and other .R files, cloning the repository will bring all the files your partner pushed to the repository. 42.4.3 Overview of Git panel Stage: stage files to commit Status A: Added D: Deleted M: Modified R: Renamed ?: Untracked Diff: show file difference Commit: commit staged files Pull/ Push to remote repository History: view history of file 42.5 Merging Conflicts Just like mentioned in the beginning, while this is a effective and efficient collaborative way to work, it does not have the real-time reponsiveness of Google docs. You can see who is logged on, who is working on what and what they are typing real-time on Google, but not here. When collaborators are working on the same file but on different lines, it is not a problem. Just remember to pull each time before pushing in case your collaborator pushed to GitHub while you were working. However, when you and your partner are working on the same line, you won’t know until you pull. (Important to pull before you push!) Even after you commit your changes, Git Pull will give you a error message, and you should decide which code to keep and what to do with it. You’ll see somthing like this: &gt;&gt;&gt; /usr/local/git/bin/git pull CONFLICT (content): Merge conflict in [filename] Automatic merge failed; fix conflicts and the commit the result. On the Git panel, you will see the status as U: unresolved conflict. To resolve the conflict, the person who first pushed should decide which to keep and delete. Then you can describe in the commit description for future reference. Pull and commit often, so that you can avoid merging conflicts. "],["math-finance-in-r.html", "Chapter 43 Math Finance in R 43.1 Introduction 43.2 Data Preparation 43.3 Empirical coveriance matrix 43.4 Find efficient portfolios using mean/variance analysis 43.5 Select a set of ticker 43.6 Visualization of Minimum Variance Portforlio", " Chapter 43 Math Finance in R Zhengyi(Skye) Chen and Shiyue Liu library(reshape2) library(ggplot2) library(dplyr) library(tidyquant) library(plotly) library(timetk) library(tidyr) 43.1 Introduction Mathematical finance is a branch of applied mathematics where people aim at using mathematics to model the behavior of financial markets. Due to the heavy overlap between mathematical finance and computational finance(or to say, financial engineering), financial mathematicians also focus on applications and modeling using stochastic asset models. Financial market produces huge amount of time series data every day, which becomes the base of analysis for many mathematical finance problems. In this session, we will first show how to use API to download data from Alpha Vantage. Then we use these stock price historical time series data to estimate the expected returns and covariance matrix. After that, we will use these to find an efficient asset allocation using mean/variance analysis. 43.2 Data Preparation The time series data comes from Alpha Vantage. To download the data, first we need to register using email address and get an API key. The API key is a string with numbers and letters. After getting the API key, save it. APIKey = &quot;API_KEY&quot; # Your API key URLbase = &#39;https://www.alphavantage.co/query?&#39; func = &#39;TIME_SERIES_DAILY&#39; Then we need to declare a list of tickers, which includes the company that we are interested in. The ticker is a abbreviation of the company name with a publicly traded stock. tickerList = c( &quot;T&quot;, # ATT (phone company) &quot;F&quot;, # Ford (car company) &quot;IBM&quot;, # IBM (computers, mostly software) &quot;MSFT&quot;, # Microsoft (software) &quot;BA&quot;, # Boeing (planes, civilian and military) &quot;GM&quot;, # General Motors (cars) &quot;AAPL&quot;, # Apple (computers, phones, software) &quot;GE&quot;, # General Electric (jet engines and other things) &quot;AMD&quot;, # AMD (chips) &quot;JPM&quot;, # J P Morgan Chase (bank) &quot;NVDA&quot;, # Nvidia (chips) &quot;SPY&quot;) # S &amp; P ETF The following ‘R’ code is to download the time series data of the company in the ‘tickerList’ and save it in separate ‘.csv’ files. It downloads time series daily closing price from \\(T\\) days before up to now. (citation) tickerCount = 0 # how many tickers you&#39;ve received so far. SeventySeconds = 70 # how long to wait every five requests for ( ticker in tickerList){ tickerCount = tickerCount + 1 # Get only five per minute if ( ( tickerCount &gt; 1 ) &amp; ( (tickerCount %% 5) == 1 ) ){ cat(&quot;sleeping\\n&quot;) flush.console() # get the message without waiting Sys.sleep(SeventySeconds) # Wait (sleep) a minute } URL = paste( c( URLbase, # create the request ... &quot;function=&quot;, func, # ... see documentation for the format. &quot;&amp;symbol=&quot;, ticker, &quot;&amp;apikey=&quot;, APIKey, &quot;&amp;outputsize=full&quot;, # get the whole series, not just 100 days &quot;&amp;datatype=csv&quot;), collapse = &quot;&quot;) cat(&quot;Fetching with url &quot;) cat(URL) cat(&quot;\\n&quot;) timeSeriesCSVfile = read.csv(URL) # send the request cat(&quot;Got ticker &quot;) cat(ticker) cat(&quot;\\n&quot;) csvFileName = paste( c(&quot;StockPriceTimeSeries_&quot;, ticker, &quot;.csv&quot;), collapse = &quot;&quot;) write.csv( timeSeriesCSVfile, file = csvFileName) } Note that ‘Alpha Vantage’ has a restriction of fetching five tickers every minute, hence the code sleeps for 70 seconds between five downloading activities. Hence, we suggest to run this part separately and save the downloaded file to save time. 43.3 Empirical coveriance matrix After getting the data, we want to read the data from the csv files and use them to compute the empicical covariance matrix for the same period of time. First, we introduce the mathematical formulation: Denote \\(R_{t,k}\\) as the actual daily return for asset \\(k\\) at time \\(t\\), \\(\\mu_k\\) as the empirical mean daily return for asset \\(k\\), \\(C\\) is \\(n*n\\) symmetric matrix as the empirical covariance matrix, \\(\\sigma_{j,k}\\) as the \\(j,k\\) element of matrix \\(C\\). Now, we read the data from csv files: tickerList = c( &quot;T&quot;, # ATT (phone company) &quot;F&quot;, # Ford (cars) &quot;IBM&quot;, # IBM (computers, mostly software) &quot;MSFT&quot;, # Microsoft (software) &quot;BA&quot;, # Boeing (planes, aerospace) &quot;GM&quot;, # General Motors (cars) &quot;AAPL&quot;, # Apple (computers, phones, software) &quot;GE&quot;, # General Electric (jet engines, other things) &quot;AMD&quot;, # AMD (chips) &quot;JPM&quot;, # J P Morgan Chase (bank) &quot;NVDA&quot;, # Nvidia (chips) &quot;SPY&quot;) # S &amp; P index ETF # Read historical time series from .csv files in this directory T = 250 # number of days of historical data to use n = length(tickerList) # number of assets # prices[t,k] = closing price on day t (counting back from today) of asset k prices = matrix( nrow = T, ncol = n) # allocate memory for the matrix for ( k in 1:n){ # iterate over k, not the ticker name ticker = tickerList[k] # get the corresponding ticker name fileName = paste( c( &quot;https://raw.githubusercontent.com/ConnieeeeeLIU/EDAV/main/StockPriceTimeSeries_&quot;, # Every series starts like this ticker, # &quot;.csv&quot;), # a .csv file collapse = &quot;&quot;) # no blanks between the parts CSVfile = readr::read_csv( fileName ) closingPriceSeries = CSVfile[[&quot;close&quot;]] # The column under &quot;close&quot;... # ... holds the closing prices for ( t in 1:T){ # Copy from the &quot;data frame&quot;... prices[ t, k] = closingPriceSeries[t] # ... to the matrix } } Let’s first take a look at our data, we choose four companies here: ATT (phone company) Ford (cars) IBM (computers, mostly software) Microsoft (software) The plot below is the closing price of these four companies in the past 250 days: colnames(prices) = tickerList day = as.Date(&quot;2021-03-04&quot;) - 0:249 # add the days of our data df_price = data.frame(day, prices) sub_df_price = df_price[c(&#39;day&#39;, &#39;T&#39;, &#39;F&#39;, &#39;IBM&#39;, &#39;MSFT&#39;)] # choose a subsample to show time serise, # if all 12 is shown, it would be messy sub_df_price &lt;- melt(sub_df_price, id=&quot;day&quot;) colnames(sub_df_price) &lt;- c(&quot;day&quot;, &quot;ticker&quot;, &quot;price&quot;) ggplot(data = sub_df_price,aes(x=day, y=price, group=ticker, color=ticker))+ geom_line()+ xlab(&quot;Day&quot;)+ ylab(&quot;Price&quot;)+ ggtitle(&quot;Closing price of four companies&quot;) We can also use histograms to get a sense of distribution of different stocks: df_price[c(&#39;day&#39;, &#39;MSFT&#39;)] %&gt;% ggplot(aes(MSFT)) + geom_histogram(color=&#39;black&#39;, fill=&#39;grey&#39;, bins=20) + ggtitle(&quot;Distribution of ticker of Microsoft&quot;) df_price[c(&#39;day&#39;, &#39;T&#39;)] %&gt;% ggplot(aes(T)) + geom_histogram(color=&#39;black&#39;, fill=&#39;grey&#39;, bins=20) + ggtitle(&quot;Distribution of ticker of ATT&quot;) Wow! we can tell how much more valuable Microsoft is than ATT here! We can also use overlapped density distribution to better compare different companies! sub_df_price%&gt;% ggplot(aes(x=price, fill=ticker)) + geom_density(alpha=0.2) + ggtitle(&quot;Overlapped Density Distribution of Four Stocks&quot;) Now let’s start the math part in R! Calculate the daily returns, the mean return, and the empirical covariance: returns = matrix( nrow = (T-1), ncol = n) for ( t in 1:(T-1)){ for ( k in 1:n){ returns[ t, k] = ( prices[ t+1, k] - prices[ t,k ])/prices[ t, k] } } mu = matrix( 0, ncol = 1, nrow = n) # Column vector of expected returns # initialized to zero so this works: for ( k in 1:n){ # For asset k ... for ( t in 1:(T-1)){ # ... add the returns over time mu[k] = mu[k] + returns[t,k] } mu[k] = mu[k]/(T-1) # Divide by the number of samples ... } # ... to get the empirical mean The empirical mean daily return, \\(\\mu_k\\), is the average daily return over the averaging period, \\(T\\). \\[\\sigma_{j,k} = \\dfrac{1}{T-2} \\sum^{T-1}_{t = 1}(R_{j,t}-\\mu_j)(R_{k,t}-\\mu_k)\\] \\[\\sigma_{j,j} = \\dfrac{1}{T-2} \\sum^{T-1}_{t = 1}(R_{j,t}-\\mu_j)^2\\] Note that, here we need to devide by \\(T-2\\) since we have \\(T\\) data points, which gives \\(T-1\\) daily returns, and this results in \\(T-2\\) degrees of freedom. C = matrix( 0, ncol = n, nrow = n) for (j in 1:n){ for (k in 1:n){ total = 0 for (t in 1:(T-1)){ total = total + (returns[t,j]-mu[j])*(returns[t,k]-mu[k]) } C[j,k] = total/(T-2) } } 43.4 Find efficient portfolios using mean/variance analysis In this part, we want to find the minimum variance portfolio. After that, we also try to find the efficient portfolio that provides the same expected return as an equal weighted portfolio for comparison. one_m = matrix(1, ncol = 1, nrow = n) a = t(one_m)%*%solve(C)%*%one_m b = t(one_m)%*%solve(C)%*%mu c = t(mu)%*%solve(C)%*%mu det_v = a*c-b^2 eq_a = a/det_v eq_b = -2*b/det_v eq_c = c/det_v min_var = -(eq_b^2-4*eq_a*eq_c)/(4*eq_a) mu_min_var = -eq_b/(2*eq_a) lam_1 = (c-b*mu_min_var)/(det_v) lam_2 = (a*mu_min_var-b)/(det_v) w = lam_2[1,1]*solve(C)%*%mu + lam_1[1,1]*solve(C)%*%one_m If it is an equal weighted portfolio, \\(w\\) would be: w_aver = matrix(1/12, ncol = 1, nrow = n) var_aver = t(w_aver)%*%C%*%w_aver Let’s have a look at the difference between equal weighted and minimum variance portfolio: #difference between equal weighted and minimum variance portfolio cat(&#39;min_Var: &#39;, min_var, &quot;\\n&quot;) ## min_Var: 0.0002455391 cat(&#39;var_aver: &#39;, var_aver, &quot;\\n&quot;) ## var_aver: 0.0009595658 cat(&#39;var_aver-min_var: &#39;, var_aver-min_var, &quot;\\n&quot;) ## var_aver-min_var: 0.0007140267 cat(&#39;var_aver/min_var: &#39;, var_aver/min_var, &quot;\\n&quot;) ## var_aver/min_var: 3.907996 Lets see the weight for the stocks! w ## [,1] ## [1,] 0.1427604624 ## [2,] 0.0062456155 ## [3,] -0.1042327822 ## [4,] -0.3373815482 ## [5,] -0.1569301515 ## [6,] -0.0185447255 ## [7,] 0.0008043995 ## [8,] -0.0391292589 ## [9,] -0.0105920351 ## [10,] -0.3292557875 ## [11,] -0.2766437179 ## [12,] 2.1228995294 From above result, we can see that the variance of equal weighted portfolio is about 400% higher than the portfolio with minimum variance! Notice that the weight for the 3, 4, 5, 6, 8, 9, 10, 11 elements are negative! 43.5 Select a set of ticker Based on the weight given for the minimum variance portfolio, there are many weights that are negative. In this session, we want to select a set of ticker based on previous information. Here, we would like to remove the tickers with negative weights. The reason is that when the weight is negative, the asset is easy to but but hard to sell. Hence, the risk is relatively high which is not what we want. First, only preserve tickers with positive weights: #only preserve tickers with positive weights tickerList = c( &quot;T&quot;, # ATT (phone company) &quot;F&quot;, # Ford (cars) &quot;AAPL&quot;, # Apple (computers, phones, software) &quot;SPY&quot;) # S &amp; P index ETF Then, as what we did in previous step, read historical time series from .csv files in this directory: T = 250 # number of days of historical data to use n = length(tickerList) # number of assets # prices[t,k] = closing price on day t (counting back from today) of asset k prices = matrix( nrow = T, ncol = n) # allocate memory for the matrix for ( k in 1:n){ # iterate over k, not the ticker name ticker = tickerList[k] # get the corresponding ticker name fileName = paste( c( &quot;https://raw.githubusercontent.com/ConnieeeeeLIU/EDAV/main/StockPriceTimeSeries_&quot;, ticker, # &quot;.csv&quot;), # a .csv file collapse = &quot;&quot;) # no blanks between the parts CSVfile = readr::read_csv( fileName ) closingPriceSeries = CSVfile[[&quot;close&quot;]] # The column under &quot;close&quot;... # ... holds the closing prices for ( t in 1:T){ # Copy from the &quot;data frame&quot;... prices[ t, k] = closingPriceSeries[t] # ... to the matrix } } Next, calculate the daily returns, the mean return, and the empirical covariance: returns = matrix( nrow = (T-1), ncol = n) for ( t in 1:(T-1)){ for ( k in 1:n){ returns[ t, k] = ( prices[ t+1, k] - prices[ t,k ])/prices[ t, k] } } mu = matrix( 0, ncol = 1, nrow = n) # Column vector of expected returns # initialized to zero so this works: for ( k in 1:n){ # For asset k ... for ( t in 1:(T-1)){ # ... add the returns over time mu[k] = mu[k] + returns[t,k] } mu[k] = mu[k]/(T-1) # Divide by the number of samples ... } # ... to get the empirical mean ######Cov matrix C = matrix( 0, ncol = n, nrow = n) for (j in 1:n){ for (k in 1:n){ total = 0 for (t in 1:(T-1)){ total = total + (returns[t,j]-mu[j])*(returns[t,k]-mu[k]) } C[j,k] = total/(T-2) } } ###### one_m = matrix(1, ncol = 1, nrow = n) a = t(one_m)%*%solve(C)%*%one_m b = t(one_m)%*%solve(C)%*%mu c = t(mu)%*%solve(C)%*%mu det_v = a*c-b^2 eq_a = a/det_v eq_b = -2*b/det_v eq_c = c/det_v min_var = -(eq_b^2-4*eq_a*eq_c)/(4*eq_a) mu_min_var = -eq_b/(2*eq_a) lam_1 = (c-b*mu_min_var)/(det_v) lam_2 = (a*mu_min_var-b)/(det_v) w = lam_2[1,1]*solve(C)%*%mu + lam_1[1,1]*solve(C)%*%one_m Lets check the weight! w ## [,1] ## [1,] 0.43520296 ## [2,] -0.16138052 ## [3,] -0.00208128 ## [4,] 0.72825884 43.6 Visualization of Minimum Variance Portforlio Next we can also visualize the process of locating the minimum variance portfolio in R; we still choose a small set with other five tickers as in last section.(citation2) We will start with creating 5000 random weights first to simulate 5000 random portfolios. tickerList = c( &quot;IBM&quot;, # IBM (computers, mostly software) &quot;MSFT&quot;, # Microsoft (software) &quot;AAPL&quot;, # Apple (computers, phones, software) &quot;JPM&quot;, # J P Morgan Chase (bank) &quot;NVDA&quot;) # Nvidia (chips) # Almost the same steps to retrieve data again T = 250 # number of days of historical data to use n = length(tickerList) # number of assets # prices[t,k] = closing price on day t (counting back from today) of asset k prices = matrix( nrow = T, ncol = n) # allocate memory for the matrix for ( k in 1:n){ # iterate over k, not the ticker name ticker = tickerList[k] # get the corresponding ticker name fileName = paste( c( &quot;https://raw.githubusercontent.com/ConnieeeeeLIU/EDAV/main/StockPriceTimeSeries_&quot;, ticker, # &quot;.csv&quot;), # a .csv file collapse = &quot;&quot;) # no blanks between the parts CSVfile = readr::read_csv( fileName ) closingPriceSeries = CSVfile[[&quot;close&quot;]] # The column under &quot;close&quot;... # ... holds the closing prices for ( t in 1:T){ # Copy from the &quot;data frame&quot;... prices[ t, k] = closingPriceSeries[t] # ... to the matrix } } # We use log returns here to make a better plot log_returns = matrix( nrow = (T-1), ncol = n) for ( t in 1:(T-1)){ for ( k in 1:n){ log_returns[ t, k] = log( prices[ t+1, k] / prices[ t,k ]) } } mean_return &lt;- colMeans(log_returns) # We also magnify the variance by annualizing it C &lt;- cov(log_returns) * 252 Now we randomly choose 5000 weights for the portfolios, to see the return, risk and sharpe ratio of each portfolio: num_iter &lt;- 5000 weights &lt;- matrix(nrow = num_iter, ncol = length(tickerList)) portfolio_returns &lt;- vector(&#39;numeric&#39;, length = num_iter) portfolio_risk &lt;- vector(&#39;numeric&#39;, length = num_iter) sharpe_ratio &lt;- vector(&#39;numeric&#39;, length = num_iter) # Run 5000 portfolios for (i in seq_along(portfolio_returns)) { w &lt;- runif(length(tickerList)) w &lt;- w/sum(w) weights[i,] &lt;- w # weights ret &lt;- sum(w * mean_return) # returns ret &lt;- ((ret + 1)^252) - 1 # annualized returns portfolio_returns[i] &lt;- ret sd &lt;- sqrt(t(w) %*% (C %*% w)) # risk portfolio_risk[i] &lt;- sd sr &lt;- ret/sd # sharpe ratio sharpe_ratio[i] &lt;- sr } # Put returns, risk and sharpe ratio of all portfolio into a small table: tibble! portfolio_values &lt;- tibble(Return = portfolio_returns, Risk = portfolio_risk, SharpeRatio = sharpe_ratio) # Also combine weights and the values together weights &lt;- tk_tbl(weights) colnames(weights) &lt;- tickerList portfolio_values &lt;- tk_tbl(cbind(weights, portfolio_values)) # take a look here head(portfolio_values) ## # A tibble: 6 x 8 ## IBM MSFT AAPL JPM NVDA Return Risk SharpeRatio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.306 0.155 0.177 0.274 0.0876 -0.0142 0.469 -0.0304 ## 2 0.0707 0.275 0.247 0.227 0.181 -0.0609 0.538 -0.113 ## 3 0.291 0.129 0.179 0.113 0.289 -0.0926 0.478 -0.194 ## 4 0.393 0.283 0.101 0.151 0.0707 -0.0690 0.411 -0.168 ## 5 0.204 0.195 0.0145 0.187 0.399 -0.305 0.430 -0.710 ## 6 0.00588 0.274 0.337 0.132 0.252 -0.0127 0.633 -0.0201 Then we are going to show the minimum variance portfolio min_var &lt;- portfolio_values[which.min(portfolio_values$Risk),] print(min_var) ## # A tibble: 1 x 8 ## IBM MSFT AAPL JPM NVDA Return Risk SharpeRatio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.428 0.449 0.00106 0.0960 0.0265 -0.154 0.379 -0.406 Now let’s plot all of the portfolios and the minimum variance portfolio. It is very easier to see that the MVP refers to the portfolio with the smallest variance in all randomly composed portfolios. portfolio_values %&gt;% ggplot(aes(x = Risk, y = Return, color = SharpeRatio)) + geom_point() + labs(x = &#39;Risk&#39;, y = &#39;Returns&#39;, title = &quot;Portfolio Management&quot;) + geom_point(aes(x = Risk, y = Return), data = min_var, color = &#39;red&#39;) + annotate(&#39;text&#39;, x = 0.18, y = 0.01, label = &quot;Minimum variance portfolio&quot;) "],["introduction-to-topology-data-analysis.html", "Chapter 44 Introduction to Topology Data Analysis", " Chapter 44 Introduction to Topology Data Analysis Yiwei Ren For the Class Contribution, I made a YouTube video about Topology Data Analysis. In the video, I talked about basic elements of Topology Data Analysis as well as how this tool related to data visualization. This 10-minute talk will give you an overall idea about what Topology Data Analysis is. You can find the video in here: https://www.youtube.com/watch?v=6G9dtBZk8SM Also, You can find the slide in here: https://docs.google.com/presentation/d/1INN3VqTr57i7X4Jm8IUrtUZMjWusohiu2AoJ2lFtE1s/edit?usp=sharing "],["zoom-session-in-finance-and-data-science.html", "Chapter 45 Zoom session in Finance and Data-Science", " Chapter 45 Zoom session in Finance and Data-Science Minwoo Choi This Zoom session was held on Wed (Mar 18) at 4:00 EST (Food for thought) Systemic approach and its application in finance has a very long history. Systemic trading, ML driven factor / alpha engine has been around quite a while even though no one really knows what is actually going on in the industry, it is a very secretive world ! Financial industry, especially trading and investment management business, is an extremely closed community that is entirely competition driven. Hence, it is very rare to experience other company’s in-house research and models. Also, in my opinion, there is a huge gap between what we learn in school and cutting-edge models being used in secretive quantitative trading firms. This is one reason I am not a big fan of finance anymore. Based on my short experience outside of the financial industry, I feel that other data science communities are more open and collaboration based. (e.g. think about Stackoverflow… and TensorFlow is free, but Bloomberg terminal costs 30k per year) Second big difference is, data in finance is not static and much more vulnerable to a sudden regime change. For example, global macro situations, market liquidity can be purely driven by sudden change in central bank policy. Unlike natural phenomena, the market is man-made and driven by unnatural forces. Market intervention, manipulation is more often than outsiders believe. So, I hear many times (even though mine is at best second hand opinions) that looking into ‘mean-reversion’, ‘overbought, oversold’ indicators will provide a better opportunity than just looking at ‘static’ price data. Darko Matovski, CEO of CausaLens, explained two major pitfalls of applying ML in finance data during the AI and DS in Trading event held this week. ‘Let’s imagine applying deep-learning to US mortgage data that had never failed for 100 years, and make a prediction. And recall what happened during a financial crisis in a sudden shift in price dynamics.’ He also emphasized that finance data frequently shows sudden regime change, and having a proper ‘state status’ setup is crucial for reinforcement learning. (Of course, he did not reveal his way of building a better simulator, his proprietary models are reserved for clients only !) Also, another common problem in ML in finance is people tend to confuse ‘correlation’ to ‘causality’. Jeff Wecker, CTO of TwoSigma, made another good point during the conference about financial data. What is special about financial data and its usefulness? In a typical ML or data-science problem, we strive to make a better prediction using our data. One major problem of using data for solving financial problems in pursuit of maximizing profit is decaying value of data. First user who developed a fancy data model takes the most benefit, and then naturally the industry is being crowded as other users jump in and make a similar prediction based on the same data. Therefore, the benefit of making a similar prediction and solving a similar financial problem is rapidly decaying. This is not always the case in other technology fields. Of course, there’s lots of areas where we can apply data-science, ML techniques in finance. Any classification, NLP can help operations perform better and save costs. Document sorting, classification, or making credit decisions, providing a financial planning solution can be rather easily automated using ML techniques. One other interesting area could be using K-mean clustering for categorizing financial assets into multiple groups. This simple technique could be used for liquidity monitoring based on daily trading volume or volatility, also for categorizing assets into different risk groups for better portfolio management as well. "],["data-science-the-environment-and-sustainability.html", "Chapter 46 Data science, the environment, and sustainability", " Chapter 46 Data science, the environment, and sustainability George Reynolds (gbr2114) and Karunakar Gadireddy (kg2911) We conducted a zoom session on how data science can be used regarding the environment and sustainability. Our main points have been documented in the following slides https://docs.google.com/presentation/d/1z55B6DfJR4gDRCTMUnrMPKyrwtmxEKrsVYSvtRmN05Q/edit?usp=sharing We have also compiled a list of job opportunities for people who are interested in using data science skills in this field. https://docs.google.com/spreadsheets/d/1L3li9QLUgJ1YJxHGXa-6mWLhZAa3FL2akzxdnaB_dFs/edit?usp=sharing "],["absentee-ballot-voting-in-2020-presidental-election.html", "Chapter 47 Absentee ballot voting in 2020 presidental election 47.1 Background and Research Idea 47.2 Analyze ballot rejection data 47.3 Categorize 16 main ballot rejection reasons 47.4 Show participation rate and ballot non-accepted rate on a Georgia map", " Chapter 47 Absentee ballot voting in 2020 presidental election Shengyuan Cao 47.1 Background and Research Idea The right to vote is one of the most important rights in today’s world. Elections provide individuals the ability to influence the politics that govern their lives, and everyone should have an equal opportunity to participate in the voting system. Without a standardized voting process, voting is unequal among individuals and people will start to lose trust in the political system. To help promote voting equality across the country, especially in Georgia, I want to investigate if there were any potential patterns related to unequal voting. Absentee ballot serve as an important way for voters who cannot physically vote at voting centers on election days. And it is more prevalent during coronavirus times. Therefore, it is evident that absentee ballots play a considerable role in America’s election process. Our main goal is to try and assess absentee ballot equality based on recent election data, which would have many useful applications, such as to provide useful guide on how absentee ballots will be utilized in future elections. The following research will provide insights into which Georgia counties reject absentee ballots, and why they were rejected in 2020 Presidential election. Data comes from open data source at https://elections.sos.ga.gov/Elections/voterabsenteefile.do In this research, I explore reasons why some absentee ballots were rejected during 2020 Presidential election in Georgia. This topic is to my interest since voting through absentee ballot becomes the trend during pandemic times. And it is very important to protect voting rights and promote voting equlity through absentee ballot. First, I find top reasons for ballot rejection in each county to gain a general idea of rejection reasons, then I group all rejection reasons into 16 main categories using keyword pattern matching method to identify top rejection reasons across Georgia. Besides, I calculate non-accepted ballot rates and absentee ballot participation rate in each county and show these values on a Georgia map. 47.2 Analyze ballot rejection data Below are 2020 Presidential Absentee ballot data. First, we load the data and unzip it. downloader::download(&quot;https://github.com/Isabellatop/5702CCdata/raw/main/statewide_small.csv.zip&quot;, &quot;statewide_small.csv.zip&quot;) statewide_small &lt;- read.csv(unz(&quot;statewide_small.csv.zip&quot;, &quot;statewide_small.csv&quot;), header = TRUE, sep = &quot;,&quot;) Application_Status is “A” means that the absentee ballot application is “Accepted”. Since we are interested in why some absentee ballots were rejected, we want the ballot status where it is “R”,“C” or “S”, which means “Rejected”, “Cancelled” or “Spoiled” respectively. The “Status_Reason” column shows why the ballot is not accepted. CRS2020&lt;-statewide_small %&gt;% filter(Ballot_Status ==&quot;R&quot;|Ballot_Status ==&quot;C&quot;|Ballot_Status ==&quot;S&quot;) head(CRS2020,15) # Show first 15 rows ## X County Application_Status Ballot_Status ## 1 2 APPLING A C ## 2 3 APPLING A C ## 3 10 APPLING A C ## 4 13 APPLING A C ## 5 14 APPLING A C ## 6 23 APPLING A C ## 7 94 APPLING A C ## 8 96 APPLING A C ## 9 171 APPLING A C ## 10 174 APPLING A C ## 11 180 APPLING A C ## 12 216 APPLING A C ## 13 228 APPLING A C ## 14 242 APPLING A C ## 15 244 APPLING A C ## Status_Reason ## 1 mailed ballot surrendered to vote in-person ## 2 voter turned in ballot at the polls ## 3 mailed ballot surrendered to vote in-person ## 4 voter requested ## 5 mailed ballot surrendered to vote in-person ## 6 mailed ballot surrendered to vote in-person ## 7 voter turned in ballot at the polls ## 8 ballot was undelivered ## 9 voter requested ## 10 mailed ballot surrendered to vote in-person ## 11 voter requested ## 12 voter requested ## 13 mailed ballot surrendered to vote in-person ## 14 ballot was undelivered ## 15 ballot was undelivered We can get a sense of why ballots were rejected by finding the top one non-accepting reason for each county in Georgia. Top_reason_for_each_county &lt;- CRS2020 %&gt;% group_by(County,Status_Reason) %&gt;% summarise(Total_Number=n()) %&gt;% top_n(1,Total_Number) Top_reason_for_each_county ## # A tibble: 159 x 3 ## # Groups: County [159] ## County Status_Reason Total_Number ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 APPLING voter requested 258 ## 2 ATKINSON mailed ballot surrendered to vote in-person 126 ## 3 BACON mailed ballot surrendered to vote in-person 184 ## 4 BAKER ballot was undelivered 30 ## 5 BALDWIN voter requested 1062 ## 6 BANKS voter requested 199 ## 7 BARROW vip 1788 ## 8 BARTOW mailed ballot surrendered to vote in-person 1049 ## 9 BEN HILL voter requested 272 ## 10 BERRIEN mailed ballot surrendered to vote in-person 430 ## # … with 149 more rows There are 159 counties in Georgia State. I want to know whar are the most common rejection reason among all counties. We observed that vast majority counties have top rejecting reasons as “mailed ballot surrendered to vote in-person”, “voter requested”, “ballot was undelivered” and “ballot was undelivered”, which is an interesting and critical observation. Top_reason_for_each_county_new &lt;- Top_reason_for_each_county %&gt;% group_by(Status_Reason) %&gt;% summarise(Total_Number=n()) %&gt;% arrange(desc(Total_Number)) Top_reason_for_each_county_new ## # A tibble: 20 x 2 ## Status_Reason Total_Number ## &lt;chr&gt; &lt;int&gt; ## 1 &quot;mailed ballot surrendered to vote in-person&quot; 67 ## 2 &quot;voter requested&quot; 33 ## 3 &quot;voter turned in ballot at the polls&quot; 16 ## 4 &quot;ballot was undelivered&quot; 13 ## 5 &quot;voted in person&quot; 9 ## 6 &quot;vio&quot; 4 ## 7 &quot;vip&quot; 3 ## 8 &quot;affidavit&quot; 2 ## 9 &quot;administrative cancellation&quot; 1 ## 10 &quot;aff&quot; 1 ## 11 &quot;cancelled voted aip&quot; 1 ## 12 &quot;early voted&quot; 1 ## 13 &quot;lost it&quot; 1 ## 14 &quot;signed affidavit&quot; 1 ## 15 &quot;vip aff signed&quot; 1 ## 16 &quot;voted aip&quot; 1 ## 17 &quot;voted early&quot; 1 ## 18 &quot;voted in office&quot; 1 ## 19 &quot;voted in person/signed &quot; 1 ## 20 &quot;voted on bmd&quot; 1 This plot gives us a general idea about ballot rejection reasons, which helps further detailed investigation. ggplot(Top_reason_for_each_county_new,aes(fct_reorder(Status_Reason,Total_Number),Total_Number))+ geom_col() + coord_flip() + xlab(&quot;Ballot rejection reason&quot;) + ylab(&quot;Total count&quot;) + ggtitle(&quot;Distribution of top 1 ballot rejection reason in each of 159 \\ncounties in Georgia&quot;) 47.3 Categorize 16 main ballot rejection reasons There are 5735 unqiue reasons out of all 343,887 observations as to why the ballot is not accepted. Since there are too many unique reasons and the reasons are messy, I will categorize the reasons for rejection by grouping them using key words to find out what are those non-accepting reasons and their distribution. status_reason &lt;- CRS2020$Status_Reason (length(status_reason)) ## [1] 343887 (length(unique(status_reason))) ## [1] 5735 We want to find ballot rejection reason related to the category “vote early”, which includes keywords early, pol(poll), person(in person), aip (means absentee in person), and vip (means voted in person). We use grep function for pattern matching and find there are 183,981 cases for errors about early voting. When looking at all the unique ballot rejection reason in this category as showed below, we observe many reasons actally meant the same thing even though they were worded differently. And many reasons had typos in them. It is very likely that reasons for rejecting ballots were not strictly standardized in Georgia, which is an area that could be improved in future absentee ballot voting. early_related &lt;- c(&#39;early&#39;,&#39;pol&#39;,&#39;person&#39;,&#39;aip&#39;,&#39;vip&#39;) (Total_early&lt;- length(grep(paste(early_related,collapse=&quot;|&quot;),status_reason))) ## [1] 183981 Similarly, we find how many ballots were rejected due to “Address related issues”, which includes keywords add (address), county and location. We see that there were 2603 ballots rejected because of address related problems and unique address related reasons were listed below. address_related &lt;- c(&#39;add&#39;,&#39;county&#39;,&#39;location&#39;) Total_address &lt;- length(grep(paste(address_related,collapse=&quot;|&quot;),status_reason)) Total_address ## [1] 2603 The third category is “Date related issues”, which includes keywords dob, birth and date. In total there are 188 counts. date_related &lt;- c(&#39;birth&#39;,&#39;dob&#39;,&#39;date&#39;) Total_date &lt;- length(grep(paste(date_related,collapse=&quot;|&quot;),status_reason,value=TRUE)) Total_date ## [1] 188 The fourth category is “Signature issue”, which contains keyword “sign” (include signed and signature). There are 3612 many of them. Total_sign &lt;-length(grep(pattern = &#39;sign&#39;,status_reason)) Total_sign ## [1] 3612 The fifth category is “Dead issue”, which contains keyword died, deceased and passed. There are 223 counts. dead_related &lt;- c(&#39;die&#39;,&#39;deceased&#39;,&#39;passed&#39;) Total_dead &lt;- length(grep(paste(dead_related,collapse=&quot;|&quot;),status_reason,value=TRUE)) Total_dead ## [1] 227 The sixth group is about “Delivery issue”, which has keywords undeliver. We observe that there are 49,448 counts, which is a large component of rejected ballots. Most reasons are related to ballots were not delivered. deliver_related &lt;- c(&#39;undeliver&#39;) unique(grep(paste(deliver_related,collapse=&quot;|&quot;),status_reason,value=TRUE)) ## [1] &quot;ballot was undelivered&quot; ## [2] &quot;undelivered. voted in person.&quot; ## [3] &quot;ballot returned - undeliverable&quot; ## [4] &quot;returned as undeliverable&quot; ## [5] &quot;returned undeliverable&quot; ## [6] &quot;voter signed affidavit missing/undelivered &quot; ## [7] &quot;undeliverable&quot; ## [8] &quot;undeliverable - mailing to temp &quot; ## [9] &quot;undelivered&quot; ## [10] &quot;canc undeliverable - will fill out another &quot; ## [11] &quot;undeliverable - moc&quot; ## [12] &quot;ret&#39;d undeliverable&quot; ## [13] &quot;ballot undelivered&quot; ## [14] &quot;undelivered- voting in person&quot; ## [15] &quot;undelivered- vip&quot; ## [16] &quot;voted in person; ballot undelivered&quot; ## [17] &quot;ballot undelivered/voted in &quot; ## [18] &quot;ballot was undeliveredd&quot; ## [19] &quot;ret. undeliverable temp away&quot; ## [20] &quot;ballot undelivered sent to wrong state &quot; ## [21] &quot;ballot was undelivered. voted in &quot; ## [22] &quot;ballot undelivered. voted in person&quot; ## [23] &quot;ballot was undelivered.voted in &quot; ## [24] &quot;ballot was undelivered from the &quot; ## [25] &quot;original ballot was undelivered from &quot; ## [26] &quot;ballot was undelivered and voter &quot; ## [27] &quot;ballot was undelivered from &quot; ## [28] &quot;ballot undelivered.voted in person&quot; ## [29] &quot;ballot undelivered via email&quot; ## [30] &quot;opt to vote in person ballot undelivered&quot; ## [31] &quot;ballot was undelivered. voted at &quot; ## [32] &quot;ballot undelivered voted in person&quot; ## [33] &quot;application was undelivered&quot; ## [34] &quot;returned usps undeliverable&quot; ## [35] &quot;returned undeliverable, phone &quot; ## [36] &quot;ballot returned undeliverable&quot; ## [37] &quot;returned undelivered&quot; ## [38] &quot;return undeliverable&quot; ## [39] &quot;undelivered;re-issue per state&quot; ## [40] &quot;9/18/20 ballot undelivered&quot; ## [41] &quot;ballot undelivered and vote in person&quot; Total_deliver &lt;- length(grep(paste(deliver_related,collapse=&quot;|&quot;),status_reason,value=TRUE)) Total_deliver ## [1] 49448 The seventh group is about “Person lost ballot issue”, which contain keyword lost,destroy (include destroyed). Most reasons are that ballots were lost or destroyed. There are 5722 many of those. lost_related &lt;- c(&#39;lost&#39;,&#39;destroy&#39;) (Total_lost &lt;- length(grep(paste(lost_related,collapse=&quot;|&quot;),status_reason))) ## [1] 5722 The eighth category is related to “Admin error”, which has keywords admin(include administrative), office, vio (voted in office). We find there are as many as 16,562 counts, which is also a relatively large chunk of all rejected ballots. admin_related &lt;- c(&#39;admin&#39;,&#39;office&#39;,&#39;vio&#39;) (Total_admin &lt;- length(grep(paste(admin_related,collapse=&quot;|&quot;),status_reason))) ## [1] 16562 The ninth category is about “Name error”, which has keyword name. There are 16 in total. (Total_name &lt;- length(grep(pattern = &#39;name&#39;, status_reason))) ## [1] 16 The 10th category is about “Reissued error”. Keywords include reissue and re-issued. When looking at specific reasons, many of those are “spoiled - reissue” and “rejected ballot that has been re-issued”. There are 2024 counts. reissue_related &lt;- c(&#39;reissue&#39;,&#39;re-issue&#39;) (Total_reissue &lt;- length(grep(paste(reissue_related,collapse=&quot;|&quot;),status_reason))) ## [1] 2024 The 11th category is about “Receiving error”. Keywords are received/rcvd/late/deadline/after/arrived and there are 3863 of them. receive_related &lt;- c(&#39;received&#39;,&#39;rcvd&#39;,&#39;late&#39;,&#39;deadline&#39;,&#39;after&#39;,&#39;arrived&#39;,&#39;returned&#39;,&#39;envelope&#39;) (Total_receive &lt;- length(grep(paste(receive_related,collapse=&quot;|&quot;),status_reason))) ## [1] 3863 The 12th category is about “Voter request”, which includes keyword request and cancel. There are 62,816 of them. Majority reason includes that voter requested to cancel ballot or requested to vote at the poll instead. request_related &lt;- c(&#39;request&#39;,&#39;cancel&#39;) (Total_request &lt;- length(grep(paste(request_related,collapse=&quot;|&quot;),status_reason))) ## [1] 62816 The 13th category is about “Affidavit issue”, which includes keyword aff (includes affidavit). There are 6898 of them. aff_related &lt;- c(&#39;aff&#39;) (Total_aff &lt;- length(grep(paste(aff_related,collapse=&quot;|&quot;),status_reason))) ## [1] 6898 The 14th category is about “Multiple ballots/applications”, which includes keyword multiple. There are 93 of them. (Total_multiple &lt;- length(grep(pattern = &#39;multiple&#39;, status_reason))) ## [1] 93 The 15th category is about “Insufficient/missing information”, which has keywords insufficient, missing and there are 14 counts. insufficient_information_related &lt;- c(&#39;insufficient&#39;,&#39; missing&#39;) (Total_insufficient_information &lt;- length(grep(paste(insufficient_information_related,collapse=&quot;|&quot;),status_reason))) ## [1] 14 The 16th category is “Ineligible voter”, which has keyword ineligible and has 16 counts. unique(grep(pattern = &#39;ineligible&#39;,status_reason,value=TRUE)) ## [1] &quot;ineligible elector&quot; (Total_ineligible &lt;- length(grep(pattern = &#39;ineligible&#39;, status_reason))) ## [1] 16 Now we create a dataset with all the identified ballot rejection reasons with their counts to see what are the most promininet reasons that cause ballot to be rejected. Reasons &lt;- c(&#39;Early voting error&#39;,&#39;Address error&#39;,&#39;Date error&#39;,&#39;Signature error&#39;,&#39;Voter dead&#39;,&#39;Ballot undelivered error&#39;,&#39;Lost ballot&#39;,&#39;Administration error&#39;,&#39;Name error&#39;,&#39;Reissued error&#39;,&#39;Receiving error&#39;,&#39;Voter request to cancel ballot&#39;,&#39;Affidavit error&#39;,&#39;Multiple ballots error&#39;,&#39;Missing information&#39;,&#39;Ineligible voter&#39;) value_counts &lt;- c(Total_early,Total_address,Total_date,Total_sign,Total_dead,Total_deliver,Total_lost,Total_admin,Total_name,Total_reissue,Total_receive,Total_request,Total_aff,Total_multiple, Total_insufficient_information,Total_ineligible) df &lt;- data.frame(Reasons, value_counts) df ## Reasons value_counts ## 1 Early voting error 183981 ## 2 Address error 2603 ## 3 Date error 188 ## 4 Signature error 3612 ## 5 Voter dead 227 ## 6 Ballot undelivered error 49448 ## 7 Lost ballot 5722 ## 8 Administration error 16562 ## 9 Name error 16 ## 10 Reissued error 2024 ## 11 Receiving error 3863 ## 12 Voter request to cancel ballot 62816 ## 13 Affidavit error 6898 ## 14 Multiple ballots error 93 ## 15 Missing information 14 ## 16 Ineligible voter 16 We observe that the top three reasons for a ballot to be rejected are related to errors about early voting, voter request to cancel ballot and ballot undelivered error. These three reasons count for the vast majority of all the identified reasons. ggplot(df,aes(fct_reorder(Reasons,value_counts),value_counts))+ geom_col() + coord_flip() + xlab(&quot;Ballot rejection reason&quot;) + ylab(&quot;Total count&quot;) + ggtitle(&quot;Absentee ballot rejection reason in Georgia&quot;) Then, we want to find ballot non-accepted rate for each county in Georgia and make observations about which counties have the highest non-accepted rates. We calculate non-accepted rate by dividing the number of R,C,S (which ballot status is rejected, cancelled and spoiled ballots) by the total number of ballots with status R,C,S,A (A means accepted) for each county. We find that counties with the highest ballot non-accepting are Baldwin, Greene and Dougherty, which are all above 10%! We also compute the total absentee ballot number by adding all non-accepted ballot_status ballots (R,C,S,A) for each county. (Not_accepted_percent_state &lt;- statewide_small %&gt;% group_by(County) %&gt;% summarise(Not_accepted_percent_state=100*sum(Ballot_Status ==&quot;R&quot;|Ballot_Status ==&quot;C&quot;|Ballot_Status ==&quot;S&quot;)/sum(Ballot_Status ==&quot;R&quot;|Ballot_Status ==&quot;C&quot;|Ballot_Status ==&quot;S&quot;|Ballot_Status ==&quot;A&quot; ), Total_absentee_number=sum(Ballot_Status ==&quot;R&quot;|Ballot_Status ==&quot;C&quot;|Ballot_Status ==&quot;S&quot;|Ballot_Status ==&quot;A&quot; ))) %&gt;% arrange(desc(Not_accepted_percent_state)) ## # A tibble: 159 x 3 ## County Not_accepted_percent_state Total_absentee_number ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 BALDWIN 11.1 16692 ## 2 GREENE 11.0 11068 ## 3 DOUGHERTY 10.9 26300 ## 4 SUMTER 10.4 10603 ## 5 DEKALB 10.1 359975 ## 6 FULTON 9.93 507773 ## 7 TALBOT 9.76 2756 ## 8 GRADY 9.74 8902 ## 9 HANCOCK 9.73 3597 ## 10 JENKINS 9.64 2843 ## # … with 149 more rows Not_accepted_percent_state$County &lt;- tolower(Not_accepted_percent_state$County) Not_accepted_percent_state$County &lt;- as.character(Not_accepted_percent_state$County) 47.4 Show participation rate and ballot non-accepted rate on a Georgia map Now import GA population data for each county and join it with our ballot data. We can compute the absentee ballot participation rate by dividing total absentee ballot number in each county by population in each county as a way to standardize data since we don’t want to favor our analysis towards counties with high population. We find out overall, absentee ballot participation rate is high in 2020 presidental election in vast majority of counties in Georgia. This is not surprising given the pandemic was going on then and people preferred to vote through absentee ballot. The top three parcipation counties are Forsyth, Towns and Oconee, which were all above 70%. This again stress the importance of all kinds of ballot related issues and ways to solve them in order to improve the voting quality. Import population data in Georgia. GA_population &lt;- read.csv(&quot;https://raw.githubusercontent.com/Isabellatop/5702CCdata/main/GAdat.csv&quot;) GA_population$County &lt;- tolower(GA_population$County) Then, join GA county population data with county ballot data by the keyword “County” and compute absentee ballot participation rate. population_and_satewide &lt;- inner_join(GA_population, Not_accepted_percent_state, by=&quot;County&quot;) population_and_satewide &lt;- population_and_satewide %&gt;% mutate(Absentee_ballot_participation_rate=100*Total_absentee_number/Population) %&gt;% arrange(desc(Absentee_ballot_participation_rate)) head(population_and_satewide,15) ## County Population Not_accepted_percent_state Total_absentee_number ## 1 forsyth 175511 7.047288 125041 ## 2 towns 10471 7.731544 7450 ## 3 oconee 32808 5.917593 23202 ## 4 greene 15994 10.995663 11068 ## 5 dawson 22330 6.214335 14370 ## 6 bryan 30233 7.339305 19089 ## 7 fayette 106567 7.891644 67223 ## 8 union 21356 7.948817 12895 ## 9 henry 203922 8.279256 116037 ## 10 cherokee 214346 5.738003 120826 ## 11 morgan 17868 6.933360 10024 ## 12 fulton 920581 9.929043 507773 ## 13 jackson 60485 5.043254 33292 ## 14 columbia 124053 7.354239 66873 ## 15 paulding 142324 6.174068 76384 ## Absentee_ballot_participation_rate ## 1 71.24397 ## 2 71.14889 ## 3 70.72056 ## 4 69.20095 ## 5 64.35289 ## 6 63.13962 ## 7 63.08050 ## 8 60.38116 ## 9 56.90264 ## 10 56.36961 ## 11 56.10029 ## 12 55.15788 ## 13 55.04175 ## 14 53.90680 ## 15 53.66909 We create a Georgia map with map_data function in maps package, which provides all the geographical information used to create a map. ga_df &lt;- map_data(&quot;state&quot;) %&gt;% filter(region == &quot;georgia&quot;) # ga_df is the base (only the STATE) of Georgia ga_base &lt;- ggplot(data = ga_df, mapping = aes(x = long, y = lat, group = group)) + coord_quickmap() + #coord_quickmap() automatic adjustment geom_polygon(color = &quot;black&quot;, fill = &quot;gray&quot;) ga_base + theme_void() + geom_polygon(data = ga_df, fill = NA, color = &quot;white&quot;) + # inside, boarder color is white geom_polygon(color = &quot;black&quot;, fill = NA) # outside boarder color # map_data(&quot;county&quot;) has state (column &quot;region&quot;) and county (column &quot;subregion&quot;) county_df &lt;- map_data(&quot;county&quot;) %&gt;% filter(region == &quot;georgia&quot;) ga_base + theme_void() + # theme_void makes backgroud empty geom_polygon(data = county_df, fill = NA, color = &quot;white&quot;) + # inside: boarder color is white geom_polygon(color = &quot;black&quot;, fill = NA) # outside boarder color is black county_df$subregion &lt;- replace(county_df$subregion, county_df$subregion==&quot;de kalb&quot;, &quot;dekalb&quot;) state_df &lt;- map_data(&quot;state&quot;) %&gt;% filter(region == &quot;georgia&quot;) mapdat &lt;- left_join(population_and_satewide,county_df, by = c(&quot;County&quot;=&quot;subregion&quot;)) head(mapdat,15) ## County Population Not_accepted_percent_state Total_absentee_number ## 1 forsyth 175511 7.047288 125041 ## 2 forsyth 175511 7.047288 125041 ## 3 forsyth 175511 7.047288 125041 ## 4 forsyth 175511 7.047288 125041 ## 5 forsyth 175511 7.047288 125041 ## 6 forsyth 175511 7.047288 125041 ## 7 forsyth 175511 7.047288 125041 ## 8 forsyth 175511 7.047288 125041 ## 9 forsyth 175511 7.047288 125041 ## 10 forsyth 175511 7.047288 125041 ## 11 forsyth 175511 7.047288 125041 ## 12 forsyth 175511 7.047288 125041 ## 13 forsyth 175511 7.047288 125041 ## 14 forsyth 175511 7.047288 125041 ## 15 forsyth 175511 7.047288 125041 ## Absentee_ballot_participation_rate long lat group order region ## 1 71.24397 -84.25917 34.17693 415 16596 georgia ## 2 71.24397 -84.27064 34.33163 415 16597 georgia ## 3 71.24397 -83.97269 34.33163 415 16598 georgia ## 4 71.24397 -83.96696 34.30298 415 16599 georgia ## 5 71.24397 -83.96124 34.29725 415 16600 georgia ## 6 71.24397 -83.94978 34.29725 415 16601 georgia ## 7 71.24397 -83.94978 34.28579 415 16602 georgia ## 8 71.24397 -83.95551 34.26860 415 16603 georgia ## 9 71.24397 -83.96124 34.24569 415 16604 georgia ## 10 71.24397 -83.98415 34.22850 415 16605 georgia ## 11 71.24397 -84.07010 34.18839 415 16606 georgia ## 12 71.24397 -84.08729 34.14828 415 16607 georgia ## 13 71.24397 -84.10448 34.13110 415 16608 georgia ## 14 71.24397 -84.11593 34.09672 415 16609 georgia ## 15 71.24397 -84.11593 34.07380 415 16610 georgia Now visualize ballots non_accepted_percent for each county on map. p &lt;- ggplot(mapdat, aes(long, lat, group = group)) + geom_polygon(aes(fill = Not_accepted_percent_state, color=&quot;yellow&quot;), # inside county colour = alpha(&quot;white&quot;, 1/2)) + scale_fill_gradient(low=&quot;blue&quot;, high=&quot;red&quot;)+ geom_polygon(data = state_df, colour = &quot;black&quot;, fill = NA) + #outside is Georgia state data, not specific theme_void() + coord_fixed(1.2) print(p+ggtitle(&quot;Percentage of non-accepted ballots in Georgia for 2020 Presidential Election&quot;)) Now visualize absentee ballot participation rate for each county on map, which is calcualted by summing up all ballots and divided by population county-wise. We can see that most counties have participation rates higher than 30 percent. s &lt;- ggplot(mapdat, aes(long, lat, group = group)) + # Each group is a different code for each county geom_polygon(aes(fill = Absentee_ballot_participation_rate, color=&quot;brown&quot;), colour = alpha(&quot;white&quot;, 1/2)) + # inside county&#39;s border is white scale_fill_gradient(low=&quot;blue&quot;, high=&quot;red&quot;)+ # fill-in color geom_polygon(data = state_df, colour = &quot;black&quot;, fill = NA) + # outside state color theme_void() + coord_fixed(1.2) print(s+ggtitle(&quot;Absentee ballot participation rate in Georgia for 2020 Presidential Election&quot;)) "],["final-project-zoom-meetup-sports.html", "Chapter 48 Final project Zoom meetup - sports", " Chapter 48 Final project Zoom meetup - sports Santos Hernandez For my community contribution, I tried to connect people to project partners that had an interest in sports. After conducting a poll on Piazza to find the best time, I held a zoom session where people could find partners and brainstorm ideas for the final project. The session was on March 9th, 2021 from 9pm-10pm. Although only a few people showed, everyone was able to find project partners, including myself! I also made a followup post with some notes/resources that were gathered during the meeting. This document also had contact information in case someone missed the meeting and still needed a partner. The resource document lives here. "],["in-person-fitness-hour.html", "Chapter 49 In-Person Fitness Hour 49.1 Piazza Announcement 49.2 Final Event", " Chapter 49 In-Person Fitness Hour Aditya Koduri and Archit Matta 49.1 Piazza Announcement In an effort to provide some real interaction amidst these virtual times, we organized an In-Person Fitness Hour. Events: 1. Yoga (Bring your own mats) 2. Jump Rope (Available with Organizers) 3. Frisbee (Available with Organizers) 4. Running Location: Riverside Park (Meeting Point - Hudson Beach) Timing: Tuesday, 23rd March 2021 - 10:00 AM Safety Guidelines: 1. Masks are compulsory and organizers would provide additional masks at the venue 2. Maintaining Social Distancing is necessary,and the activities have been planned keeping that in mind 3. Please bring your own sanitizers and the organizers would bring additional sanitizers to the venue 4. The organizers would ensure all equipment being offered is sanitized before use 5. Please refrain from attending if you show symptoms or have been in a high-risk zone 6. In addition, please also follow the latest guidance from the U.S. Centers for Disease Control and Prevention (CDC) 49.2 Final Event 49.2.1 Jump Rope 49.2.2 Monkey Bars 49.2.3 Frisbee "],["work-culture-and-time-management.html", "Chapter 50 Work culture and time management", " Chapter 50 Work culture and time management Sai Thrinath Gunda and Tarun Devireddy Everyone is aware of the difference in the work culture, the management and type of work at Startups and MNCs, and across different organizations in consulting, technology and finance. Both Tarun and I have worked for 3years+ at multiple firms, including MNCs and Startups, and see a huge contrast in the kind of soft-skills required along with skills to manage timelines and understanding and meeting the expectations. We began the session with giving our backgrounds. We then discussed about the differences in Hierarchy, Goal setting, Expectations, Work Environment, Knowledge Transfer, Appraisals at an MNC vs start up Slides can be found at: https://docs.google.com/presentation/d/1QUNz830hUb6Q8XmV3wgG9Gx1gK-p-NRe-LR40vXz6Ho/edit?usp=sharing "],["data-science-for-social-good.html", "Chapter 51 Data Science for Social Good 51.1 R Markdown", " Chapter 51 Data Science for Social Good Rahul Subramaniam and Tushar Agrawal 51.1 R Markdown Data Science has a plenitude of applications. With virtually every sector incorporating data science to bolster their efficiency/sales/revenues/performance, novel applications of Data Science come to light every day. One such field is the field of Social Good. A lot of Government/Non-Profit organizations use Data Science to develop applications/models that will benefit the society as a whole. https://drive.google.com/file/d/1cnZwDFeIhi4RqTITP6OHwMutynk7jeuI/view?usp=sharing Rahul Subramaniam and Tushar Agrawal conducted a Zoom session that discussed various applications of Data Science for Social Good. Click the link above to view the session. "],["wine-and-wine-tasting.html", "Chapter 52 Wine and wine tasting", " Chapter 52 Wine and wine tasting Gregor Z. Hanuschak I emailed the class and TAs on 2/28 to let them know I’d be presenting a little about my knowledge of wine and wine tasting on Saturday, March 6. The presentation I put together can be found here: https://github.com/gregorzh/Wine/blob/main/Intro%20to%20wine%20tasting%202.pdf "],["meet-the-pets-zoom-session.html", "Chapter 53 Meet the pets zoom session", " Chapter 53 Meet the pets zoom session Zhuoyue Hao Overview To help overcome the lack of personal interactions associated with remote learning, the project is designed to be hosting a zoom session on meeting the pets, which all pet lovers can participant no matter if they have pets or not. At the beginning of the event, people who come with their pets will present a brief introduction about their pets, such as their names, breeds, ages, hobbies, favorite foods. Then, people who come by themselves are expected to have some short videos (best if between 3-5 minutes) ready to share with everyone else. The videos could be any types – as long as it’s about pets! It should be really easy to find on YouTube or Tik-Tok. We will also have some times for participants to exchange ideas on pet training techniques or feeding tips at the end of the session. People are welcomed to exchange contact information and stay in touch after the event, and hopefully we can have a chance to hang out in-person after the Covid period. Milestones 2/8/2021: Introduced the idea of hosting a pet night during class discussion, and started a poll survey on Piazza o 12 out of 16 responders said that they were interested in the event, and 6 of them said that they will come with their pets 2/10/2021: Conducted a follow up google survey about participants’ preferable times and information of their pets 3/9/2021: Published the meeting details on Piazza and call for all pet lovers to participant 3/11/2021: The event took place at 8 p.m. EST, March 11th. "],["github-initial-setup.html", "Chapter 54 Github initial setup 54.1 GitHub Actions 54.2 DESCRIPTION file", " Chapter 54 Github initial setup Joyce Robbins Create a new repository. (For cc21 I started with a new repo on GitHub since I wanted the main branch to be called main and that did not seem possible with usethis functions or RStudio \" Copy the following files from the previous version and edit as necessary. (Search for name of previous repo to catch all instances.) _bookdown.yml _common.R _output.yml appendix_initial_setup.Rmd appendix_pull_request_tutorial.Rmd DESCRIPTION index.Rmd sample_project.Rmd /.github /resources/sample_project /resources/tutorial_pull_request_mergers 54.1 GitHub Actions 54.1.1 Secrets https://medium.com/@delucmat/how-to-publish-bookdown-projects-with-github-actions-on-github-pages-6e6aecc7331e Secret #1: Create a token here https://github.com/settings/tokens and paste it in a secret in the repo named GH_PAT Secret #2: Add a Secret called EMAIL with GitHub email See: https://github.com/r-lib/actions/tree/master/examples#managing-secrets 54.1.2 Create a gh-pages branch: https://jiafulow.github.io/blog/2020/07/09/create-gh-pages-branch-in-existing-repo/ (May happen automatically???) 54.1.3 GitHub Pages in repo settings (May happen automatically???) 54.2 DESCRIPTION file Need a better process… Downloaded submissions from CourseWorks Create DESCRIPTION file. Add add dependencies with projthis::proj_update_deps() https://twitter.com/ijlyttle/status/1370776366585614342 Add these Imports to the real DESCRIPTION file. Found problematic packages by looking at reverse dependencies of the packages that failed to install: devtools::revdep() Also used pak::pkg_deps_tree() Problems: magick rJava dependency of qdap "],["tutorial-for-pull-request-mergers.html", "Chapter 55 Tutorial for pull request mergers 55.1 General 55.2 Check branch 55.3 Examine files that were added or modified 55.4 Check .Rmd filename 55.5 Check .Rmd file contents 55.6 Request changes 55.7 Merge the pull request", " Chapter 55 Tutorial for pull request mergers 55.1 General The following is a checklist of steps to perform before merging the pull request. At any point, if you’re not sure what to do, request a review from one of the PR leaders. 55.2 Check branch PR should be submitted from a non-main branch. If PR was submitted from the main branch, provide these instructions on how to fix the problem: Close this PR. Follow the instructions here for forgetting to branch if you committed and pushed to GitHub: https://edav.info/github#fixing-mistakes If you have trouble with 2., then delete the local folder of the cc21 project and reclone. (In other words, start over.) Open a new PR. 55.3 Examine files that were added or modified There should be only ONE .Rmd file. All of the additional resources should be in the resources/&lt;project_name&gt;/ folder. There should be no other files in the root directory besides the .Rmd file. 55.4 Check .Rmd filename The .Rmd filename should be words only and joined with underscores, no white space. (Update: It does not need to be the same as the branch name.) The .Rmd filename can only contain lowercase letters. (Otherwise the filenames do not sort nicely on the repo home page.) 55.5 Check .Rmd file contents The file should not contain a YAML header nor a --- line. The second line should be blank, followed by the author name(s). The first line should start with a single hashtag #, followed by a single whitespace, and then the title. There should be no additional single hashtag headers in the chapter. (If there are, new chapters will be created.) Other hashtag headers should not be followed by numbers since the hashtags will create numbered subheadings. Correct: ## Subheading. Incorrect: ## 3. Subheading. If the file contains a setup chunk in .Rmd file, it should not contain a setup label. (The bookdown render will fail if there are duplicate chunk labels.) i.e. use {r, include=FALSE} instead of {r setup, include=FALSE}. See sample .Rmd Links to internal files must contain resources/&lt;project_name&gt;/ in the path, such as: ![Test Photo](resources/sample_project/election.jpg) The file should not contain any install.packages(), write functions, setwd(), or getwd(). If there’s anything else that looks odd but you’re not sure, assign jtr13 to review and explain the issue. 55.6 Request changes If there are problems with any of the checks listed above, explain why the pull request cannot be merged and request changes by following these steps: Then, add a changes requested label to this pull request. Your job for this pull request is done for now. Once contributors fix their requests, review again and either move forward with the merge or explain what changes still need to be made. 55.7 Merge the pull request If all is good to go, it’s time to merge the pull request. There are several steps. 55.7.1 Add chapter filename to _bookdown.yml in PR’s branch To access the PR branch: Make sure you are on the PR branch by checking that the PR branch name is shown (not main): Open the _bookdown.yml file. delete everything in the file beginning with rmd_files: [ and then add the name of the new file in single quotes followed by a comma: Why? Because it will be easier to fix the merge conflicts this way. (A better way to do this is to merge main into the PR branch before adding the new file but this can’t be done on GitHub. If there’s interest I will explain how to do this locally.) Save the edited version. Click the resolve conflicts button: Cut the new filename and paste it into the proper location. Then delete the lines with &lt;&lt;&lt;&lt;&lt;&lt;&lt; xxxx, ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; main. In short, the file should look correct when you’re done. Click the “Marked as resolved” button and then the green “Commit merge” button. 55.7.2 PR Leaders only: Add part names to .Rmd for every first article in part Only do this if you are adding the first chapter in a PART. For every first article of each part, add the chapter name on the top of the .Rmd file, then propose changes. The example is like this. 55.7.3 Merge PR and leave a comment Now comes the final step. If you’re not sure that you did things correctly, assign one of the PR merge leaders or @jtr13 to review before you merge the PR. Go back to the conversation tab of the pull requests page, for example: https://github.com/jtr13/cc20/pull/23#issuecomment-728506101 Leave comments for congratulations 🎉 (type :tada:) and then click on the green button for merge. 55.7.4 Check updated version A successful merge means that the addition file or files were added to the project with no merge conflicts. It does not mean that the book will render and deploy to GitHub pages without issues. After the merge, it will take about 5-10 minutes for GitHub Actions to render the book and deploy the updated version. If there’s a problem I will be notified by email and will address it. In other words, your job is done. However if you’re interested, you can check the progress by clicking Actions at the top of the repo. "]]
